AWS Regions

	Region   Name	Region	Endpoint
	US East (N. Virginia)	us-east-1	
	US West (N. California)	us-west-1	
	US West (Oregon)	    us-west-2

--------------------------------------------------------------------------------------------
### AWS CLI ### 
--------------------------------------------------------------------------------------------

$ sudo pip install awscli --ignore-installed six 		<= version 1.16, it supports EKS
--------------------------------------------------------------------------------------------
	# Alternative Installation using CentOS Repository YUM
	https://devopsmates.com/install-configure-aws-cli-amazon-web-services-command-line-interface/
	$ yum install epel-release –y
	$ yum install python-pip –y
	$ pip install awscli
	$ pip install –upgrade awscli
	$ aws –version							<=1.14  !Not supporting the ESK(starts on 1.15.32)yet
	 aws help 
--------------------------------------------------------------------------------------------

	$ aws --version
	aws-cli/1.16.115 Python/3.6.7 Linux/3.10.0-957.1.3.el7.x86_64 botocore/1.12.105   <= EKS Works

	$ aws configure
	  AWS Access Key ID [None]: 
	  AWS Secret Access Key [None]: 
	  Default region name [None]: us-west-2  <= Oregon
	  Default output format [None]: json
	
	$ aws configure --profile=clusterAdmin					<= Specific AWS USER account credential
	  AWS Access Key ID [****************UVFA]:
	  AWS Secret Access Key [****************KJ2l]:
	  Default region name [us-region-2]: us-west-2			<= Oregon
	  Default output format [None]: json					<= ONLY 'JSON' WORKS!!!! 
-------------------------------------------------------------------------------
# Connection test
-------------------------------------------------------------------------------
$ aws s3 ls
	2017-02-27 20:39:36 backup-jenkins-sign
	2017-05-22 16:10:49 mobile-xpromo
	
-------------------------------------------------------------------------------
# Canonical ID ? #
	https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html
-------------------------------------------------------------------------------
$ aws s3api list-buckets
{
    "Owner": {
        "DisplayName": "aws-gdpr",
        "ID": "25c4bb...............d729ef2e"  				<= Canonical ID
    },

--------------------------------------------------------------------------------------------



##############################################################################
### EKS  (Elastic Kubernetes Service)
#   Training: https://www.linkedin.com/learning/running-kubernetes-on-aws-eks
##############################################################################

API/CLI		---> Kubectl, Ajax, etc
Management				|
------------------------V-----------------------------------------------------
Secheduler	\						Kubelet Node		
									Service Proxy
								/	
  etcd		-	API Server		-	Kubelet Node		<- User from Internet
			   (Master Node)	\	Service Proxy

Controller /						Kubelet Node
Manager								Servcie Proxy

<------ EKS ---------------->|<------- EC2 -------->|<---- Internet User ----> 



Pre-EKS Deployment
	1. AWS 		CLI
	2. Kubectl  CLI				<= Auto completion
	3. BASH 	CLI
	
Pre-AWS EKS Deployment
	1. IAM Configuration
		- Create a 'ROLE' defined with EKS Permissions
		- EKS:* 'Policy' applied to user/group
		
	2. VPC create for ESK use
		- Create 'ESK VPC' using CloudFormation

EKS Core Service
	1. Create cluster 'Controle Plane'(system configuration and management)
	2. Establish kubectl credentials
		- Install aws-iam-authenticator(binary) and kubectl
		- $ aws eks update-kubeconfig --name '#Rename to cluster's_name'
		
Create Worker Nodes	
	1. Create an AutoScaling group of nodes with CloudFormation
	2. Create Node Auth with '$ kubectl apply -f aws-auth-cm.yaml'
	
Wait for the nodes to register
	$ kubectl get nodes -w
	
Ensure you can create an ELB
	$ aws iam create-service-linked-role --aws-service-name elasticloadbalancing.amazonaws.comaws
	
Use your Kubernetes Envrionment
	

--------------------------------------------------------------------------------------------
# EKS Supports Regions 
--------------------------------------------------------------------------------------------
https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/
us-west-2 (Oregon), us-east-1 (N. Virginia), us-east-2 (Ohio)
Ireland, Frankfurt, London,	Paris, Stockholm, Singapore, Tokyo, Sydney, Seoul, Mumbai
--------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------
### ESK Setup Lab Info Gethering  ###
--------------------------------------------------------------------------------------------
IAM-ROLE-ARN: arn:aws:iam::688595016292:role/Cluster_EKS_Role

VPC-ID:vpc-0121526bc433d8e2b
SECURITY-GROUP-ID:sg-09222c96502064a3a
SUBNET-IDS:subnet-09442239f4e596b6d,subnet-069fb4ed48718a9f7,subnet-05a8b3907455065ce

NODE-ROLE-ARN:
LABEL-NODE-ROLE-ARN:
USER_ARN:


--------------------------------------------------------------------------------------------
1. Create IAM Accounts and Policy for 'CRUD' operations 
--------------------------------------------------------------------------------------------
  Create 2x Policies for EKS and CloudFormation Admins
	AWS -> IAM -> Policies -> Create Policy
		=> AdminEKSPolicy 
		=> AdminCloudFormationPolicy

----------------------------------------------------------
# Create Policy		
----------------------------------------------------------
  # Name: AdminEKSPolicy			<= for Kubernetes 
----------------------------------------------------------
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",			<= Full permisson
            "Action": [
                "eks:*"					<= All resource for EKS
            ],
            "Resource": "*"
        }
    ]
}
----------------------------------------------------------
  # Name: AdminCloudFormationPolicy		<= fot CM Tool
----------------------------------------------------------
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",			<= Permisson
            "Action": "*",				<= Full permisson
            "Resource": "*"				<= All resource
        }
    ]
}
----------------------------------------------------------
# Create Roll 
----------------------------------------------------------
	AWS -> IAM -> Roles -> Create Role -> AWS Service -> EKS -> Add 
	  - AmazonEKSServicePolicy 			<= From Existing Policy
      - AmazonEKSClusterPolicy 			<= From Existkuing Policy

	Create -> Name: Cluster_EKS_Roll

	IAM-ROLE-ARN: arn:aws:iam::6...2:role/Cluster_EKS_Role

-------------------------------------------------------------------
IAM: Creating ClusterAdmin and ClusterUser accounts
-------------------------------------------------------------------

-----------------------------------------------------
# Create User #1:  clusterAdmin
-----------------------------------------------------
  IAM -> Username: clusterAdmin 
	  -> Access Type: both Programmatic and console Access 
	  -> Password 
	  -> Attach 4 existing Policies 
	  
			1. AdminEKSPolicy					<= Newly created
			2. AdminCloudFormationPolicy		<= Newly created
			3. AmazonEKSServicePolicy 			<= From Existing   Policy
			4. AmazonEKSClusterPolicy 			<= From Existkuing Policy
	  
	  -> Download CSV
		
 - eks admin policy
 - k8s admin "system:master" group
 

-----------------------------------------------------
# Create User #2: clusterUser
-----------------------------------------------------
	IAM -> Username: clusterAdmin 
		-> Access Type: both Programmatic and console Access 
		-> Password 
		-> Attach 4 existing Policies 
		-> Download CSV


	Add policies: 
	  - AdminEKSPolicy					<= NEW
		
	- no IAM policies
	- k8s admin "system:master" group


--------------------------------------------------------------------------------------------
# CloudFormation - Create a Stack VPC Template From S3
--------------------------------------------------------------------------------------------
1. Check Region: us-west-02(Orgegon)

2. AWS Colsone 
	-> ESK 
	-> Create Cluster 
	-> Specify an AWS S3 template URL
	https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2018-08-30/amazon-eks-vpc-sample.yaml

  -> Stack name: classCluster (or classEKSVPC) 
	VPCBlock: 		192.168.0.0/16
	Subnet01Block: subnet1(192.168.64.0./18...)
	Subnet02Block: subnet2(192.168.128.0./18...)
	Subnet03Block: subnet3(192.168.192.0./18...)

  -> Reset Default to create
  -> From Output
		Key				Value					Description	       Export Name
		SecurityGroups	sg-09222c96502064a3a		
		VpcId			vpc-0121526bc433d8e2b		
		SubnetIds		subnet-09442239f4e596b6d,subnet-069fb4ed48718a9f7,subnet-05a8b3907455065ce 
	
	Copy from output:
	IAM-ROLE-ARN: arn:aws:iam::688595016292:role/Cluster_EKS_Role
	
	VPC-ID: vpc-0121526bc433d8e2b
	SECURITY-GROUP-ID:s g-09222c96502064a3a
	SUBNET-IDS: subnet-09442239f4e596b6d,subnet-069fb4ed48718a9f7,subnet-05a8b3907455065ce 

-------------------------------------------------------------------------------
# Setup 3x-Remote Tools 
	- Kubectl 
	- aws-iam-authenticator 
	- AWS CLI
	
-------------------------------------------------------------------------------
  1. Install "kubectl" 
	https://kubernetes.io/docs/tasks/tools/install-kubectl/
-------------------------------------------------------------------------------
  Linux:
	curl -LO https://storage.googleapis.com/kubernetes-release/release/\
		$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
  MacOS:
	curl -LO https://storage.googleapis.com/kubernetes-release/release/\
		$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl

	$ chmod + x kubectl
	$ mv kubectl /usr/local/bin
	# $ source .bash_profile				<= If kubectl not works, reload profile
	
	$ kubectl version --short --client
	
	-------------------------------------------------
	| ###  kubectl command syntax ###				|
	| $ kubectl [command] [TYPE] [NAME] [flags]		|	
	-------------------------------------------------
	
	---------------------------------------------------
	# Enabling shell auto-completion
	---------------------------------------------------
	https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion
	$ kubectl completion -h						<= Check first
	
	$ yum install bash-completion -y
	$ source <(kubectl completion bash)
	$ echo "source <(kubectl completion bash)" >> ~/.bashrc
	
	$ kubectl  <TAB>		<= Autocompletion works!
	  annotate       autoscale      cordon         drain          label          proxy          taint
	  api-resources  certificate    cp             edit           logs           replace        top
	  api-versions   cluster-info   create         exec           options        rollout        uncordon
	  apply          completion     delete         explain        patch          run            version
	  attach         config         describe       expose         plugin         scale          wait
	  auth           convert        diff           get            port-forward   set

	
-------------------------------------------------------------------------------
  2. Download "aws-iam-authenticator" binary:
-------------------------------------------------------------------------------
	https://docs.aws.amazon.com/eks/latest/userguide/configure-kubectl.html
	https://github.com/kubernetes-sigs/aws-iam-authenticator
	-------------------------------------------------------------------------------
  Linux:
	curl -sLO https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/linux/amd64/aws-iam-authenticator
  MacOS:
	curl -sLO https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/darwin/amd64/aws-iam-authenticator
	
  $ chmod + x 	aws-iam-authenticator
  $ mv aws-iam-authenticator /usr/lcoal/bin   			<= echo $PATH location
  
-------------------------------------------------------------------------------
  3. Install "AWS CLI" 
	https://docs.aws.amazon.com/cli/latest/userguide/installing.html
-------------------------------------------------------------------------------
	$ sudo pip install awscli --ignore-installed six 		<= 1.16 EKS Works
	$ pip install awscli --upgrade --user
-------------------------------------------------------------------------------
	$ aws --version
		aw s-cli/1.16.115 Python/3.6.7 Linux/3.10.0-957.1.3.el7.x86_64 botocore/1.12.105

	# AWS sts (Security Token Service)
	$ aws sts get-caller-identity --output text --query 'Account'
		688595016292		<= account ID
	# get your account ID
		ACCOUNT_ID=$(aws sts get-caller-identity --output text --query 'Account')

	# 
	$ aws eks update-kubeconfig --name classEKS

	$  cat credentials
[default]
aws_access_key_id = AKIAJ...EZPUVFA
aws_secret_access_key = NJUpg...4fKJ2l
[clusterAdmin]
aws_access_key_id = AKIAJR...PUVFA
aws_secret_access_key = NJUpgo2+y...
[clusterUser]
aws_access_key_id = AKIA...7ABA
aws_secret_access_key = j8/8a....Rmgf2


-------------------------------------------------------------------------------
# Configuration on Linux Shell 
# Add to Environment Variable
-------------------------------------------------------------------------------
  # Config CLI for AWS and Kubectl
  # AWS CLI
  $ aws configure									<= Setup or update
	  AWS Access Key ID [****************UVFA]:
	  AWS Secret Access Key [****************KJ2l]:
	  Default region name [us-west-2]: 				<= Oregon
	  Default output format [json]: 				<= Only json works
   
    SYNOPSIS
     aws configure [--profile profile-name]  
	
	$ aws configure --profile=clusterAdmin			<=for 'clusterAdmin' User
	
	$ export AWS_PROFILE=clusterAdmin

	$ env | grep AWS_PROFILE									<= If no, add to env

***	$ echo "export AWS_PROFILE=clusterAdmin" >> .bash_profile	<= .bash_profile <= login shell 
																<= .bashrc 		 <= after login user shell
	
***	$ aws eks update-kubeconfig --name classEKS					<= Updated CLI
	
	# $ aws eks --region us-west-2 update-kubeconfig --name classEKS	<= Old CLI
	  => Updated context arn:aws:eks:us-west-2:688595016292:cluster/classCluster in /home/awseks/.kube/config

	$ aws eks list-clusters			<= Get EKS cluster info
	------------------------------------------
	{
		"clusters": [
			"classEKS"				<= AWS EKS Cluster Name
		]
	}
	------------------------------------------


-------------------------------------------------------------------------------
# EKS is Kubernetes Master Server
# Create a EKS Cluster from AWS Console
-------------------------------------------------------------------------------
AWS -> EKS -> Create Cluster -> 

# A New Cluster configuration(A new Kubernetes SERVER)
	Cluster name: classEKS
	Kubernetes Version -> 1.11   				<= Latest
	Role Name: Cluster_EKS_Role					<= grep from Role creation

# Networking	
	VPC 	-> vpc-xxx(192.168.0.0/16..)
	Subnets	-> subnet1(192.168.64.0./18...)
			   subnet2(192.168.128.0./18...)
			   subnet3(192.168.192.0./18...)	
			   
# Security groups -> sg-09222c96502064a3a
	
	>>> Create <<<	

-------------------------------------------------------------------------------
# Reigster cluster 'classEKS' to Kubectl
-------------------------------------------------------------------------------

# Check the cluster list
$ aws eks list-clusters
------------------------------------------
	{
		"clusters": [
			"classEKS"
		]
	}
------------------------------------------
# Register
------------------------------------------
$ aws eks update-kubeconfig --name classEKS
	Added new context arn:aws:eks:us-west-2:688595016292:cluster/classEKS to /home/awseks/.kube/config

----------------------------------------------------------------------------
# check if kubeclt actually communicating with 'classEKS'
----------------------------------------------------------------------------
$ kubectl get pods
	No resources found.						<= Good sign, no error
	
$ kubectl get nodes
	No resources found.
----------------------------------------------------------------------------
# test if pod can create apply hostname.yaml
----------------------------------------------------------------------------
$ kubectl apply -f hostname.yaml				
	deployment.extensions/hostname-v1 created	<= ok
	service/hostname-v1 created					<= ok

$ kubectl get pods
	NAME                           READY   STATUS    RESTARTS   AGE
	hostname-v1-56bc754656-njdgp   0/1     Pending   0          13s		
											^ <= It's pending cuz no workers to deploy
------------------------------------------
	# hostname.yaml
------------------------------------------
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hostname-v1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hostname-v1
        version: v1
    spec:
      containers:
      - image: rstarmer/hostname:v1
        imagePullPolicy: Always
        name: hostname
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hostname-v1
  name: hostname-v1
spec:
  ports:
  - name: web
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hostname-v1
----------------------------------------

-------------------------------------------------------------------------------
# CloudFormation		<= for WORKER NODES Addition
# Adding Wokers Nodes(No-Labeled Nodes) to EKS 
-------------------------------------------------------------------------------

AWS -> CloudFormation 
	-> Create a New stack 
	-> Specify an Amazon S3 template URL
	   https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2018-08-30/amazon-eks-nodegroup.yaml
	-> Stack Name: eksNolabeledWorkerNodes
	-> Cluster Name: noLabeledCluster
	-> ClusterControlPlaneSecurityGroup: Learning-EKS-VPC
	-> NodeGroupName: noLabeledworkerNodes
	-> NodeAutoScaling size: 1~3				<= Min ~ max size of Node Group AutoScaleGroup.
	-> NodeVolumeSize: 20 						<= default
	-> NodeImageId: ami-0c28139856aaf9c3b		<= us-west-2(Oregon)<- ESK Optimized AMI
	-----------------------------------------------------------------------
	# Amazon EKS-Optimized AMI List
	https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html
	-----------------------------------------------------------------------
	-> KeyName:eksAdmin				<= EC2 access PEM key (EC2 -> Create a key Pair)

	-> Worker Network Configuration
		VpcId	: vpc-xxx(192.168.0.0/16..)					<= EKS' classCluster VPC
		Subnets	: subnet1(192.168.64.0./18...)				<= EKS' classCluster subnets
				  subnet2(192.168.128.0./18...)
				  subnet3(192.168.192.0./18...)			

	-> Options:  											<= pass on this		
	-> Check on Agree then Click on Create!
	
	Status: eksNolabeledWorkerNodes 2019-03-04 CREATE_IN_PROGRESS NOT_CHECKED Amazon EKS - Node Group 

-------------------------------------------------------------------------------
# After 'No-Labeled Workder Nodes' creatation is done, get eksNolabeledWorkerNodes' 
	'ARN' and add to 'rolearn' in order to the Kubectl accesses the 'eksNolabeledWorkerNodes'
    -> CouldFormation 
	-> Stacks 
	-> chose 'eksNolabeledWorkerNodes' 
	-> Stack Details 
	-> Outputs 
	-> copy value : arn:aws:iam::6...2:role/eksNolabeledWorkerNodes-NodeInstanceRole
	
-------------------------------------------------------------------------------
 $ vi aws-auth-cm.yaml
-------------------------------------------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: arn:aws:iam::6...2:role/eksNolabeledWorkerNodes-NodeInstanceRole-14RJGE47079RC
    #- rolearn: <ARN of instance role (not an instance profile)>
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
-------------------------------------------------------------------------------
$ kubectl apply -f aws-auth-cm.yaml
  Result => configmap/aws-auth created

$ kubectl config view

$ kubectl get nodes
	NAME                                            STATUS   ROLES    AGE   VERSION
	ip-192-168-120-42.us-west-2.compute.internal    Ready    <none>   58s   v1.11.5
	ip-192-168-165-134.us-west-2.compute.internal   Ready    <none>   1m    v1.11.5
	ip-192-168-201-63.us-west-2.compute.internal    Ready    <none>   1m    v1.11.5

-------------------------------------------------------------------------------
# Now it is deployed into the Woker Nodes and so as the POD
-------------------------------------------------------------------------------
$ kubectl get pods
	NAME                           READY   STATUS    RESTARTS   AGE
	hostname-v1-56bc754656-njdgp   1/1     Running   0          1h

-------------------------------------------------------------------------------
# AutoScalingGroup_Policy
-------------------------------------------------------------------------------
	AWS -> EC2 
		-> AutoScaling 
		-> Auto Scaling Group 
		-> Choose: eksNolabeledWorkerNodes
		-> Scaling Policies  
	------------------------------------------------------------------
	Name 	Launch Config.. 	 Instances Desired Min Max Availability Zones
	eks..	eksNolabeledWorkerNodes  3		3      1    3  us-west-02a,b,c
	------------------------------------------------------------------
	Add Policy  -> Name: scale-cpu-70 
				-> metric type: Ave CPU
				-> Target Value: 70
				-> Instances need: none
				- Disable scale-in: none
				
	>>> CREATE <<<
  ------------------------------------------------------------------------------------	
  # After AutoScalePolicy creation, it immediatly reduces number of Nodes from 3 to 1
  ------------------------------------------------------------------------------------
	$ kubectl get nodes
	NAME                                           STATUS   ROLES    AGE   VERSION
	ip-192-168-120-42.us-west-2.compute.internal   Ready    <none>   29m   v1.11.5
	ip-192-168-201-63.us-west-2.compute.internal   Ready    <none>   29m   v1.11.5

	
	$ kubectl get nodes
	NAME                                           STATUS   ROLES    AGE   VERSION
	ip-192-168-120-42.us-west-2.compute.internal   Ready    <none>   30m   v1.11.5 <= Only 1

-------------------------------------------------------------------------------
# Differenciate Workers - Video 12
# Auto Labeling Nodes during Auto Scaling since number of nodes are keep changing 
  manually labeling nodes update is impossible
-------------------------------------------------------------------------------	

	Manual update: $ kubectl lab nodes <node-name> <label-key>=<label-value>
	
-------------------------------------------------------------------------------
AWS -> CloudFormation 
	-> Create Stack 
	-> Specify template 
	-> Template source 
	-> Next
	-> S3 template
	https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2018-08-30/amazon-eks-nodegroup.yaml

--------------------------------------------
# checking EKS Cluster Name
--------------------------------------------

	$ aws eks list-clusters
	----------------------------------------
	{
		"clusters": [
			"classEKS"    <== USE THIS Cluster Name !!
		]
	}
--------------------------------------------
  # Specify stack details
--------------------------------------------
	Stack name: labeledNodes
	
	EKS Cluster
		ClusterName: classEKS				<===!!!  If wrong, CAN't JOIN EKS Cluster!!!!
		ClusterControlPlaneSecurityGroup: Learning-EKS-VPC....3a)
	
	Worker Node Configuration
		NodeGroupName: labeledNodes
		NodeAutoScalingGroupMinSize: 1
		NodeAutoScalingGroupMaxSize: 3
		NodeInstanceType: t2.small
		NodeImageID: ami-0c28139856aaf9c3b		<= us-west-2(Oregon)<- ESK Optimized AMI
		KeyName: eksAdmin						<= Key Pair
		
		# A script pass to the argument and add '--node-labels' for labeling #
		BootstrapArguments:--kubelet-extra-args '--node-labels "nodetype=generalpurpose"'	
			
	
	Worker Network Configuration
		VpcId:vpc-xxx(192.168.0.0/16..)
		Subnets: subnet1(192.168.64.0./18...)
				 subnet2(192.168.128.0./18...)
				 subnet3(192.168.192.0./18...)	

	# Go to labelNodes -> Outputs -> get the ARN 
	
-------------------------------------------------------------------------------				 
# Edit and Add 'Label Nodes ARN' to aws-auth-cm+NodesLabel.yaml
-------------------------------------------------------------------------------				 
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
	# No-Labeled Nodes Cluster
    - rolearn: arn:aws:iam::688595016292:role/eksWorkerNodesStack-NodeInstanceRole-14RJGE47079RC
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
	# Labeled Nodes Enabled Cluster	
    - rolearn: 	arn:aws:iam::688595016292:role/labelNodes-NodeInstanceRole-8QKQNDGMVNQ5
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes		
-------------------------------------------------------------------------------	
$ kubectl apply -f aws-auth-cm+NodesLabel.yaml
# System should apply for TWO different ROLES 'No-Labeled' and 'Labeled' Clusters

$ kubectl get nodes
	  NAME                                            STATUS   ROLES    AGE   VERSION
Labeled	=> ip-192-168-112-219.us-west-2.compute.internal   Ready    <none>   8m    v1.11.5
Labeled	=> ip-192-168-137-151.us-west-2.compute.internal   Ready    <none>   7m    v1.11.5
Labeled	=> ip-192-168-199-194.us-west-2.compute.internal   Ready    <none>   6m    v1.11.5

No-Labeled=> ip-192-168-120-42.us-west-2.compute.internal    Ready    <none>   19h   v1.11.5

-------------------------------------------------------------------------------	
# $ vi hostnameLabeled.yaml
-------------------------------------------------------------------------------	
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hostname-v2
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hostname-v2
        version: v2
    spec:
      containers:
      - image: rstarmer/hostname:v2
        imagePullPolicy: Always
        name: hostname
      nodeSelector:								# <=
        nodetype: generalpurpose				# <= 
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hostname-v2
  name: hostname-v2							#Labeled as hostname-v2
spec:
  ports:
  - name: web
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hostname-v2
-------------------------------------------------------------------------------	
$ kubectl apply -f hostnameLabeled.yaml

$ kubectl get pods

				NAME                           READY   STATUS    RESTARTS   AGE
No-Labeled 	=>	hostname-v1-56bc754656-j8mwh   1/1     Running   0          19h
Labeled 	=>	hostname-v2-6499cb8cc8-w6w4s   1/1     Running   0          1m			
				 v2 <= renamed with V2

-------------------------------------------------------------------------------
$ kubectl describe pod hostname-v2-6499cb8cc8-w6w4s

Name:               hostname-v2-6499cb8cc8-w6w4s
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               ip-192-168-199-194.us-west-2.compute.internal/192.168.199.194
Start Time:         Wed, 06 Mar 2019 11:38:03 -0800
Labels:             app=hostname-v2						## Labels
                    pod-template-hash=2055764774
                    version=v2
Annotations:        <none>
Status:             Running
IP:                 192.168.201.177
Controlled By:      ReplicaSet/hostname-v2-6499cb8cc8
Containers:
  hostname:
    Container ID:   docker://61ce17272e9
    Image:          rstarmer/hostname:v2
    Image ID:       docker-pullable://rstarmer/hostname@sha256:e0
    Port:           <none>
    Host Port:      <none>
    State:          Running
    Started:        Wed, 06 Mar 2019 11:38:10 -0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rgwhg (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-rgwhg:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-rgwhg
    Optional:    false
QoS Class:       BestEffort


------------------------------------------
Node-Selectors:  nodetype=generalpurpose		<= This is what we are looking for
------------------------------------------
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From                    Message
  ----    ------     ----   ----                    -------


-------------------------------------------------------------------------------
# Creating Storage Class - Video 14
	Read: https://kubernetes.io/docs/concepts/storage/storage-classes/#aws-ebs
	Storage types: io1, gp2, sc1, st1. See AWS docs for details. 
	Default: gp2 <= General Purpose Storage
-------------------------------------------------------------------------------
- By default EKS doesn't have any storage classes defined, and we need to 
	have a storage class model in order to be able to create 'persistent storage.'
=> Simply enable the 'storage class connection' to the underlying EBS service.

- Create a "standard" EBS volume to set as DEFAULT.

----------------------------------------------------
	# GP2 - General Purpose Storage
----------------------------------------------------

	$ kubectl apply -f generalPurpose-storage.yaml

----------------------------------------------------
	$ vi generalPurpose-storage.yaml
----------------------------------------------------
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gp2
  annotations:
    storageclass.kubernetes.io/is-default-class: 'true'
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2			
reclaimPolicy: Retain			<= Persistent EBS Volume Retain
mountOptions:
  - debug

----------------------------------------------------
# Fast Storage
# Creating some Fast (100 iops/GB) SSD Storage
----------------------------------------------------
	
	$ kubectl apply -f fast-storage.yaml
	
----------------------------------------------------
$ vi fast-storage.yaml
----------------------------------------------------
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast-100
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1							<= io1 type
  iopsPerGB: "100"					<= 20,000 iops is Max
reclaimPolicy: Retain				<= Persistent EBS Volume Retain
mountOptions:
  - debug
-------------------------------------------------------------------------------
$ kubectl create -f fast-storage.yaml

	storageclass.storage.k8s.io/fast-100 created

$ kubectl create -f generalPurpose-storage.yaml		

	Error from server (AlreadyExists): error when creating 	
	"generalPurpose-storage.yaml": storageclasses.storage.k8s.io "gp2" already exists


$ kubectl get storageclasses
	NAME            PROVISIONER             AGE
	fast-100        kubernetes.io/aws-ebs   1m
	gp2 (default)   kubernetes.io/aws-ebs   22h

-------------------------------------------------------------------------------
# Storage Persistent Claim	- Video 15 
# Mapping Storage
-------------------------------------------------------------------------------


-------------------------------------------------------------------------------
	$ vi hostname-volume.yaml
-------------------------------------------------------------------------------
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hostname-volume
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hostname-volume
        version: v1
    spec:
      volumes:
      - name: hostname-pvc					#PVC - persist volume claim
        persistentVolumeClaim:
          claimName: hostname-pvc
      containers:
      - image: rstarmer/hostname:v1
        imagePullPolicy: Always
        name: hostname
        volumeMounts:
          - mountPath: "/www"
            name: hostname-pvc
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hostname-volume
  name: hostname-volume
spec:
  ports:
  - name: web
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hostname-volume
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: hostname-pvc
spec:
  storageClassName: gp2
  accessModes:
    - ReadWriteOnce					# Can't share 'ReadWriteOnce' 
  resources:
    requests:
      storage: 1Gi
-------------------------------------------------------------------------------

$ kubectl apply -f hostname-volume.yaml

	deployment.extensions/hostname-volume created
	service/hostname-volume created
	persistentvolumeclaim/hostname-pvc created

$ kubectl get pv			# Persistent Volume

  NAME     CAPACITY  ACCESS MODES  RECLAIM POLICY STATUS CLAIM             STORAGECLASS   AGE
  pvc-60.d4  1Gi      RWO            Delete       Bound  default/hostname-pvc   gp2       19s

$ kubectl get pvc -o wide

  NAME           STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
  hostname-pvc   Bound    pvc-...   1Gi        RWO            gp2            3m

-------------------------------------------------------------------------------

$ kubectl get pod -l app=hostname-volume -o jsonpath={.items..metadata.name}

  hostname-volume-8479ffdd6f-nhjv2

$ kubectl exec -it $(kubectl get pod -l app=hostname-volume -o jsonpath={.items..metadata.name}) -- df -h /www

	Filesystem      Size  Used Avail Use% Mounted on
	/dev/xvdbr      976M  2.6M  958M   1% /www

-------------------------------------------------------------------------------
### Clean Up + Persistent Volumes ###
-------------------------------------------------------------------------------
clean up hostname-volume "Stack" with:

$ kubectl delete -f hostname-volume.yaml
-------------------------------------------------------------------------------
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hostname-volume
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: hostname-volume
        version: v1
    spec:
      volumes:
      - name: hostname-pvc
        persistentVolumeClaim:
          claimName: hostname-pvc
      containers:
      - image: rstarmer/hostname:v1
        imagePullPolicy: Always
        name: hostname
        volumeMounts:
          - mountPath: "/www"
            name: hostname-pvc
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hostname-volume
  name: hostname-volume
spec:
  ports:
  - name: web
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hostname-volume
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: hostname-pvc
spec:
  storageClassName: gp2
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
-------------------------------------------------------------------------------
 
$ kubectl delete -f hostname-volume.yaml

	deployment.extensions "hostname-volume" deleted
	service "hostname-volume" deleted
	persistentvolumeclaim "hostname-pvc" deleted

# To Check  EC2 -> Volume 

  kubernetes-dynamic-pvc-60069873-..

$ kubectl get pv

  No resources found.

$ kubectl delete pv $(kubectl get pv -o jsonpath={.items..metadata.name})


-------------------------------------------------------------------------------
# Networking  (File > 03-05)
# Video 18
-------------------------------------------------------------------------------
Networking in EKS uses the 'VPC-CNI' project to use the AWS VPC network model 
to provide connectivity across the cluster.  
This is more efficient than having another layer of networking (e.g. Flannel, 
Calico, Weave, etc.) deployed as an overlay on top of the system, and maps 
perfectly into the VPC environment, using the VPC network management and IPAM 
services to support address management further improving the efficiency of 
the overall Kubernetes deployment.
-----------------------------------------------------------
# Deploy Alpine image -	A minimal Docker image based on Alpine Linux with 
						a complete package index and only 5 MB in size!
-----------------------------------------------------------
$ kubectl run --image alpine alpine sleep 3600 #(1hr)

	kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed 
	in a future version. 
	Use 'kubectl run --generator=run-pod/v1' or 'kubectl create' instead.
	deployment.apps/alpine created

$ kubectl get pods
	NAME                           READY   STATUS    RESTARTS   AGE
	alpine-6b9858595b-7zncb        1/1     Running   0          1m	<= Alpine image
	hostname-v1-56bc754656-j8mwh   1/1     Running   0          1d
	hostname-v2-6499cb8cc8-w6w4s   1/1     Running   0          5h

# Finding IP Addresses
$ IPs=`kubectl get pod $(kubectl get pod -l run=alpine -o \
	  jsonpath={.items..metadata.name}) -o yaml | awk '/IP/ {print $2}'`

$ echo $IPs
	192.168.137.151 192.168.133.249

$ for i in $IPs; do kubectl exec -it $(kubectl get pod -l run=alpine -o \
	jsonpath={.items..metadata.name})  traceroute $i ; done

	traceroute to 192.168.137.151 (192.168.137.151), 30 hops max, 46 byte packets
		1  192.168.137.151 (192.168.137.151)  0.006 ms  0.007 ms  0.002 ms
	
	traceroute to 192.168.133.249 (192.168.133.249), 30 hops max, 46 byte packets
		1  alpine-6b9858595b-7zncb (192.168.133.249)  0.005 ms  0.002 ms  0.002 ms

-------------------------------------------------------------------------------
# Load Balancing and Ingress( Adding ingress is a Kubernetes function)
# Adding 'traefik' Loadbalancer
# video-19 (File 03-06)
-------------------------------------------------------------------------------
	We'll add the 'Traefik load balancer' as an ingress function, and make use of 
	the EKS integration with Amazon ELB to enable external access.

	As ingress can route based on DNS, we can also do a little DNS manipulation to 
	get traffic routed to our resources.

1) Since we're using 1.10.0 Kubernetes(or newer) we'll need to make sure we have 
   a 'cluster role binding' for the services to use(Much like IAM):
   
   # RBAC <= Role-based access control  

$ kubectl apply -f https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-rbac.yaml
#-----------------------------------------------------
### Cluster Roles and Role Binding (Much like IAM) ###
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: traefik-ingress-controller
rules:
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
    - extensions
    resources:
    - ingresses/status
    verbs:
    - update
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: traefik-ingress-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-ingress-controller
subjects:
- kind: ServiceAccount
  name: traefik-ingress-controller
  namespace: kube-system
#-----------------------------------------------------
	#Result
	clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created
	clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created


2) Turn on traffic deployment
	We'll leverage the deployment model for our ingress controller, as we don't 
   necessarily want to bind host address, and would rather have the ingress transit 
   through the normal 'kube-proxy' functions (note that we're changing the default 
   "NodePort" type to "LoadBalancer"):

$ kubectl apply -f <(curl -so - https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-deployment.yaml\
					| sed -e 's/NodePort/LoadBalancer/')
					
#-----------------------------------------------------					
### Turn on traffic deployment ### 
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
---
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: traefik-ingress-lb
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      serviceAccountName: traefik-ingress-controller
      terminationGracePeriodSeconds: 60
      containers:
      - image: traefik
        name: traefik-ingress-lb
        ports:
        - name: http
          containerPort: 80
        - name: admin
          containerPort: 8080
        args:
        - --api
        - --kubernetes
        - --logLevel=INFO
---
kind: Service
apiVersion: v1
metadata:
  name: traefik-ingress-service
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
    - protocol: TCP
      port: 80
      name: web
    - protocol: TCP
      port: 8080
      name: admin
  type: NodePort
#-----------------------------------------------------
  # Result
  serviceaccount/traefik-ingress-controller created
  deployment.extensions/traefik-ingress-controller created
  service/traefik-ingress-service created

-----------------------------------------------------
# Get Services info
-----------------------------------------------------
  $ kubectl get svc

	NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
	hostname-v1   ClusterIP   10.100.81.10    <none>        80/TCP    1d
	hostname-v2   ClusterIP   10.100.148.78   <none>        80/TCP    5h
	kubernetes    ClusterIP   10.100.0.1      <none>        443/TCP   1d

  # Get system service
  $ kubectl get svc -n kube-system

	NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP                     	   PORT(S)                       AGE
	kube-dns                   ClusterIP      10.100.0.10    <none>                          	   53/UDP,53/TCP                 1d
	traefik-ingress-service    LoadBalancer   10.100.35.58   a63..714.us-west-2.elb.amazonaws.com  80:32575/TCP,8080:30184/TCP   2m
	   ^ <= We're looking for  ^ <=Changed NodePort to LB

-----------------------------------------------------
# Get the Load Balancer service address
-----------------------------------------------------

$ export INGRESS=`kubectl get svc -n kube-system traefik-ingress-service -o jsonpath={.status.loadBalancer.ingress[0].hostname}`

$ echo $INGRESS
	a63d...714.us-west-2.elb.amazonaws.com			<= Same as 
			^ <= Same as: traefik-ingress-service    LoadBalancer   10.100.35.58   a63..714.us-west-2.elb.amazonaws.com  

$ export INGRESS_ADDR=`host $INGRESS | head -1 | cut -d' ' -f 4`
$ echo $INGRESS_ADDR
	52.25.57.209

$ host $INGRESS
	a63dff7ec407411e999b70ac8c63844e-2061030714.us-west-2.elb.amazonaws.com has address 52.25.57.209
	a63dff7ec407411e999b70ac8c63844e-2061030714.us-west-2.elb.amazonaws.com has address 34.217.247.157
	a63dff7ec407411e999b70ac8c63844e-2061030714.us-west-2.elb.amazonaws.com has address 52.36.19.123


# We can now expose our hostname app as an ingress resource:

$ kubectl create -f hostname-ingress.yaml

------------------------------------------------------
# hostname-ingress.yaml
------------------------------------------------------
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: hostname-ingress					# hostname-v1
  namespace: default
spec:
  rules:
  - host: hostname-v1.local
    http:
      paths:
      - path: /
        backend:
          serviceName: hostname-v1			# Pointing at
          servicePort: web
------------------------------------------------------

  # Result 
    ingress.extensions/hostname-ingress created


------------------------------------------------------
3) Add an entry to the local Linux server's /etc/hosts file to point to our resource:
------------------------------------------------------

$ echo "$INGRESS_ADDR hostname-v1.local" | sudo tee -a /etc/hosts
  
  52.25.57.209 hostname-v1.local

$ cat /etc/hosts
  127.0.0.1    localhost localhost.localdomain ...
  ::1          localhost localhost.localdomain ...
  52.25.57.209 hostname-v1.local

$ curl -v http://hostname-v1.local
  * About to connect() to hostname-v1.local port 80 (#0)
  *   Trying 52.25.57.209...
  * Connected to hostname-v1.local (52.25.57.209) port 80 (#0)
  > GET / HTTP/1.1
  > User-Agent: curl/7.29.0
  > Host: hostname-v1.local
  > Accept: */*


# Curl Data Flow

1. 	=> $Curl to Exlternal Address(52.25.57.209) with 'hostname-v1.local' 
	=> Map throught Ingress Controller
	=> AWS ELB 
	=> EKS Cluster 
	=> EKS Cluster Ingress 
	=> Talks to host-v1 service 
	=> host-v1 service Points to Deployment 
	=> Points to the POD
	
	
	
	
------------------------------------------------------
------------------------------------------------------
# Network policy with Calico
# Video 21,  file 04-01
------------------------------------------------------
------------------------------------------------------
In order to enable VPC Layer Network Policy, a CNI(Container Network Interface) network needs to be in place, 
and by default the VPC based networking in EKS is already configured appropriately.  

Policy however is not part of the VPC networking provided by Amazon, and instead, 
an integration with the Calico policy manager has been integrated with the VPC CNI service.

# calico CNI policy manifest from the Amazon VPC CNI project:
# Install Calico Extension top of VPC
# This will create a daemonset running the calico policy engine on each configured node.

$ kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.1/config/v1.1/calico.yaml

#------------------------------------------------------
# https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.1/config/v1.1/calico.yaml
---
kind: DaemonSet
apiVersion: extensions/v1beta1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        k8s-app: calico-node

   ........... To many lines

---
apiVersion: v1
kind: Service
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  ports:
    - port: 5473
      protocol: TCP
      targetPort: calico-typha
      name: calico-typha
  selector:
    k8s-app: calico-typha
#------------------------------------------------------

  # Result
	daemonset.extensions/calico-node created
	
	error: error validating "https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/release-1.1/config/v1.1/calico.yaml": 
	error validating data: ValidationError(CustomResourceDefinition): unknown field "description" in io.k8s.apiextensions-
	apiserver.pkg.apis.apiextensions.v1beta1.CustomResourceDefinition; if you choose to ignore these errors, turn validation 
	off with --validate=false
	??????????????????????????????  too many errors
	
	
   # Let's run a container with curl enabled to test our target system 
	(hostname-v1 from the initial install):
	
	$ kubectl run --image rstarmer/curl:v1 curl

	  kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed 
	  in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
	
	  => deployment.apps/curl created

	
  # Verify <= communicate to the http://hostname-v1 service endpoint:

	$ kubectl exec -it $(kubectl get pod -l run=curl -o jsonpath={.items..metadata.name})  \
	          -- curl --connect-timeout 5 http://hostname-v1
	# Output		  
	------------------------------------------------------
	<HTML>
	<HEAD>
	<TITLE>This page is on hostname-v1-56bc754656-j8mwh and is version v1</TITLE>
	</HEAD><BODY>
	<H1>THIS IS HOST hostname-v1-56bc754656-j8mwh</H1>
	<H2>And we're running version: v1</H2>
	</BODY>
	</HTML>
	------------------------------------------------------
	
	$kubectl get pods
	NAME                           READY   STATUS    RESTARTS   AGE
	alpine-6b9858595b-7zncb        1/1     Running   23         23h
	curl-66df598c4-zwz5v           1/1     Running   0          4m
	hostname-v1-56bc754656-j8mwh** 1/1     Running   0          1d		<= This is responding
	hostname-v2-6499cb8cc8-w6w4s   1/1     Running   0          1d

  #------------------------------------------------------
  # DENY All Acess
  #------------------------------------------------------
	$ vi deny-all.yaml
#------------------------------------------------------
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-all
  namespace: default
spec:
  podSelector:
    matchLabels: {}		<= Nothing match, so no access
#------------------------------------------------------

	$ kubectl apply -f default-deny.yaml
		networkpolicy.networking.k8s.io/default-deny created 	<= Confirm

	
	# Verify
	
	$ kubectl exec -it $(kubectl get pod -l run=curl -o jsonpath={.items..metadata.name})  \
	          -- curl --connect-timeout 5 http://hostname-v1

	# curl:(28)Connection time out after 5001 milliseconds <= Should be 
	

  #------------------------------------------------------
  # Allow All Acess
  #------------------------------------------------------

	$ kubectl apply -f allow-all.yaml


# $ vi  allow-all.yaml

#------------------------------------------------------
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
  namespace: default
  name: allow-all
spec:
  podSelector:
    matchLabels:
      app: hostname-v1				<= Allow to this host
  ingress:
    - from:
        - namespaceSelector:
            matchLabels: {}
#------------------------------------------------------


$ kubectl exec -it $(kubectl get pod -l run=curl -o jsonpath={.items..metadata.name})  \
	-- curl --connect-timeout 5 http://hostname-v1

<HTML>
<HEAD>
<TITLE>This page is on hostname-v1-56bc754656-j8mwh and is version v1</TITLE>
</HEAD><BODY>
<H1>THIS IS HOST hostname-v1-56bc754656-j8mwh</H1>
<H2>And we're running version: v1</H2>
</BODY>
</HTML>

#------------------------------------------------------
# AWS IAM and K8s RBAC(Role Based Access Control)
# Video 22 | file 04-02
#------------------------------------------------------
AW IAM(Tokens) <--> AWS-IAM-Authenticator(Local) <--> Kubectl CLI

  Users within the EKS environment are authenticated through AWS IAM, which provides enhanced security.  
  If we add our 'clusterUser' credentials to the local aws client, we will see that kubernetes 
  will still try to talk to the API, but will fail:

# clusterUser Info
	User name: 			clusterUser
	Access key ID: 		AKIAIN...Y7ABA
	Secret access key:	j8/8ar...Rmgf2
	
$ cat ~/Downloads/credentials-clusterUser.csv
$ aws configure --profile=clusterUser
$ export AWS_PROFILE=clusterUser
$ env | grep AWS
	AWS_PROFILE=clusterUser
	
$ kubectl get pods		<= This will and should FAIL!!!
	error: the server doesn't have a resource type "pods"  <= Cuz of Permission

# Change to clusterAdmin profile to work again
	
$ export AWS_PROFILE=clusterAdmin

$ kubectl get pods
	-----------------------------------------------------------------
	NAME                           READY   STATUS    RESTARTS   AGE
	alpine-6b9858595b-7zncb        1/1     Running   24         1d
	curl-66df598c4-zwz5v           1/1     Running   0          54m
	hostname-v1-56bc754656-j8mwh   1/1     Running   0          2d
	hostname-v2-6499cb8cc8-w6w4s   1/1     Running   0          1d
	-----------------------------------------------------------------

$ export AWS_PROFILE=clusterAdmin

$ kubectl edit configmap aws-auth -n kube-system

  Adding additional users to the kubernetes cluster in EKS is done by adding new
  users to the "system:masters" group which maps to the equivalent of the ClusterAdmin 
  role in Kubernetes RBAC rules.

  The key parameter we need is the User's IAM ARN, which can be pulled from the 
  User IAM page in the AWS console:

---------------------------------------------
data:
  mapUsers: |
    - userarn: USER-ARN
      username: admin
      groups:
        - system:masters
---------------------------------------------
		
  We need to add the mapUsers: section to the aws-auth-cm.yaml document, and we 
  can do that either locally and "apply" the changes, or we can edit the document 
  in place in the kubernetes service.

We will edit the file in place, as we don't want to have to recreate the 
worker node role mappings which are part of the same auth structure:

$ export AWS_PROFILE=clusterAdmin
$ kubectl edit configmap aws-auth -n kube-system

Once we're done with the edit, we can switch back to our 'clusterUser' and we 
should have access to the system:

$ export AWS_PROFILE=clusterUser
$ kubectl get pods

Add the mapUsers: section right after the data: key, above the mapRoles: | line. 
It should look similar to the aws-auth-cm.yaml document.
---------------------------------------------
$ vi aws-auth-cm.yaml
---------------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapUsers: |
    - userarn: USER-ARN
      username: admin
      groups:
        - system:masters		<= Master Group allows to access the EKS
  mapRoles: |
    - rolearn: NODE-ROLE-ARN
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
    - rolearn: LABEL-NODE-ROLE-ARN
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
---------------------------------------------



#------------------------------------------------------
#------------------------------------------------------
# Prometheus Install on LOCAL PC
# Video 24 | file 05-02
#------------------------------------------------------
#------------------------------------------------------

Install on CentOS7
$ wget https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz
$ tar -xzvf helm-v2.11.0-linux-amd64.tar.gz
$ sudo cp linux-amd64/helm /usr/local/bin/

---------------------------------------------
$ vi helm-rbac.yaml
---------------------------------------------
# Create a service account for Helm and grant the cluster admin role.
# It is assumed that helm should be installed with this service account
# (tiller).
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: kube-system
---------------------------------------------

$ kubectl create -f helm-rbac.yaml
	serviceaccount/tiller created
	clusterrolebinding.rbac.authorization.k8s.io/tiller created

$ sudo cp linux-amd64/helm    /usr/local/bin/

$ helm init --service-account=tiller
------------------------------------------------------------------------------------------
Creating /home/awseks/.helm
Creating /home/awseks/.helm/repository
Creating /home/awseks/.helm/repository/cache
Creating /home/awseks/.helm/repository/local
Creating /home/awseks/.helm/plugins
Creating /home/awseks/.helm/starters
Creating /home/awseks/.helm/cache/archive
Creating /home/awseks/.helm/repository/repositories.yaml
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com
Adding local repo with URL: http://127.0.0.1:8879/charts
$HELM_HOME has been configured at /home/awseks/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.

Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.
To prevent this, run `helm init` with the --tiller-tls-verify flag.
For more information on securing your installation see: 
https://docs.helm.sh/using_helm/#securing-your-helm-installation
https://github.com/helm/helm

Helm      <= is a tool for managing Kubernetes charts. 
   Charts <= are packages of pre-configured Kubernetes resources.
------------------------------------------------------------------------------------------
 
 Once Helm is installed, launching Prometheus is a simple command, though note that we are
 defining the storage class that Prometheus should use to store it's metrics:


$ helm install --name PrometheusEKS --set server.persistentVolume.storageClass=gp2 stable/prometheus

------------------------------------------------------------------------------------------
NAME:   promeks
LAST DEPLOYED: Mon Mar 11 10:44:59 2019
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==> v1beta1/ClusterRole
NAME                                   AGE
promeks-prometheus-kube-state-metrics  1s
promeks-prometheus-server              1s

==> v1beta1/ClusterRoleBinding
promeks-prometheus-kube-state-metrics  1s
promeks-prometheus-server              1s

==> v1beta1/DaemonSet
promeks-prometheus-node-exporter  1s

==> v1beta1/Deployment
promeks-prometheus-alertmanager        1s
promeks-prometheus-kube-state-metrics  1s
promeks-prometheus-pushgateway         1s
promeks-prometheus-server              1s

==> v1/Pod(related)

NAME                                                    READY  STATUS             RESTARTS  AGE
promeks-prometheus-node-exporter-hrh5q                  0/1    ContainerCreating  0         1s
promeks-prometheus-node-exporter-krb62                  0/1    ContainerCreating  0         1s
promeks-prometheus-node-exporter-nd5cz                  0/1    ContainerCreating  0         1s
promeks-prometheus-node-exporter-r5k8p                  0/1    ContainerCreating  0         1s
promeks-prometheus-alertmanager-769c8b64b5-g5hfh        0/2    Pending            0         1s
promeks-prometheus-kube-state-metrics-5b44db6dbc-jx8sf  0/1    ContainerCreating  0         1s
promeks-prometheus-pushgateway-7dcf9b6cd8-wdgd4         0/1    ContainerCreating  0         1s
promeks-prometheus-server-5486d488d5-mrjvl              0/2    Init:0/1           0         1s

==> v1/ConfigMap

NAME                             AGE
promeks-prometheus-alertmanager  1s
promeks-prometheus-server        1s

==> v1/PersistentVolumeClaim
promeks-prometheus-alertmanager  1s
promeks-prometheus-server        1s

==> v1/ServiceAccount
promeks-prometheus-alertmanager        1s
promeks-prometheus-kube-state-metrics  1s
promeks-prometheus-node-exporter       1s
promeks-prometheus-pushgateway         1s
promeks-prometheus-server              1s

==> v1/Service
promeks-prometheus-alertmanager        1s
promeks-prometheus-kube-state-metrics  1s
promeks-prometheus-node-exporter       1s
promeks-prometheus-pushgateway         1s
promeks-prometheus-server              1s


NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
promeks-prometheus-server.default.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=server" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9090


The Prometheus 'alertmanager' can be accessed via port 80 on the following DNS name from within your cluster:
promeks-prometheus-alertmanager.default.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=alertmanager" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9093


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
promeks-prometheus-pushgateway.default.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9091

------------------------------------------------------------------------------------------

# Watch the deployment Progress 
$ kubectl get pods -w  

NAME                                                     READY   STATUS              RESTARTS   AGE
alpine-6b9858595b-7zncb                                  1/1     Running             113        4d17h
curl-66df598c4-zwz5v                                     1/1     Running             8          3d17h
hostname-v1-56bc754656-j8mwh                             1/1     Running             0          5d17h
hostname-v2-6499cb8cc8-w6w4s                             1/1     Running             0          4d22h
promeks-prometheus-alertmanager-769c8b64b5-g5hfh         0/2     ContainerCreating   0          40s
promeks-prometheus-kube-state-metrics-5b44db6dbc-jx8sf   1/1     Running             0          40s
promeks-prometheus-node-exporter-hrh5q                   1/1     Running             0          40s
promeks-prometheus-node-exporter-krb62                   1/1     Running             0          40s
promeks-prometheus-node-exporter-nd5cz                   1/1     Running             0          40s
promeks-prometheus-node-exporter-r5k8p                   1/1     Running             0          40s
promeks-prometheus-pushgateway-7dcf9b6cd8-wdgd4          1/1     Running             0          40s
promeks-prometheus-server-5486d488d5-mrjvl               1/2     Running             0          40s
promeks-prometheus-alertmanager-769c8b64b5-g5hfh   1/2   Running   0     44s
promeks-prometheus-server-5486d488d5-mrjvl   2/2   Running   0     61s
promeks-prometheus-alertmanager-769c8b64b5-g5hfh   2/2   Running   0     76s
curl-66df598c4-zwz5v   0/1   Completed   8     3d18h
curl-66df598c4-zwz5v   1/1   Running   9     3d18h


$ kubectl get nodes
NAME                                            STATUS   ROLES    AGE   VERSION
ip-192-168-112-219.us-west-2.compute.internal   Ready    <none>   4d    v1.11.5
ip-192-168-120-42.us-west-2.compute.internal    Ready    <none>   5d    v1.11.5
ip-192-168-137-151.us-west-2.compute.internal   Ready    <none>   4d    v1.11.5
ip-192-168-199-194.us-west-2.compute.internal   Ready    <none>   4d    v1.11.5


TO expose the Prometheus UI(from Local PC) so that we can have a look at some of the Pod/Container level metrics:

$ kubectl --namespace default port-forward $(kubectl get pods --namespace default -l\
  "app=prometheus,component=server" -o jsonpath="{.items[0].metadata.name}") 9090 &

$ Forwarding from 127.0.0.1:9090 -> 9090
	Forwarding from [::1]:9090 -> 9090


$ links http://localhost:9090


$ kubectl get pod -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP             
alpine-6b9858595b-7zncb         1/1     Running   117        4d    192.168.133.249 
curl-66df598c4-zwz5v            1/1     Running   9          3d    192.168.78.101 

# Finding a Service's IP
$ kubectl get service --all-namespaces





-------------------------------------------------------------------------------

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

-------------------------------------------------------------------------------


-------------------------------------------------------------------------------

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
# Debug	connecting AWS EKS clusterAdmin
  
  # Check Who's ID is used for connection 
  $ aws sts get-caller-identity			<= AWS Security Token Service (STS)
	{
      "UserId": "AIDA...EYRK",
      "Account": "688595016292",
      "Arn": "arn:aws:iam::688595016292:user/clusterAdmin"
	}

  $ kubectl config view
    apiVersion: v1
	clusters:
		- cluster:
			certificate-authority-data: DATA+OMITTED
			server: https://F08A5CEEF5AC38E949E4AA91C2FE028D.sk1.us-west-2.eks.amazonaws.com
			name: arn:aws:eks:us-west-2:688595016292:cluster/classCluster
			......
			
  $ kubectl get svc --v=10				<= --v=5  verbose level 5
  
  # Version Check
  $ aws --version
		aws-cli/1.16.115 Python/3.6.7 Linux/3.10.0-957.1.3.el7.x86_64 botocore/1.12.105
		
  $ kubectl version --short --client
		Client Version: v1.13.4
	
  # RBAC <= Role-based access control (RBAC)

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------













































































































































############################################################################
1. CloudTrail
############################################################################

	a. CloudTrail Viewer Tool
	https://github.com/githublemming/CloudTrailViewer/releases

# Create a S3 bucket	
$aws s3api create-bucket --bucket bnea-cloudtrail-bucket --region us-east-1
{
    "Location": "/bnea-cloudtrail-bucket"
}

# Create a Policy
##############################################################################
{
	"Version": "2012-10-17",
	"Statement": [{
			"Sid": "AWSCloudTrailAclCheck20150319",
			"Effect": "Allow",
			"Principal": {
				"Service": "cloudtrail.amazonaws.com"
			},
			"Action": "s3:GetBucketAcl",
			"Resource": "arn:aws:s3:::bnga-cloudtrail-bucket"
		},
		{
			"Sid": "AWSCloudTrailWrite20150319",
			"Effect": "Allow",
			"Principal": {
				"Service": "cloudtrail.amazonaws.com"
			},
			"Action": "s3:PutObject",
			"Resource": "arn:aws:s3:::bnea-cloudtrail-bucket/[optional prefix]/AWSLogs/myAccountID/*",
			"Condition": {
				"StringEquals": {
					"s3:x-amz-acl": "bucket-owner-full-control"
				}
			}
		}
	]
}
##############################################################################













