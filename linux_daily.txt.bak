아무리 바빠도 해야할 일 - 책
돈이 많아도 하지 말아야 할일 -도박, 마약

1.역사책 - 역사의식 책임
2.문화책 - 감성
3.경영책 - 논리
4.철학책 - 생각에 방법 
5.사회학 인간학 - 인간 관계

-----------------------------------------------------------------------------------
Windows CMD kill process
http://tweaks.com/windows/39559/kill-processes-from-command-prompt
c:\tasklist
c:\taskkill /IM pdflite.exe /F 
# Rename file name  'F2 key'
-----------------------------------------------------------------------------------
Notepad++

Settings > Short cut Mapper Menu.
1. 
	Ctrl + L			Delete Current line
	Ctrl + D			Duplicate(copy) line

2. Auto-completion
	Ctrl+Space			Function completion
	Ctrl+Enter			Word completion
	Ctrl+Shift+Space	Function parameter hint

4. Basic File Management
	Ctrl+N				Create new document.
	Ctrl+P				Display Print... dialog box.
	Ctrl+O				Display Open File... dialog box.
	Ctrl+S				Save current document.
**	Ctrl+Shift+S		Save all open documents

	Ctrl+Alt+S			Display Save As... dialog box.
	
	Ctrl+W				Close current document.
	Alt+F4				Exit Notepad++
-----------------------------------------------------------------------------------
SMACK Stack

Spark - fast and general engine for distributed, large-scale data processing
Mesos - cluster resource management system that provides efficient resource
		isolation and sharing across distributed applications
Akka -  a toolkit and runtime for building highly concurrent, distributed, 
		and resilience message-driven app on the JVM
Cassandra - distributed, highly available DB designed to handle large amounts of data
		across multiple datacenters
Kafka - a high-throughput, low-latency distributed messaging system designed for
		handling real-time 'data feeds'
-----------------------------------------------------------------------------------		
### Chrome ###
Alt + Home				Open your homepage.
Alt + Tab				Toggle between browser windows
Alt + Left Arrow		Back a page.
Alt + Right Arrow		Forward a page.
F11						Full screen
Esc						Stop page or download from loading.
Ctrl + Shift + I		Development Tools*** 
Ctrl + Shift + O		Open the Bookmark manager.
Ctrl + Shift + N		New Incognito Window

Ctrl + H				Open history in a new tab
Ctrl + K or Ctrl + E	Perform a Google search
Ctrl + T				Opens a new tab.
Ctrl + U				View a web page's source code
-----------------------------------------------------------------------------------	
Daily Linux CMD List & Scripts
-----------------------------------------------------------------------------------	

32bit is 2^32 =~ 4.3 billion
IPv4 address is 32bit 

### Enable color & history time stamp ###
echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'" >> ~/.bashrc			<= Green
###
echo 'export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"'   >> ~/.bashrc		<= Green|Red 
###
echo 'export HISTTIMEFORMAT="%F "' >> ~/.bashrc								<= History time stamp
###

http://vimdoc.sourceforge.net/htmldoc/syntax.html#%3ahighlight
### Vi environment set-up (setting)
	Install VIM first 
	$ alias vi='vim

Create a .vimrc file in home 
$ vi .vimrc
	:set num
	:set tabstop=4					<= most of useful
	:set autoindent
	---------------------------------------------------------------------------------------------------
	####	Vim color scheme enable   ###
	http://www.server-world.info/en/note?os=CentOS_7&p=initial_conf&f=7

	1. Install vim-enhanced
	yum -y install vim-enhanced 

	2. Edit profile
	vi /etc/profile
	# add at the last line
	alias vi='vim'

	3. reload
	source /etc/profile 

	4.Configure vim. 
	( Apply to a user below. If you applly to all users, Write the same settings in '/etc/vimrc',  
	some settings are applied by default though. )

	# Add following 
	syntax on
	colorscheme desert
	---------------------------------------------------------------------------------------------------
esc highlight
	shift+v to copy  == yy
	shift+p to paste == pp

vi / vim  set number
	$ vi -V 			<= version check
	$ vi -V 			<= version check
	:set nu / set nu!   <= set number on/off
	:e file_name        <= open & create  file
 	:w new_name 	    <= save as to new_name
	
Cut and Paste(v/V-d/y-p/P)

	 a. Position the cursor where you want to begin cutting.
V    b. Press v to select characters (or V - whole lines).
     c. Move the cursor to the end of what you want to cut.
d    d. Press d to cut (or y to copy).
     e. Move to where you would like to paste.
p    f. Press P to paste before the cursor, or p to paste after.

Undo
	esc and u
	

# Switch user
	$ su user_id					<= just switch user but same current directory
	$ su - user_id					<= go to user's HOME directory
	$ 
	
	
# CD command
	mkdir -p /1/2/3/4/5
	from /1/2/3/  =>  /1   			$ cd ../..
	from /1/2/3   =>  /1/2-1/ 		$ cd ../2-1
	
	$ cd -				<= move to previous directory
	# cd ~-				<= same
	$ echo ~-			<= out put previous directory
	
# Touch command
	$ touch apple banana cherry						<= creating multiple files
	$ touch file_{01..1000}
	$ echo {1..10..2}
	  1 3 5 7 9
	$ echo {A..Z}
	$ echo {A..z}			<= Capital 1st
	$ echo {w..d..2}		<= backward + every 2nd letter
	$ touch cherry_{01..100}{w..d}.txt

# number line	
	$ cat file | nl 

# long command using ' \' for extent command

### Putty, mRemote, Cygwins, Shell, Terminal,
	
3. SSH connection login
	$ ssh -v root@172.16.248.xx > result.txt					<= -v debugging mode
	$ ssh -v root@172.16.248.xx 2>&1 > result.txt	
	$ ssh id@x.x.x.x -p 2222									<= different port
	$ ssh -l ubuntu ip_address (or hostname)									<= -l login user name
	
	# Cipher options
	$ ssh -c aes256-ctr apark@45.55.5.69
	
	# Creating SSH config file and single Ciphers option
	$ vi .ssh/config
	----------------------
	Host *
	Ciphers aes256-ctr
	----------------------
	
	# $ ssh -c 3des-cbc apark@45.55.5.69
	
	$ ssh -Q [cipher | cipher-auth | mac | kex | key]

	
	### Set SSH debugging mode on server
	https://en.wikibooks.org/wiki/OpenSSH/Logging_and_Troubleshooting
	$ systemctl stop sshd 					<= your connection is still live
	$ /usr/sbin/sshd -ddd					<= running debugging mode on screen	

	## Client can't connect
	error msg: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
	Authentication refused: bad ownership or modes for directory .ssh
	
	# Resolution: change permission on user .ssh folder to 700
	$ chown 700 -R .ssh
	




	
	# MD5 (Message-Digest algorithm 5)
	https://help.ubuntu.com/community/HowToMD5SUM
	$ md5sum file_name
	When one has downloaded an ISO file for installing or trying Ubuntu, it is recommended to test 
	that the file is correct and safe to use. The MD5 calculation gives a checksum (called a hash value), 
	which must equal the MD5 value of a correct ISO.
	
	
	$ basename 					<= strip directory and suffix from filenames
	$ basename $0
	

		
	
  #	Install SSH on Ubuntu(Debian)
	$ apt-get install openssh-server openssh-client
	$ update-rc.d ssh defaults			<= put it into start up
	$ update-rc.d ssh enable 			# sets the default runlevels to on 
	$ update-rc.d ssh disable 			# sets all to off

  # login ec2-user
	$ su apark
	$ chmod 700 .ssh

  # Verbose mode using -v
	$ ssh -vvvv nozatech@192.168.221.129						<= Max verbose debugging mode
    
	OpenSSH_6.7p1, OpenSSL 1.0.1k 8 Jan 2015
    debug1: Reading configuration data /etc/ssh_config
    debug1: Connecting to 192.168.221.129 [192.168.221.129] port 22.
    debug1: Connection established.
    debug1: identity file /home/apark/.ssh/id_rsa type 1

	
	PSSH tool includes parallel versions of OpenSSH and related tools such as:

	pssh – is a program for running ssh in parallel on a multiple remote hosts.
	
	pscp – is a program for copying files in parallel to a number of hosts.
		   Copy/Transfer Files Two or More Remote Linux Servers
	
	prsync – is a program for efficiently copying files to multiple hosts in parallel.
	
	pnuke – kills processes on multiple remote hosts in parallel.
	
	pslurp – copies files from multiple remote hosts to a central host in parallel.

	
$ yum install python-pip
$ pip install pssh
	
	
	
	
	
	
	
### Run command on remote shell
	$ ssh apark@192.241.190.57 	  'sudo iptables -nL'					
	$ ssh -t apark@192.241.190.57 'sudo iptables -nL'					<= -t      Force pseudo-tty allocation
	  
	### run script remotely
	
	$ ssh apark@remote_server  'bash -s' < from_local_script.sh			<= run a script remotely from local using sudoer
	$ ssh apark@45.55.5.69     'bash -s' < do_agent.sh                  <= installing as sudo	
	
	
	
	
	
	
4.  RSA KEY( 1024bit, 2k, 4k bit)
	$ ssh-keygen -t rsa							<= Generate  id_rsa (Private Key ) & id_rsa.pub (Public Key)	
	
    $ ssh-copy-id user_id@remote_server			<= Transferring Public Key
	
	$ cat ~/.ssh/id_rsa.pub | ssh user@123.45.56.78 "mkdir -p ~/.ssh && cat >>  ~/.ssh/authorized_keys"

	$ ssh remote_server "cat .ssh/*.pub"


5. WGET to download a file using user_id & PW
	$ wget --user=user_id --password='my_passwd' http://download.com/foo.pdf
	$ wget http://www.gnu.org/software/gettext/manual/gettext.html  or file_name	
	$ time wget http://www.gnu.org/software/gettext/manual/gettext.html  			<= measuring download time

### cURL  
	Testing on REST API
	A. Content of the URL and display it in the STDOUT (i.e on your terminal)
		$ curl http://www.centos.org
	
	B. To store the output in a file, you an redirect it.
		$ curl http://www.centos.org > /tmp/centos-org.html  <= save to file
	
	C. Save the cURL Output to a file
		-o (lowercase o) saved in the file name e
		-O (uppercase O) the file name in the URL will be taken and it will be used as the file name to store the result
		-l --list-only
		-L --location
	  $ curl -o saving_name.html http://www.gnu.org/text_info.html							
	  $ curl -O http://www.gnu.org/software/gettext/manual/gettext.html or file_name		<= save the file to local 
	
	D.  Follow HTTP Location Headers with -L option  
		$ curl -L http://google.com
	
	E.	
		$ curl --limit-rate 1000B -O http:
	
	F. Proxy
		$ curl -p 
	
	G.	$ curl -u username:password URL
	
	H.  Verbose
		$ curl -v http://google.com
	
	I. $ curl dict://dict.org/d:bash
	

	# to get the HEADER info	
	$curl -i http://address:3000/json-test 
  	
	HTTP/1.1 403 Forbidden
	Date: Wed, 19 Oct 2016 02:07:18 GMT
	Server: Apache/2.4.6 (CentOS)
	...
	Content-Length: 4897
	Content-Type: text/html; charset=UTF-8


*C*R*U*D*

POST method
	$ curl -d "first=Alber&last=park" http://address:port/method_name

PUT method
	$ curl -X PUT -d "first=Alber&last=park" http://address:port/method_name

Delete
	$ curl -X DELETE  http://address:port/method_name

	$ curl -u 

	$ curl -X GET -H 
	

### curl can download or upload through any protocol, http, ftp, sftp, scp, ldap, telnet

curl www.yahoo.com -o output.html
curl -O www.yahoo.com/index.html
curl -# -u id:pw ftp://ftphost/fuzicast/sample.txt -o sample.txt 				# the -# adds progress bar
curl -r 0-99 -u id:pw ftp://ftphost/fuzicast/sample.txt							# get first 100 bytes
curl -r -500 -u id:pw ftp://ftphost/fuzicast/sample.txt 						# get last 500 bytes
echo "Hello World" | curl -T - -u id:pw ftp://ftphost/fuzicast/sample2.txt
curl -T UNIX-1.14 -u id:pw ftp://ftphost/fuzicast/unix
curl -T localfile1 servername/remotefile1 -T localfile2 servername/remotefile2
curl -T UNIX-1.14 -u id:pw -a ftp://ftphost/fuzicast/unix 						# append to FTP file
curl --ftp-create-dirs -T UNIX-1.14 -u id:pw ftp://ftphost/fuzicast/unix/test.txt
curl --limit-rate 10240 -u id:pw ftp://ftphost/fuzicast/sample.txt 				# limit number of bytes per second .curlrc # curl configuration file
curl -u id:pw -z sample.txt ftp://ftphost/fuzicast/sample.txt 					# download remotefile only if it's newer than localfile
curl -z "Jan 12 2012" -u id:pw ftp://ftphost/fuzicast/sample.txt 				# download remote file only if it's newer than Jan 12 2012
curl -B -u id:pw ftp://ftphost/fuzicast/sample.txt 								# enforces ASCII transfer during FTP download 
curl -u id:pw ftp://ftphost/fuzicast/sample.txt --create-dirs -o sampledir/sample.txt		# create directory if not exist
curl --key id_rsa # use SSH key
curl -u id:pw ftp://ftphost -Q 'RNFR /fuzicast/sample.txt' -Q 'RNTO /fuzicast/sampleyue.txt'	 # rename a remote file in FTP protocol
curl -u id:pw ftp://ftphost -Q 'rename /fuzicast/sample.txt /fuzicast/sampleyue.txt' 			# rename in SFTP is different from FTP
curl -R -u id:pw ftp://ftphost/fuzicast/sample.txt -o output.txt # reserve original file timestamp
curl -l -u id:pw ftp://ftphost/fuzicast/ 										# list remote filenames
curl -m 1800 -Y 3000 -y 60 servername/filename 									# speed must be greater than 3000 bytes per second for a minute and 
																				download process must be completed within 1800 seconds, otherwise the 
																				download will abort

	
### text browser ###
	$ sudo apt-get install -y links
	$ links http://www.google.com
	
### base64 - base64 encode/decode data and print to standard output
http://stackoverflow.com/questions/201479/what-is-base-64-encoding-used-for

	$ printf id:pw | base64
	https://www.base64decode.org/	
	
	
6. Environment variable
	AWS Access Key/ Secret key
	
	/home/ubuntu/jarvis/ami/tcg/production/
	
    $ sudo sh -c "export OC_AWS_ACCESS_KEY=AKIAI6FBDACL5LFOEACA; \
				export OC_AWS_SECRET_KEY=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; \
				fab tcg.credit_gems:host=10.10.1.50,player_id="544053a5c66354d43f5f07d3",amount=50"
		
	To make a usage listing of the directories in the /home partition.  Note that this runs the commands in a
    sub-shell to make the cd and file redirection work.

    $ sudo sh -c "cd /home ; du -s * | sort -rn > USAGE"

	
4. free  
	
	### Most memory used by process
	
	$ ps aux --sort=-%mem | awk 'NR<=10{print $0}'	

		  <= Check memory
	
	Displays the total amount of free and used physical and swap memory in the system, 
	as well as the buffers used by the kernel. The shared memory column should be ignored; it is obsolete.
		
	$ free -m | xargs | awk '{ print "Free/Total memory " $10 "/" $8 "MB" }'

	$ free -m | grep Mem | awk '{print $4 "/" $2 "MB free"}'

	$ free -tom
         -m MB 
         -t switch displays a line containing the totals.
         -o switch disables the display of a "buffer adjusted" line.  If the -o
            option is not specified, free subtracts buffer memory from the  used
            memory and adds it to the free memory reported.
	$ egrep --color 'Mem|Cache|Swap' /proc/meminfo

5. vmstat   
	virtual memory statistics 
	#reports information about processes, memory, paging, block IO, traps, and cpu activity.	
	
	$ vmstat 1 20 		<= one (1) second twenty (20) times:
	$ vmstat 30 		<= ongoing report for intervals of 30 seconds
	$ vmstat -S k 1 10  <= S-switch unit (k kilobyte, K, m, M)
	$ vmstat 10(sec) 6(times)  	
	$ vmstat -a (active)
		procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
		r  b   swpd   free  inact active   si   so    bi    bo   in   cs us sy id wa st
		1  0      0 600324 140380 175672    0    0     5     2    8   13  0  0 100  0  0
		
		procs	<= umber of processing jobs waiting to run
		memory  <= same as free -m
		swap	<= memory is sent to or retrieved from the swap system
		io		<= input and output activity per second  in terms of blocks read and blocks written
				   bi -block in, bo-block out
		system	<= number of system operations per second
		cpu		<= always add to 100 and reflect “percenDATEe of available time”.	
			
	$ vmstat -s    <=-s switch displays summary of various event counters and memory statistics.
      1009992 K total memory
       410888 K used memory
       176468 K active memory
       140376 K inactive memory

	$ vmstat -d  		<=  -d option display all disks statistics.
	    disk- ------------reads------------ ------------writes----------- -----IO------
				total merged sectors      ms  total merged sectors      ms    cur    sec
		ram0       0      0       0       0      0      0       0       0      0      0
		ram1       0      0       0       0      0      0       0       0      0      0
		ram2       0      0       0       0      0      0       0       0      0      0

		



		
6. Creating swap space in CentOS7
	http://www.cyberciti.biz/faq/linux-add-a-swap-file-howto/
	
	$ swapon -s   #summary
	$ free -m
	
#### Create swap space	#4GB with 1MB bitesize
#--------------------------------------------------------------------------------

dd if=/dev/zero of=/swapfile count=4096 bs=1MiB   
#dd if=/dev/zero of=/swapfile bs=2048 count=512k

sudo chmod 600 /swapfile
mkswap /swapfile
swapon /swapfile

echo "/swapfile  swap  swap  defaults  0 0"  >>  /etc/fstab   <=	# To enable it at boot time, edit /etc/fstab 

#--------------------------------------------------------------------------------	


# How do I verify swap is activated or not?
	$ free -h
	$ less /proc/meminfo
	$ top           									<= check in KiB Swap
	$ cat /proc/swaps
	$ cat /proc/meminfo | grep -i swap
	
	
	### bug, not working	###
#	$ sudo fallocate -l 2G /swapfile   <= 2GB space for swap
#	$ sudo chmod 600 /swapfile
#	$ ls -lh /swapfile
#	$ sudo mkswap /swapfile
#	$ sudo swapon /swapfile
###	
	
7. sysstat	Linux Performance Monitoring package

	Collective CPU usage
	Individual CPU statistics
	Memory used and available
	Swap space used and available
	Overall I/O activities of the system
	Individual device I/O activities
	Context switch statistics
	Run queue and load average data
	Network statistics
	Report sar data from a specific time

$ iperf
	Measures TCP bandwidth; 
	Reports on maximum segment size and maximum transmission unit;
	Support for TCP Window size;
	Multithreaded for multiple simultaneous connections;
	Creates specific UDP bandwidth streams;
	Measures packet loss;
	Measures delay jitter;
	Runs as a service or daemon; and
	Runs under Windows, Linux OSX or Solaris.
	
	$ iperf -s
	$ iperf -c remote_server     				<= -c host


	
https://www.server-world.info/en/note?os=CentOS_7&p=sysstat
$ yum install -y sysstat
$ systemctl start sysstat
$ systemctl enable sysstat

$ sudo cat /etc/cron.d/sysstat
	# Run system activity accounting tool every 10 minutes
	*/10 * * * * root /usr/lib64/sa/sa1 1 1
	# 0 * * * * root /usr/lib64/sa/sa1 600 6 &
	# Generate a daily summary of process accounting at 23:53
	53 23 * * * root /usr/lib64/sa/sa2 -A
	
#	
#	$ vi /etc/default/sysstat   						<= change to true for data collection
# 	$ service sysstat restart
#	$ sar -A > $(date +`hostname`-%d-%m-%y-%H%M.log)    <=save the statistics
	
http://www.tecmint.com/sysstat-commands-to-monitor-linux/
	
	iostat
		displays CPU and I/O statistics of all partitions as shown below.
	$ iostat
		Linux 3.16.0-23-generic (puppet.nozatech.com)   03/24/2016      _x86_64_        (8 CPU)
		avg-cpu:  %user   %nice %system %iowait  %steal   %idle
				0.01    0.01    0.08    0.05    0.00   99.86
		Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
		sda               2.01        36.44        11.34     236588      73606
		dm-0              2.98        33.48        11.34     217373      73592
		dm-1              0.04         0.17         0.00       1080          0
	
	$ iostat -N 			<= With -N (Upper-case) parameter displays only LVM statistics as shown.
	$ iostat -p sda 		<= By default it displays statistics of all partitions, with -p and 
								device name arguments displays only disks I/O statistics for specific device only as shown.
	$ iostat -d				<= -d arguments displays only disks I/O statistics of all partitions as shown.
		
	$ mpstat -P ALL
	
	$ pidstat
	
	$ cifsiostat 
		
		
		
		
		
8.	slabinfo - kernel slab allocator statistics
	cat /proc/slabinfo
	Slab allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects.
	
	
	
	
9. Utility to check real time processing
    A. top 
        $ top -bn 1 			<= Top Batch Number 1			    
								<= capture into single page file  
								<= -b <= batch mode, n <= number, 1 <= count 
		
		$ top -bn 1 | awk "/$1/ {tot =+ \$6; n++} END {print tot\" \"n}"      <=??
		
		
		### Top 5 CPU processes
		
		### Using head & tail
		$ top -bn 1 | head -n 12 | tail -n 6  	<= head displays head of 12 lines 
												   tail from bottom 6 lines ( Displays 7,8,9,10,11,12 lines)
		  
		  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.74 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.13 kworker/u128:0

		### using sed
		$ $top -bn 1 | sed -n '6,12p'

			PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.74 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.14 kworker/u128:0
		
		### Using awk
		$ top -bn 1 | awk '6 <=NR && NR <=12'

			PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.76 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.15 kworker/u128:0

						
		### Top commands short cuts:
	***	1 – To display or hide all other CPU’s
		A - Split into multiple CPU screen
		b - bache mode sending output from top to other programs or to a file
		c – To display or hide command full(absolute) path
		d - display rate from 3sec to any sec
		n - number of iteration 
		l – To display or to hide load average line
		t – To display or to hide task/cpu line
		m – to display or to hide RAM and SWAP details
		s – To change the time interval for updating top results(value is in sec’s)
		R – To sort by PID number
		u — Press u then user name to get only that user process details
		P – To sort by CPU utilization
		M – To sort by RAM utilization

		r – To renice a process, press r then the PID no then the renice value to renice a process.
	***	k – To kill a process, press k then PID number then enter to kill a process
		w – To save the modified configuration permanently.
		
		q – To quit the top command.
	***	h – for getting help on top	

		
	shift + m 		<= list process according to memory usage
	shift + p		<= list the process according to cpu usage
	shift + w		<= for saving the top output in a file(/root/.toprc.)
	shfit + o		<= for sorting the process as per requirement
		
	# User process list	
	$ top -u apark
	
	# quit after 1 iteration
	$ top -n 1 	
		
	# Batch mode( capture as text file)
	$ top -bn 1	 > top_process.txt
	
	$top -d300 -b > top_log.txt
	
	


	
	B. atop
		Atop is an ASCII full-screen performance monitor which can log and report 
		the activity of all server process up to 28days log.
	
		$ atop -a <= sort in order of most active resource.
		$ atop -c <= revert to sorting by cpu consumption (default).
		$ atop -d <= sort in order of disk activity.
		$ atop -m <= sort in order of memory usage
		$ atop -n <= sort in order of network activity
  
    C. htop
		Up/Down arrow keys to select a process, and then you can kill it with the F9 key

10. Disk usage / folder size / disk space
	$ du -hs * | sort -rh | head -5
	$ find / -type f -exec du -Sh {} + | sort -rh | head -n 5
	
	$ du -ah              					<= all total size files & directories, -a all, -h human 
	$ du -ah *   							<= No total size
	$ du -sh /home/noza*					<= total holder size including subfolders
	$ du -sh /home/noza*  | sort -nr		<= Folder list usage, not files
	$ du -sh /home/noza*  | sort -nr		<= Folder list usage, not files
	
######################################################################################
				total       used       free     shared    buffers     cached
	Mem:           482        304        178          0         51        137
	-/+ buffers/cache:        115      **367** <= Actual Free Memory


	$ free -m | awk 'NR==3 {print $4 " MB"}'
	#
	# When thinking about 'how much memory is really being used' :
	# 'used' - ('buffers' + 'cached')
	Actual Memory usage 116MB = 304-(51+137)
	#
	# When thinking about 'how much memory is really free' :
	# 'free' + ('buffers' + 'cached')
	Actual Free memory   336MB = 178+(51+137)
######################################################################################


	$ iotop  				<=check I/O usage


11. Date
	$ date +%F
	  2016-08-11
	$ timestamp=`date +%F`
	
	$ date +%Y-%m-%d-%H-%M-%S
	
	$DATE=`date +%Y-%m-%d-%H-%M-%S`   <= %Y is 2016, %y is 16
	$echo $DATE
	2016-03-24-22-15-39

	$DATE=`date +%H:%M:%S`
	$echo $DATE
	 22:18:35

	# milliseconds since 1-1-1970(Use %3N to truncate the nanoseconds to the 3 most significant digits)
	$ date +%s%3N
	1397392146866 
	 
	$ date +%Y-%m-%d-hour-%H:%M:%S
	$ date +Name:`hostname`/Date:%Y-%m-%d/hour:%H:%M:%S 
	  Name:PC_Name/Date:2016-10-28/hour:11:37:43

	 
12. Route or IP

	$ route
	$ ip route
	
	$ /sbin/route -n					<=Numeric value
	Kernel IP routing table
	Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
	0.0.0.0         192.168.1.1     0.0.0.0         UG    0      0        0 eth0
	192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0

	Add / set-up a new route
	
	$ route add default gw {IP_ADDRESS} {INTERFACE_NAME}
	$ route add default gw 192.168.1.254 eth0
	
	$ ip route add 192.168.1.0/24 dev eth0
	$ ip route add 192.168.1.0/24 via 192.168.1.254
	
	
13. tar (backup) and cron (schedule job) 

	A. Creating tar file 
	# tar -c(create)v(verbose)p(preserve permission)z(compression)f(filename)
	# tar -czvf archive-name.tar.gz directory-name	
	
	$ tar -czvf  backup.tar.gz /home/nozatech
	$ tar -cvpzf backup.tar.gz /(root) --exclude=/mnt /
	$ tar -cvpzf backup.tar.gz /home/nozatech
	
	Extract tar file
	# tar -x(extract)v(verbose)p(permission)z(uncompress)f(file name) backup.tar.gz -C(change to different directory) /recover
	$ tar -xvzf  backup.tar.gz /home/nozatech
	$ tar -xvpzf backup.tar.gz /home/nozatech	
	$ tar -xzvf nexus-3.2.0-01-unix.tar.gz   -C   /opt


	$ tar -zcvf  <= create tar zip file
	$ tar -zxvf  <= extract tar zip file
	$ tar -tf    <= extract tar file
	
#-----------------------------------------------------------------------------------------------	
B. Cron Jobs
	$  crontab -e (first time ask a editor - choose vim)
		* * * * *
		min         hr         day of month       month          day of month          command
		0-59 mins   0-23 hrs   1-31days           1-12           0(sun)-6(sat)
		30	        2     	   * 	              *              2(tuesday)
		
cron job search from command line
* * * * * *
| | | | | | 
| | | | | +-- Year              (range: 1900-3000)         
| | | | +---- Day of the Week   (range: 1(Monday)-7)
| | | +------ Month of the Year (range: 1-12)
| | +-------- Day of the Month  (range: 1-31)
| +---------- Hour              (range: 0-23)
+------------ Minute            (range: 0-59)

# examples 
	0 0 * * 1,3,5   			   <= 12AM at every Mon, Wed, Fri
	@reboot        				   <= Run once, at start up.  Put start up script!!!!
	0 0 1 1 *		@yearly        <= Run once a year
	@annually       @yearly 
	0 0 1 * *		@monthly       <= Run once a month
	0 0 * * 0		@weekly        <= Run once a week
	0 0 * * *		@daily         <= Run once a day
	@midnight       @daily 
	0 * * * *		@hourly        <= Run once an hour
	
	* * * * * /sbin/ping -c 1 192.168.0.1 > /dev/null
	0 0,12 1 */2 * 				   <= every 12am & 12pm on 1st day of every 2nd month
	30 08 10 06 * 				   <= June 10th 8:30AM (every year)
	00 09-18 * * 1-5 			   <= Everyday 9am to 6pm Mon ~ Fri
	
# Install crontab
	$ crontab cron_jobs.txt
	
	###
	$ cat cron_jobs.txt
	* * * * *         /sbin/ping -c 1 192.168.0.1 > /dev/null
	0 0,12 1 */2 *    /cmd				    # every 12am & 12pm on 1st day of every 2nd month
	30 08 10 06 * 	  /cmd					# June 10th 8:30AM
	00 09-18 * * 1-5  /cmd			   		# Everyday 9am to 6pm Mon~Fri
	###
	
	$ crontab -l				<= current user's list of cron jobs 
	$ crontab -e				<= edit cron job
	
	### Finding cron job list for specific user ### 	
	$ crontab -u user_id -l					<= list jobs
	$ crontab -u user_id -e					<= edit
	
	### Finding cron job list for all users ###
	$ for user in $(cut -f1 -d: /etc/passwd); do crontab -u $user -l; done
		
	# permission issue
	$ sudo "for user in $(cut -f1 -d: /etc/passwd); do echo $user; crontab -u $user -l; done"

	
	a. Getting all user id info from /etc/passwd using 
		#cut -f1 -d:  /etc/passwd
    
	b. using crontab utility to check list of cron job list
		$ crontab -u $user -l

	##########################################
	# #!/bin/bash
	# for user in $(cut -f1 -d: /etc/passwd); 
	# do 	
	#    echo $user; crontab -u $user -l; 
	# done
    ##########################################
#-----------------------------------------------------------------------------------------------

	
14. Zipping or archiving
	$ bzip2 ls.txt &
	 [1] 28072
	$ jobs
	 [1]+  Done                    bzip2 ls.txt
	$ ll
	 -rw-r--r--. 1 root root  932 Jun  9 14:42 ls.txt.bz2


		
		
		
15. List files by modified date 
    ls -atlh   			 	 <= -a all, -t modified time, -l per line 
    ls -atlhr   			 <= -r reverse

	$ ls -l
-rwxrw-r--    10    root   root 2048    Jan 13 07:11 afile.exe
	
?UUUGGGOOOS   00  UUUUUU GGGGGG ####    ^-- date stamp and file name are obvious ;-)
^ ^  ^  ^ ^    ^      ^      ^    ^
| |  |  | |    |      |      |    \--- File Size
| |  |  | |    |      |      \-------- Group Name (for example, Users, Administrators, etc)
| |  |  | |    |      \--------------- Owner Acct
| |  |  | |    \---------------------- Link count (what constitutes a "link" here varies)
| |  |  | \--------------------------- Alternative Access (blank means none defined, anything else varies)
| \--\--\----------------------------- Read, Write and Special access modes for [U]ser, [G]roup, and [O]thers (everyone else)
\------------------------------------- File type flag
	
	
	
16. Filesystem type  
	$ df -T						<= disk free
	$ df -Th
	$ df -Tha
		Filesystem              Type         Size  Used Avail Use% Mounted on
		rootfs                  rootfs        18G  1.1G   17G   7% /

###########################################
### Add new hard drive or partition
###########################################
	1. 	$ fdisk -l
		$ ls /dev/sd*      (Ubuntu /dev/xvd*)
		
	2. 	$ fdisk /dev/sdb			<= Create new partitions
		$ command(m for help): p
			p	 print the partition table
		   Device Boot      Start         End      Blocks   Id  System

		$ command(m for help): n
			n   add a new partition
			Partition type:
			p   primary (0 primary, 0 extended, 4 free)
			e   extended
			Select (default p): p
			Partition number (1-4, default 1): 1
			
		$ command(m for help): p
			Device Boot      Start         End         Blocks   Id  System
			/dev/sdb1        2048        10485759     5241856   83  Linux
		
		$ Expert command (m for help): w
		The partition table has been altered!
		
		
		$ fdisk -l
	    Device Boot      Start         End      Blocks   Id  System
		/dev/sdb1            2048    10485759     5241856   83  Linux
		
		$ file -sL /dev/xvd*
		  /dev/xvda1: Linux rev 1.0 ext4 filesystem data  					<= ext4
		
	3. 	$ mkfs.ext3 /dev/sdb1
	    $ mkfs.ext4 /dev/xvdb                                               <= ext4 

	4. 	$ mount -t ext3 /dev/sdb1 /mnt/mysql -rw
	    $ mount -t ext4 /dev/xvdb /puppet/ -rw
		# $ umount /mnt/mysql

	$ cat /etc/fstab					<= check hdd list
	5. 	$ echo '/dev/sdb1  /mnt/mysql  ext3  defaults 0 0' >> /etc/fstab
	
	
	6. $ mount | column -t				<= Check mount list
	   $ cat /proc/mounts
	
	# Mount recovery disk
	$ fdisk -l
	$ mount /dev/xvdf /mnt/dir or /tmp/dir
	
	# Unmount
	$ umount /mnt/dir
	 -l, --lazy              detach the filesystem now, and cleanup all later



	check lsof and fuser
	# fuser - identify processes using files or sockets
	$ fuser -vm /mnt/dir

	
	File descriptor
	https://en.wikipedia.org/wiki/File_descriptor
	
	### lsof ### 					<= list open files
	$ lsof /mnt/dir
	
	# Which processes have this file open
	$ lsof  /var/log/nginx/access.log
		COMMAND   PID  USER    FD   TYPE DEVICE SIZE/OFF      NODE NAME
		nginx   13786  root    5w   REG  253,0        0 135700508 /var/log/nginx/access.log

	# Which files does PID have open?
	$ ps aux | grep nginx
	$ lsof -p nginx_PID

	$ lsof -p 1									<= all files are opened with Init or SystemD
	$ lsof -p `pgrep ABC`
		
	# Where is the Binary for this process?
	$ lsof -p ABC | grep bin
	
	# Which shared Libraries is this program using? (manually upgrading software, i.e. openssl)
	$ lsof -p PID | grep .so
	
	# Where is this thing logging to?
	$ lsof -p ABC | grep log
	
	# Which processes still have this old library open?
	$ lsof grep libname.so
	
	# Which files does user_id have open?
	$ lsof -u user_id
	$ lsof -u user_id -i 					<= network only
	
	# Which process is listening on Port X (or using Protocol Y)?
	$ lsof -i :80
	$ lsof -i tcp
	
	
	
	
	
	
	/dev/mapper/U14--vg-root on / type ext4 (rw,errors=remount-ro)
	
	

	
	What Kernel supports filesystem
	$ cat /proc/filesystems
		nodev   devpts
        ext3
        ext2
        ext4
	$ cat $(which sys-unconfig)
	
	$ df -h    <= report file system disk space usage (Check Disk Free)
	$ df -T   <= file system type
	$ df -i   <= inodes
		Filesystem                Inodes IUsed    IFree IUse% Mounted on
		/dev/mapper/centos-root 18358272 26054 18332218    1% /

	df -h | xargs | awk '{ print $11 " / " $9 " are free" }'

	df -h | grep /dev/mapper | awk '{print $4 "/" $2 "GB is free"}'

	df -h | awk '{print $5}' | grep % | grep -v Use |  sort -n | tail -1 | cut -d "%" -f1 -

 
  169  cat /proc/mounts
  170  ls /mnt
  171  mount -l
  172  history | grep mount
  173  ps aux | grep jarvis
  174  ls -la /
  175  ls -la /mnt/
  176  history
  177  lsblk
  
  178  sudo mount /dev/xvdf /mnt
  179  ls -la /mnt/
  180  sudo vim /etc/fstab
  181  history | grep start
  182  sudo start jarvis-01 && sudo start jarvis-02 && sudo start jarvis-03 && sudo start jarvis-04
  183  less /mnt/log/jarvis-01/jarvis.log
  184  sudo service mongos start
  185  sudo stop jarvis-01 && sudo stop jarvis-02 && sudo stop jarvis-03 && sudo stop jarvis-04 && sudo start jarvis-01 && sudo start jarvis-02 && sudo start jarvis-03 && sudo start jarvis-04
  186  less /mnt/log/jarvis-01/jarvis.log
  187  ps aux | grep nginx
  188  curl 'http://localhost:53213/tcg/api/1/status'
  189  less /mnt/log/jarvis-01/jarvis.log
  190  exit
	
	


17. Count Active connections

	netstat -na | grep ESTA | wc -l				<== n no reverse lookup(quick), t tcp, a all, p program
	netstat -na | grep ESTABLISHED.*sshd		<= sshd connection list
	ps auxwww | grep sshd:						<= sshd connection list
	netstat -plunt								<= which ports are open and which program(application) is listening
	netstat -punt								<= program, udp, no lookup, tcp
	netstat -ntl								<= no lookuop, tcp, LISTEN port

	watch -d -n1 "netstat -ntap | grep ESTA"  		<= Print active connections n1 <= every 1sec
	
	watch -n5 'netstat -ntap | grep ESTA | wc -l'	<= Print active connections n1 <= every 1sec
	
	watch -n 0.1 "dmesg | tail -n $((LINES-6))"
	
	while true; do dmesg -c ; sleep 1 ; done
	
	ss <= socket statistic


18. vmstat - report virtual memory statistics
	- vmstat reports information about processes, memory, paging, block IO, 
	  traps, disks and cpu activity.
	vmstat is a tool that collects and reports data about your system’s memory, 
	swap, and processor resource utilization in real time. It can be used to 
	determine the root cause of performance and issues related to memory use.
	$ vmstat [interval] [count]
	$ vmstat 1 20 	<= 1sec  20 times
	$ vmstat -S k 1 10
		In the default operation, vmstat displays memory statistics in kilobytes. 
		vmstat considers a single kilobyte equal to 1024 bytes. To generate vmstat 
		reports where 1 kilobyte is equal to 1000 bytes, use the following form:
	
	

	
	
19. Alias
	$ cd; vi .bash_profile
	$ alias server1='ssh root@ip_address -p 3404'   <= add your alias in .bash_profile
	$ alias lp='ls -al ../'							<= ls -al parent directory  


19. Time NTP

	alias today='date +"%A, %B %-d, %Y"'

	date  => Thu Dec  4 10:11:00 EST 2014
	today => Thursday, December 4, 2014
	date +"%m-%d-%Y" => 12-04-2014

	### CentOS7 ###
	$ timedatectl - Control the system time and date
 	   --adjust-system-clock
           If set-local-rtc is invoked and this option is passed, the system
           clock is synchronized from the RTC again, taking the new setting
           into account. Otherwise, the RTC is synchronized from the system
           clock.

	To correct zone and time
	$ systemctl restart  ntpd.service
	
	# TimeZone changes to PST 
	https://www.cyberciti.biz/faq/centos-linux-6-7-changing-timezone-command-line/
	$ date
	$ ls -l /etc/localtime
	$ timedatectl
	$ timedatectl list-timezones
	$ timedatectl list-timezones | grep -i los
		America/Los_Angeles
	$ timedatectl set-timezone America/Los_Angeles

	
	
	
20. cat and strings
	$ cat /usr/bin/bash XXXX
    
	$ zcat file.archived.tar.gz			<= reading gzip (view zip) file without extract first 
	
	$ strings /usr/bin/bash  <= human readable content buried inside the program.
 

   
21. uptime		<= uptime, number of users, CPU load 1, 5, 15 mins
	09:35:45 up 10 days, 21:18,  2 users,  load average: 0.03, 0.03, 0.05
	
	$ uptime | awk '{print $3}' | cut -f1 -d,
	$ uptime | awk '{print $3,$4}' | cut -f1 -d,	
   
22. Script 				<= Record commands in terminal session


23. List of services running
	/usr/sbin/service --status-all
  

24. Echo 

	$ HOSTNAME=hostname						<=variable HOSTNAME into command hostname
	$ echo "PC name is $HOSTNAME." 			<= PC name is PC.

    $ echo "PC name is \"$HOSTNAME\"."		<=  
    
    $ echo 'Single quotes "protect" double quotes'
          Single quotes "protect" double quotes

    $ echo "Well, isn’t that \"special\"?"
    $ Well, isn’t that "special"?

    $ echo "You have `ls | wc -l` files in `pwd`"
    $ You have 43 files in /home/bob
 
    $ x=100
	$ echo "The value of \$x is $x"
          The value of $x is 100

	# echo -e    						 <= Enables interpretation of backslash escapes
	$ echo -e "Inserting blank\n\n\n" 					<= \n  adding blank newline lines to text
		
	$ echo -e "worlds\tseperate\tby\ttabs."				<= \t  inserting tabs
	
	$ echo -e "\aMy computer \\went \"beep\"." 			<= \a  alert	makes your terminal beep										
														<= \\	backslash	insert a backslash
	
	
25. $ last						<= show listing of last logged in users
###	$ Last Reboot ###
	$ who -b
	$ last reboot
	reboot system boot 3.2.13-grsec-xxx Tue Apr 3 07:34 - 09:17 (9+01:42) 
																	^= 9days 1hr 42mins after restart
	
	$ last
	$ last -x
	$ last -x reboot
	$ last -x shutdown
	$ lastlog					<= reports the most recent login of all users 
    
	$ who
	$ who -r 					<= runlevel
	$ who -b




15. Is .bash_profile is a file???? 
	if [ -f .bash_profile ] ; then 					<= -f file is exist & regular file
		echo "you have the file in your home dir"		
    	else 
		echo "You do not have it"
	fi

16. id -u        (-u <= --user for script)   
	print real and effective user and group ID
	uid=0(root) gid=0(root) groups=0(root) 				context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023


21. rm -rf ./tmp/test/delme/*    <=delete folder contents
    
	## search all files bigger than 100MB
	
	$find /  -type f -size +100M   		<= k-kilo, M-mega, G-giga

	find / -name "*.txt" | xargs rm -rf				<= search all and delete them
	
	find / -name “*.txt” | xargs grep “Tecmint”		<= search and search string "Techmint"
	
    !!! Danger !!!
    rm -fr ./tmp/test/delme/ *   <=delete whole root folder(./tmp/test/delme)   

	$ find . -type f -name "*.bak" -exec rm -i {} \;
	
	$ find . -type f -name "FILE-TO-FIND" -exec rm -f {} \;
	
		-name "FILE-TO-FIND" : File pattern.
		-exec rm -rf {} \; : Delete all files matched by file pattern.
		-type f : Only match files and do not include directory names.
	
	# Delete local backup folder older than 2 days
	$ sudo find /backup/ -mindepth 1 -mtime +2 -delete

	$ sudo find /data/backups/ -type f -mtime +0 -name 'rrs_us_db.sql.*' -delete   	<= mtime (days), mmin (mins)
	
	-mtime   
		Day 0~1, 2, 3, 4, 5
		+2 <= 3,4,5 older than 2days, -2 <= 1,2 less than 2days
		
	ctime
		ctime is the inode or file change time. The ctime gets updated when the file attributes are changed, 
		like changing the owner, changing the permission or moving the file to an other filesystem but will 
		also be updated when you modify a file.

	mtime
		mtime is the file modify time. The mtime gets updated when you modify a file. Whenever you update 
		content of a file or save a file the mtime gets updated. 
		
		Most of the times ctime and mtime will be the same, unless only the file attributes are updated. 
		In that case only the ctime gets updated.

	atime
		atime is the file access time. The atime gets updated when you open a file but also when a file is 
		used for other operations like grep, sort, cat, head, tail and so on.

 
	
22. touch file_{1..100}    <= creating 100 files
    touch file_{01..100}   <= to resolve file format problem above logic
    touch {1,2,3,4,5}	   <= creating 1,2,3,4,5 files
    touch ./ls/{1..100}    <= creating files under ./ls/ folder
    touch ./tmp/test/delme/file_{01..100}
  
    echo {1..10}            	 <= print out 1,2,..10
    echo {1..10..2}	   			 <= Prints out 1 3 5 7 9
    echo {1..10..3}        		 <= Prints out 1 4 7 10
    echo {A..Z}              
    echo {A..z}		   			 <= Capital first    
    echo {w..d..2}	    		 <= Reverse order for every 2nd alphabet

    touch {apple,banana,cherry,durian}_{01..100}{1..10} 
    ls -1 | wc -l    	   		 <= 4000 result

    Prints special character without errors	
    echo '"! # $ % & '\'' ( ) * + , - . / : ; & < = > ? @ [ \ ] ^ _ { | } ~"'
    echo "! # $ % & '\'' ( ) * + , - . / : ; & < = > ? @ [ \ ] ^ _ { | } ~"

23. 
	echo $BASH_VERSION
	echo $MACHTYPE
	echo $SECONDS 					<= Count script started 


	d=$(pwd)
	echo $d

	echo 1/3 | bc -l                <= .33333333333333333333

24.	IP address 						<= finding IP from ifconfig using grep regex
	$ ifconfig | grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b"
	 192.168.232.132

	$ ip neighbour show			<= how your subnet's computers

23-1. tail
  tail -f *Slave.log
  tail -f *vSlave.log | grep "Info: Slave - Sending status"	



25. a="hello"
	echo ${#a}   <= 5 number for character
	


26. date 
	$ date +"%m-%d-%Y"   		<= 12-17-2014
	$ date +"%H:%M:%S"   		<= 10:20:18
	$ date +"Today's date is %m-%d-%Y and time is %H:%M:%S."
	$ date +%T
	$ date +%D
	$ date +%F


27. printf
	






29. Empty file contents or logs
						Bourne POSIX  zsh    csh/tcsh  rc/es  fish
	> file                Y      Y      N(1)   N(1)      N      N
	: > file              N/Y(2) Y(3)   Y      Y(4)      N(5)   N(5)
	true > file           Y(5)   Y      Y      Y(5)      Y(5)   Y(5)
	cat /dev/null > file  Y(5)   Y      Y(5)   Y(5)      Y(5)   Y(5)
	cp /dev/null file (7) Y(5)   Y      Y(5)   Y(5)      Y(5)   Y(5)
	printf '' > file      Y(5)   Y      Y      Y(5)      Y(5)   Y
	eval > file           Y(3,8) Y(3)   Y      Y(6)      Y      Y


30. df . and  df / are same result

	df -h / | grep -E "\/$" | awk '{print $4}'
	df -h / | xargs | awk '{print $11}'       <= Same result
	
	fdisk -l				
	
	sfdisk -l -uM				<=partition table manipulator for Linux
	
	cfdisk /dev/sda1  			<= a linux partition editor with an interactive user interface based on ncurses
	
	parted -l  <= list out partitions 
	
	df -h | gep ^/dev
	
	$ lsblk  				<= Lists out all the storage blocks
		NAME  MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
		xvda1 202:1    0  12G  0 disk /

	$ blkid 			 <= prints the block device(partitions and storage media) 
							attributes like uuid and file system type.
	




36. 







40. Clean Up Logs
	
    $ cd /var/log
	$ > log_file
    $ cat /dev/null > messages
	$ cat /dev/null > wtmp			<= utmp, wtmp - login records
  
	echo "Log files cleaned up."

	utmp <= complete picture of users logins at which terminals, logouts, system events and current status 		of the system, system boot time (used by uptime) etc.
	wtmp <= historical data of utmp.
	btmp <= records only failed login attempts.

	$ last 		<= default location without using -f(file) /var/log/wtmp  
	$ last -f /var/run/utmp   <= /var/run/utmp for different location using -f flag
	$ last -f /var/log/btmp

41.	Watch command to real time monitoring

	$ watch lsof -i			<= Watching who are connecting to a system and disconnecting, every 2sec
  

  
	$ watch -n 1 "ps aux | grep app_name"
<<<<<<< Updated upstream

=======
>>>>>>> Stashed changes

### encryption and decryption
	$ openssl passwd -1 your_password  
	  $1$nd1lHN8J$ehlK62ZgzrZDBq6ZHPT/   <= hashed value
	
	
	$ mkpasswd -l 15 -d 3 -C 5

	$ pwgen -s -1



	
42. strace - trace(monitor) system calls and signals of a program
	$ strace foobar.sh  	<= debugging a command/script   
	$ strace -p PID
	
	1. Trace the Execution of an Executable
		$ strace ls
		execve("/bin/ls", ["ls"], [/* 21 vars */]) = 0
		................
	2. Trace a Specific System Calls in an Executable Using Option -e
		$strace -e open ls
		open("/etc/ld.so.cache", O_RDONLY)      = 3
		.............
	#only the open system call of the ls command
		$ strace -e trace=open,read ls /home
	3. Save the Trace Execution to a File Using Option -o
		$ strace -o output.txt ls
	4. Execute Strace on a Running Linux Process Using Option -p
		$ strace -p PID
		
	5. Print Timestamp for Each Trace Output Line Using Option -t
		$ strace -t -e open ls /home
		11:59:25 open("/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
		11:59:25 open("/lib64/libselinux.so.1", O_RDONLY|O_CLOEXEC) = 3

	6. Print Relative Time for System Calls Using Option -r
		$ strace -r ls 
	7. Generate Statistics Report of System Calls Using Option -c
		$ strace -c ls /home
		noza  nozatech
		% time     seconds  usecs/call     calls    errors syscall
		------ ----------- ----------- --------- --------- ----------------
		25.48    0.000859          29        30           mmap
		14.12    0.000476          24        20           mprotect
	8. 

# Network Troubleshooting
Traceroute, Ping, and MTR
	
	mtr(Mutli function TRracert) 
	
	https://www.linode.com/docs/networking/diagnostics/diagnosing-network-issues-with-mtr
	combines  the  functionality  of the traceroute and ping in a single network diagnostic tool.
	
	# Generating report
	$ mtr google.com 				<= realtime
	$ mtr -rw google.com			<= report
    $ mtr -rw -c 5 google.com       <= -r report, -w wide report, -c count 5 instead of 10 by default
	
	Start: Thu Dec  1 13:55:32 2016
	HOST: cos7    Loss%   Snt   Last   Avg  Best  Wrst StDev  <= standard deviation of the latencies to each host
	1.|-- gateway  0.0%     5    0.1   0.1   0.1   0.2   0.0
	2.|-- i7       0.0%     5    0.3   0.3   0.3   0.4   0.0

	$ mtr --no-dns -rw -c 5 google.com

	# traceroute
	$ traceroute -n -w 3 -q 1 -N 32 -m 16 google.com
		-n         : Disable DNS lookup to speed up queries.
		-w seconds : Set the time (in seconds) to wait for a response to a probe (default 5.0 sec).
		-q  NUMBER : Sets the number of probe packets per hop. The default is 3.
		-m  max hop:
	
	* * *
	The firewall to block ICMP packets. As such, you will sometimes see a series of asterisks indicating 
	that trace route was not able to get information from a particular host.
	
	
	# Ping
	Windows Ping TTL is 128
	Redhat	Ping TTL is 64
	Other linux < 128
	
	$ ping -i 4 127.0.0.1    	<= send out ping packets every 4 sec
	$ ping -i 0.1 127.0.0.1    	<= send out ping packets every 0.1 sec
	
	# ping localhost
	$ ping loacalhost
	$ ping 127.0.0.1
	$ ping 127.0.0.2~254
	$ ping 0.0.0.0
	$ ping 0
	
	# flood network
	$ ping -f 0
	
	# change the packet size which is by default 64 bytes
	$ ping -s 200 0
	
	# how to sepcify timeout
	$ ping -w 7 0 	
	
	
	
<<<<<<< Updated upstream
43. stty 		   <= change and print terminal line settings
	$ stty -echo   <= hide(turn off) terminal typing(Can't see typing)
    $ stty echo    <= show(restore) terminal typing(able to see typing)
    $ TTY  		   <= TeleTYpe



43. stat 				<= detailed and statistic on a file (ls -l foo.bar)
	
	$ stat foo.bar 
		File: ‘foo.bar’
		Size: 354             Blocks: 8          IO Block: 4096   regular file
		Device: fc00h/64512d    Inode: 1046543     Links: 1
		Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
		Access: 2016-04-08 21:16:25.603244615 -0700
		Modify: 2016-04-08 21:16:45.355245237 -0700
		Change: 2016-04-08 21:16:45.355245237 -0700
		Birth: -
=======
	
42. strace - trace system calls and signals	
	$ strace foobar.sh  	<= debugging a command/script   

43. stty 		 <= change and print terminal line settings
	$ stty -echo   <= hide(turn off) terminal typing(Can't see typing)
    $ stty echo    <= show(restore) terminal typing(able to see typing)
    $ TTY  		 <= TeleTYpe


>>>>>>> Stashed changes

43. stat 				<= detailed and statistic on a file (ls -l foo.bar)
	
	$ stat foo.bar 
		File: ‘foo.bar’
		Size: 354             Blocks: 8          IO Block: 4096   regular file
		Device: fc00h/64512d    Inode: 1046543     Links: 1
		Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
		Access: 2016-04-08 21:16:25.603244615 -0700
		Modify: 2016-04-08 21:16:45.355245237 -0700
		Change: 2016-04-08 21:16:45.355245237 -0700
		Birth: -

<<<<<<< Updated upstream
44. lsmod = Show the status of modules in the Linux Kernel
			same as  "cat /proc/modules"

=======

44. lsmod = Show the status of modules in the Linux Kernel
			same as  "cat /proc/modules"
			
	
	mrmod					 <= rmmod - Simple program to remove a module from the Linux Kernel
	Error msg  "blk_update_request: I/O error, dev fd0, sector 0 errors"
	http://unix.stackexchange.com/questions/282845/blk-update-request-i-o-error-dev-fd0-sector-0
	$ rmmod floppy		
	$ echo "blacklist floppy" | sudo tee /etc/modprobe.d/blacklist-floppy.conf
	
	
	more info:
	fstab(5), 
	findfs(8), 
	mount(8) 
	blkid(8) 

	
	
>>>>>>> Stashed changes

45. Disabling User account
	Change login passwd in /etc/passwd
	apark:x:503:504::/home/apark:/bin/bash
	apark:* or !:503:504::/home/apark:/bin/bash   <= X to * or !  
		note: will require you to assign a new password to the user if you re-enable the account,
	apark:x:503:504::/home/apark:/bin/bash/nologin    <= add /nologin at the end
	apark:x:503:504::/home/apark:/bin/nologin
	apark:x:503:504::/home/apark:/usr/sbin/nologin
	e.g. $ ssh test@192.168.232.138
			test@192.168.232.138's password:
			Permission denied, please try again.
	
	$ usermod --lock --expiredate 1970-01-01 <username>
	$ sudo usermod --lock --shell /bin/nologin username
	$ sudo usermod -L -s /bin/nologin username

	$ chage -E 0 user_ID                                       <= $chage -E -1 user_ID   
	$ ssh test1@192.168.232.138
	test1@192.168.232.138's password:
	Your account has expired; please contact your system administrator
	Connection closed by 192.168.232.138

# Referece 
http://unix.stackexchange.com/questions/7690/how-do-i-completely-disable-an-account
	$ passwd -l user_ID			<= -l lock
	
Doesn't satisfy constraints because public key authentication bypasses PAM and still allows direct login.
	$ usermod -s /sbin/nologin <user>
	
Doesn't satisfy constraints because breaks the enterprise scheduler
### $ usermod --lock --expiredate 1970-01-01 <user>  ###
	$ usermod -l -e 1970-01-01 user_ID
	This is our winner. Remote login disabled, yet root can still su <user>, as can other users via 
	sudo so the scheduler functions properly and authorized end users can become the target service account as needed.


45. Wall					<= messaging to users within system 				
	$echo testing > wall.txt
	$wall wall.txt
		Broadcast Message from noza@fm
			(/dev/pts/1) at 19:46 ...
		testing
	
	$ echo "Please log out" | wall 
		Broadcast Message from noza@fm
			(/dev/pts/1) at 19:47 ...
		Please log out


		
45. CentOS 6.5(sysV) vs CentOS 7(systemd)

A. CentOS 6.5(sysV)
	
/sbin/chkconfig --list

	a.List of all services for system
	  chkconfig --list  | more
	b.Runlevel
	  cat /etc/inittab
	c.check runlevel
	  runlevel
	d.Service scripts
	  ls /etc/init.d/
 	e.symbolic link to init.d directory
	  ls /etc/rc3.d
	  



B. CentOS 7(systemd)
	a.List of all services for system
	  systemctl list-units --type service --all
	b.
	  /etc/systemd.system

	c. same as /etc/init.d
	  /usr/lib/systemd/system

	d.
  
  
  
  
List processes
chkconfig:

# chkconfig --list
systemd:

# systemctl list-units
Enable a service
chkconfig:

$ systemctl list-unit-files | grep 'influx\|grafana'

# chkconfig <service_name> on
systemd:

# systemctl enable <service_name>.service
Disable a service
chkconfig:

# chkconfig <service_name> off
systemd:

# systemctl disable <service_name>.service
Start a service
chkconfig:

# service <service_name> start
systemd:

# systemctl start <service_name>.service
Stop a service
chkconfig:

# service <service_name> stop
systemd:

# systemctl stop <service_name>.service
Check the status of a service
chkconfig:

# service <service_name> status
systemd:

# systemctl status <service_name>.service



	
CentOS 6.5(sysV)     		   vs     	 CentOS 7 (systemd)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
service httpd status				    systemctl -l(full) status httpd
 
chkconfig --level 35 httopd  on			systemctl enable/disable httpd.service
chkconfig --list | grep httpd			systemctl start httpd
service httpd start/stop		     	systemctl stop httpd
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
target					systemctl list-unit-files
cat /etc/inittab		systemctl list-units --type target  (--all for not active)					                        systemctl list-units --type services
						systemctl get-default
						change to multi-user target
						systemctl set-default multi-uiser.target
						systemctl reboot
						startx <= Windows environment 
						systemctl set-default graphical.target
						systemctl reboot
						systemctl start/stop sshd.service
						systemctl enable/disalbe sshd.service  (constant run)					systemctl is-enabled sshd  <= check for sshd
  

journalctl - Query the systemd journal

	journalctl may be used to query the contents of the systemd(1)
    journal as written by systemd-journald.service(8).

	$ journalctl | grep fail
	$ journalctl -b          <= current boot
	$ journalctl -b -1       <= previous boot
	$ journalctl -f          <= follow
	$ journalctl -xe		 <= -x  Augment log lines with explanation texts from the message catalog.
							    -e  jump to the end of the journal 
	
	# Check program logs
	$ journalctl -u nginx 		 <= -u unit
	$ journalctl -u sshd    

	$ journalctl --since "10 min ago"


----------------------------------------------------------------
Distribution			|	SystemD	  |	System V  |	 Upstart
----------------------------------------------------------------
Amazon Linux											V
CentOS 6												V
CentOS 7						V
Debian 7 ("Wheezy")							V
Debian 8 ("Jessie")				V 
RHEL 6													V
RHEL 7							V
Ubuntu, 14.04 or lower									V
Ubuntu, 16.04 or higher			V	
----------------------------------------------------------------		
	
	
46. List of package installed
	$ rpm -qa | grep ^httpd
# CentOS 'YUM'
	$ yum list *httpd  											<= online
    $ yum list installed (same as 'rpm -qa') | grep httpd		<= on system 
    $ yum info httpd  											<= version and detail pkg info


47. EPEL (Extra Packages for Enterprise Linux) 
	CentOS 6 x64 EPEL 
	wget http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
	rpm -ivh epel-release-6-8.noarch.rpm


	CentOS7 
	sudo yum install epel-release


	
49. echo command
	#!/bin/bash
	now=$(date)			              <= $(date) will print date command
	echo $now			              <= invoke variable now
	echo "Today's date is $(date)."   <= $(date) will print date command
	cal 	 		                  <= display calendar


	50.History with time stamp

	export into ~/.bash_profile as well as /root/.bash_profile
 
	export HISTTIMEFORMAT="%F %T "



51. Diff between two remote folders through SSH


Server 1

IP: images.server1.com
User: ubuntu1
Password: pa$$word1
Images Path: /home/www/images/test_images

Server 2

IP: images.server2.com
User: ubuntu2
Password: pa$$word2
Images Path: /var/www/site/images/test_images


-------------------------------------------------------------------------------------------
diff -B <(sshpass -p 'pa$$word1' \
	ssh ubuntu1@images.server1.com "find /home/www/images/test_images -type f\ 
	| sed 's/\/home\/www\/images\/test_images\///g'" | sort -n) < \
	(sshpass -p 'pa$$word2' ssh ubuntu2@images.server2.com \ 
	"find /var/www/site/images/test_images -type f |  \ 
	sed 's/\/var\/www\/site\/images\/test_images\///g'" | sort -n)\ 
	| grep ">" | awk '{print $2}'
-------------------------------------------------------------------------------------------

Explanation:

You can use diff -B <() <() for taking the diff between two streams. The command first uses sshpass to ssh into the two servers without having to enter your passwords interactively.

Each parameter for diff -B uses find command to recursively list all your images in the specified directory and uses sed to remove the root path of the files (because they are different for two servers - and to make it work for the diff command); and the sort command to sort them.

Since the output of the diff command returns either > or <, grep is used to filter out only the diffs from your Server 2. Last, awk prints out only the second column (removes the > column from the output).

NOTE: You need to install sshpass first. 
sudo apt-get install sshpass


  csplit   Split a file into context-determined pieces
  cut      Divide a file into several parts


52. # chkconfig --add <script_name>
	# chkconfig <script_name> on


chkconfig  provides  a  simple  command-line  tool to maintaining the
       /etc/rc[0-6].d directory hierarchy by relieving  system  administrators
       of  the  task  of  directly manipulating the numerous symbolic links in
       those directories.

       This implementation of chkconfig was inspired by the chkconfig  command
       present  in the IRIX operating system. Rather than maintaining configu‐
       ration information outside of the  /etc/rc[0-6].d  hierarchy,  however,
       this  version  directly  manages  the  symlinks in /etc/rc[0-6].d. This
       leaves all of the configuration  information  regarding  what  services
       init starts in a single location.

       chkconfig  has five distinct functions: adding new services for manage‐
       ment, removing services from management, listing  the  current  startup
       information  for  services,  changing  the startup information for ser‐
       vices, and checking the startup state of a particular service.

       When chkconfig is run with only a service name, it checks to see if the
       service  is configured to be started in the current runlevel. If it is,
       chkconfig returns true; otherwise it returns false. The --level  option
       may be used to have chkconfig query an alternative runlevel rather than
       the current one.

       When chkconfig is run with the --list argument, or no arguments at all,
       a listing is displayed of all services and their current configuration.

       If  one  of  on,  off, reset, or reset priorities is specified after the
       service name, chkconfig changes the start-up information for the  specified  
	   service.  The on and off flags cause the service to be started or
       stopped, respectively, in the runlevels being changed. The  reset  flag
       resets  the  on/off state for all runlevels for the service to whatever
       is specified in the init script in question, while the  reset priorities
       flag  resets  the  start/stop priorities for the service to whatever is
       specified in the init script.

       By default, the on and off options affect only runlevels 2, 3,  4,  and
       5,  while  reset and reset priorities affects all of the runlevels.  The
       --level option may be used to specify which runlevels are affected.

       Note that for every service, each runlevel has either a start script or
       a  stop  script.   When  switching runlevels, init will not re-start an
       already-started service, and will not re-stop a  service  that  is  not
       running.

       chkconfig also can manage xinetd scripts via the means of xinetd.d con‐
       figuration files. Note that only the on, off, and --list  commands  are
       supported for xinetd.d services.

       chkconfig  supports  a  --type argument to limit actions to only a spe‐
       cific type of services, in the case where services of either  type  may
       share a name. Possible values for type are sysv and xinetd.

53. Nagios
	systemctl start nagios.service
	systemctl start httpd.service
	systemctl enable httpd.service		<= Apache in startup
	ln -s '/usr/lib/systemd/system/httpd.service'  '/etc/systemd/system/multi-user.target.wants/httpd.service'

54. List of groups
	groups
	/etc/group
 	$ getent group | cut -d: -f1

 
55. SElinux <= add port to security-enhanced linux 
  semange port -a -t http_port_t -p tcp 10051  

	### set SELINUX Permissive mode
    $ setenforce permissive
	$ sed -i 's\=enforcing\=permissive\g' /etc/sysconfig/selinux
	# To Check
	$ getenforce
	Permissive
  
  
  
  
  
56. Region and Time NTP

#### Region - TIME ZONE ####

  Ubuntu
  sudo dpkg-reconfigure tzdata
  sudo vi /etc/timezone	<= change to **America/Los_Angeles or  							       America/New_York


#### NTP - TIME ZONE ####

	# CenOS 7.x #

	$ yum install ntp
	$ systemctl start ntpd
	$ systemctl enable ntpd

	$ ls -l /etc/localtime 
	$ timedatectl list-timezones
	$ timedatectl set-timezone America/Los_Angeles


	# CentOS 6.x # 
	America/Tijuana

	$ ntpdate pool.ntp.org  		<=0~3.us.pool.ntp.org 
	$ chkconfig ntpd on				<= put in into startup
	$ /etc/init.d/ntpd start

	# Ubuntu 14.04(Trusty) #
	$ apt-get install ntp
	$ ntpq -p  							<=-p peer list of ntp servers  (ntpq=std NTP query program)
	$ vi /etc/ntp.conf					<= replace with **US** 0~3.us.pool.ntp.org 
		########################
		server 0.us.pool.ntp.org
		server 1.us.pool.ntp.org
		server 2.us.pool.ntp.org
		server 3.us.pool.ntp.org
		########################
	$ sudo service ntp restart 

	# To stop ntpd #
	$ sudo /etc/init.d/ntp stop
	$ sudo service ntp stop

	# To prevent it from starting at boot:
	$ sudo update-rc.d -f ntp remove

# Update NTP time Ubuntu
	$ sudo ntpdate pool.ntp.org
	4 May 17:34:07 ntpdate[38018]: step time server 204.2.134.163 offset 53.202434 sec

	$ sudo ntpdate pool.ntp.org  <=  **** no servers can be used, exiting ***
		31 Aug 19:05:55 ntpdate[8911]: the NTP socket is in use, exiting
	$ sudo service ntp stop
		[ ok ] Stopping NTP server: ntpd.
	$ sudo ntpdate pool.ntp.org
		31 Aug 19:07:11 ntpdate[10355]: adjust time server 46.29.176.115 offset -0.002893 sec
	$ sudo service ntp start

	
	
57. CentOS6.x MySQL auto start and start MySQL services
	$ chkconfig mysqld on
	$ /etc/init.d/mysqld start


	
	
### Ubuntu 16.04 Iptables setup ###
	http://linux-sys-adm.com/ubuntu-16.04-lts-how-to-configure-firewall-iptables-fail2ban/
	https://oitibs.com/easy-ubuntu-16-server-firewall/
	
	# Install IPTables Persistent Package 
	$ apt-get install -y iptables-persistent

	# Add netfilter-persistent Startup
	$ invoke-rc.d netfilter-persistent save
	
	$ service netfilter-persistent stop
	
	### After Edit IPtables ##
	# Start netfilter-persistent Service
	
	$ service netfilter-persistent start
	
	$ iptables -nL	


	
	
### CentOS7 ### 

Iptables opens all	
$ iptables -I INPUT -j ACCEPT
	
sudo iptables -L
sudo yum -y install iptables-services
sudo systemctl enable iptables 		
sudo systemctl mask firewalld			
sudo systemctl stop firewalld

vi /etc/sysconfig/iptables
sudo systemctl start iptables

	
	Writing a Simple Rule Set( https://wiki.centos.org/HowTos/Network/IPTables)


IMPORTANT: At this point we are going to clear the default rule set. If you are connecting remotely 
to a server via SSH for this tutorial then there is a very real possibility that you could lock yourself 
out of your machine. You must set the default input policy to accept before flushing the current rules, 
and then add a rule at the start to explicitly allow yourself access to prevent against locking yourself out.
We will use an example based approach to examine the various iptables commands. In this first example, 
we will create a very simple set of rules to set up a Stateful Packet Inspection (SPI) firewall that 
will allow all outgoing connections but block all unwanted incoming connections:


# iptables -P INPUT ACCEPT
# iptables -F
# iptables -A INPUT -i lo -j ACCEPT
# iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
# iptables -A INPUT -p tcp --dport 22 -j ACCEPT
# iptables -P INPUT DROP
# iptables -P FORWARD DROP
# iptables -P OUTPUT ACCEPT
# iptables -L -v
which should give the following output:


Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 ACCEPT     all  --  lo     any     anywhere             anywhere
    0     0 ACCEPT     all  --  any    any     anywhere             anywhere            state RELATED,ESTABLISHED
    0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:ssh
Chain FORWARD (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
Now lets look at each of the 8 commands above in turn and understand exactly what we've just done:

iptables -P INPUT ACCEPT 
	If connecting remotely we must first temporarily set the default policy on the INPUT chain to ACCEPT otherwise once we flush the current rules we will be locked out of our server.
iptables -F 
	We used the -F switch to flush all existing rules so we start with a clean state from which to add new rules.
iptables -A INPUT -i lo -j ACCEPT 
	Now it's time to start adding some rules. We use the -A switch to append (or add) a rule to a specific chain, the INPUT chain in this instance. Then we use the -i switch (for interface) to specify packets matching or destined for the lo (localhost, 127.0.0.1) interface and finally -j (jump) to the target action for packets matching the rule - in this case ACCEPT. So this rule will allow all incoming packets destined for the localhost interface to be accepted. This is generally required as many software applications expect to be able to communicate with the localhost adaptor.
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT 
	This is the rule that does most of the work, and again we are adding (-A) it to the INPUT chain. Here we're using the -m switch to load a module (state). The state module is able to examine the state of a packet and determine if it is NEW, ESTABLISHED or RELATED. NEW refers to incoming packets that are new incoming connections that weren't initiated by the host system. ESTABLISHED and RELATED refers to incoming packets that are part of an already established connection or related to and already established connection.
iptables -A INPUT -p tcp --dport 22 -j ACCEPT 
	Here we add a rule allowing SSH connections over tcp port 22. This is to prevent accidental lockouts when working on remote systems over an SSH connection. We will explain this rule in more detail later.
iptables -P INPUT DROP 
	The -P switch sets the default policy on the specified chain. So now we can set the default policy on the INPUT chain to DROP. This means that if an incoming packet does not match one of the following rules it will be dropped. If we were connecting remotely via SSH and had not added the rule above, we would have just locked ourself out of the system at this point.
iptables -P FORWARD DROP 
	Similarly, here we've set the default policy on the FORWARD chain to DROP as we're not using our computer as a router so there should not be any packets passing through our computer.
iptables -P OUTPUT ACCEPT 
	and finally, we've set the default policy on the OUTPUT chain to ACCEPT as we want to allow all outgoing traffic (as we trust our users).
iptables -L -v 	
	Finally, we can list (-L) the rules we've just added to check they've been loaded correctly.



	
### Changing data directory on Centos 7 + MySQL 5.6	
	$ wget http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm
	$ yum localinstall mysql57-community-release-el7-7.noarch.rpm
	
	
	$ systemctl mysqld stop
	$ cp -rap /var/lib/mysql/ /target_dir/mysql/
	$ chown mysql:mysql /target_dir/mysql
	
	# edit /etc/my.conf to redirect datadir & socket to new path
	$ vi /etc/my.conf 
	# change to new data, socket & client connection location
	datadir=/target_dir/mysql
	socket=/target_dir/mysql/mysql.sock
	[client]
	socket=/target_dir/mysql/mysql.sock
	
	
	# manage SELinux => install semanage (In Centos 7 minimal is not installed)
	$ getenforce
	$ yum provides /usr/sbin/semanage
	$ yum -y install policycoreutils-python


	$ getenforce
	$ semanage fcontext -a -t mysqld_db_t "/new_dir/mysql(/.*)?"
	$ restorecon -Rv /new_dir/mysql
	$ systemctl start mysqld
	
	# Check SELinux reports any error on MySQL
	$ grep mysqld /var/log/audit/audit.log

	


	
	
58. # CentOS 6 Turn off firewall and SELINUX

	chkconfig iptables off && chkconfig ip6tables off && 
	service iptables stop &&  service ip6tables stop && setenforce 0
	$ vi /etc/sysconfig/iptables
	$ iptables -L
	$ iptables -L -n   <= NO DNS lookup (Faster)
	$ iptables -nL
  
    
  ### Ubuntu 14.04 ###
	$ sudo apt-get update
	$ sudo apt-get install iptables-persistent
	$ sudo vi /etc/iptables/rules.v4 
  	
	$ /etc/init.d/iptables-persistent {start|restart|reload|force-reload| save|flush}
	$ service iptables-persistent start | stop | restart | reload
  
  ### Check error restore test ###
	$ iptables-restore -vv < /etc/iptables/rules.v4

  ### Ubuntu 16.04 Iptables setup ###
	http://linux-sys-adm.com/ubuntu-16.04-lts-how-to-configure-firewall-iptables-fail2ban/
	https://oitibs.com/easy-ubuntu-16-server-firewall/
	
	# Install IPTables Persistent Package 
	$ apt-get install -y iptables-persistent

	# Add netfilter-persistent Startup
	$ invoke-rc.d netfilter-persistent save
	
	$ service netfilter-persistent stop
	
	### Edit IPtables ##
	# Start netfilter-persistent Service
	
	$ service netfilter-persistent start
	
	$ iptables -nL







# Security Selinux
vi /etc/selinux/config #replace SELINUX=enforcing with SELINUX=disabled

58. Bash History with time stamp
	###										###	
	#  $ export HISTTIMEFORMAT='%F '          #		<= %F Equivalent to %Y - %m - %d
	#  $ export HISTTIMEFORMAT='%F %T  ' 	  #		<= %F Equivalent to %Y - %m - %d,  %T Replaced by the time ( %H : %M : %S )
	** Check the export list	
	$ export -p | grep HIST*
	$ env | sort | grep HIST


59. Prompt Color red

	Permanently add Green color scheme to .bashrc file
	######															  ####
	# echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'" >> ~/.bashrc #   <= GREEN Color
	######															  ####

	### Temp current shell
	export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]' 		 >> ~/.bashrc		<= Green 
	export PS1='\[\e[1;31m\][\u@\h \W]\$\[\e[0m\]' 		 >> ~/.bashrc		<= Red  
    export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"  	 >> ~/.bashrc		<= Green|Red 
    
	### Permanent
	echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'" 	 >> ~/.bashrc		<= Green 
    echo 'export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"'   >> ~/.bashrc		<= Green|Red 

								
	


	
59. network monitoring and troubleshooting
	
	# iftop
	$ iftop -n -i eth1        <= -n no name lookup -i interface eth1
	
	# vnstat	 					<= a console-based network traffic monitor
		$ vnstat -l					<= real time network statistic
		$ vnstat --testkernel		<= check the kernel is providing all the information
		$ vnstat -d					<= -h hour, -d day, -w week, -w month,
		$ vnstat --dumpdb			<= export to Excel or 
	
	# traceroute TTL(time-to-live) value, also known as hop limit, option
	$ traceroute -w 3 -q 1 -m 16 example.com

	# MTR
		MTR - MTR represents an evolution of the traceroute command by providing a greater data sample, as if augmenting 
		traceroute with ping output. This document provides an in depth overview of MTR, the data it generates, and how 
		to properly interpret and draw conclusions based on the data provided by it.


		
### netsniff-ng 								<- the packet sniffing beast
	Its gain of performance is reached by zero-copy mechanisms, so that on packet reception and transmission 
	the kernel does not need to copy packets from kernel space to user space and vice versa.
	toolkit can be used for network development and analysis, debugging, auditing or network reconnaissance.

	$ apt-get|yum install netsniff-ng 					
	http://netsniff-ng.org/
		
	For geographical AS TCP SYN probe trace route to a website:
	$ astraceroute -d eth0 -N -S -H netsniff-ng.org			<- autonomous system trace route utility

	For kernel networking statistics within promiscuous mode:
	$ ifpps -d eth0 -p						<-fetch and format kernel network statistics

	For high-speed network packet traffic generation, trafgen.txf is the packet configuration:
	$ trafgen -d eth0 -c trafgen.txf

	For compiling a Berkeley Packet Filter fubar.bpf:
	$ bpfc fubar.bpf

	For live-tracking of current TCP connections (including protocol, application name, city and country of source and destination):
	$ flowtop

	For efficiently dumping network traffic in a pcap file:
	$ netsniff-ng -i eth0 -o dump.pcap -s -b 0	
	
	
###	tcpdump
	
	$ tcpdump -i eth1 -n tcp port 10050
	####################################
	# tcpdump -i any -n tcp port 10050 #    <= zabbix agent port
	####################################
	-i = listening interface
	-n = not to convert addresses

	### Capture Ping - icmp message ##
	$ tcpdump -n icmp
	$ tcpdump -v -n icmp
	$ tcpdump -n icmp and 'icmp[0] != 8 and icmp[0] != 0'
	$ tcpdump -n icmp and icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply

	### Capture any packets where the source host is 192.168.1.1. Display IP addresses and port numbers:
	http://www.rationallyparanoid.com/articles/tcpdump.html
	$ tcpdump -n src host 192.168.1.1
  
###	NetCat
	###################################
	#  nc -v 192.168.110.220  22 	  #  <= netcat  -v verbose / -z listening daemon
	###################################   

    Connection to 192.168.110.220 10051 port [tcp/zabbix-trapper] succeeded!
	
	nc -l 22 							<= listening mode
  
  
	Ping and capture IP only
	$ ping -c 1 www.google.com | gawk -F'[()]' '/PING/{print $2}'
  
  
    dig, host, nslookup, 
    yum install bind-utils
    host $HOSTNAME | xargs | cut -f4 -d' '
	
	$ dig +short +trace www.name.com
	

  
  
61. telnet     ip_address    port_number  <= check port is open for connection
	$ telnet ip_address 22
	
	to exit ' ctrl + ] '  or ' ^] ' then type "close"



	
62. Ubuntu Package Clean ### Package Clean ###

# Installed package search
  
  apt-cache search mysql

  sudo apt-get update

# Backing ups:
  sudo cp /etc/apt/sources.list /etc/apt/sources.list.original
  sudo cp /var/lib/dpkg/status /var/lib/dpkg/status.original
  sudo cp -r --parents home/aws/* /home/apark/awn/   #--recursive

  sudo apt-get clean 
  sudo apt-get autoclean 


  # Fixing dependencies using apt's fix-broken mode
	sudo apt-get -f install
	
 E:Internal Error when using apt-get	
	sudo apt-get update
	sudo apt-get clean
	sudo apt-get cehck                       <= 
	sudo apt-get install -fy
	sudo dpkg -i /var/cache/apt/archives/*.deb
	sudo dpkg --configure -a
	sudo apt-get install -fy
	sudo apt-get dist-upgrade
	
####	Broken package fix ### 
sudo dpkg --configure -a
sudo apt-get install -f

$ sudo vi /var/lib/dpkg/status 			<= remove whole trouble block
$ cat /etc/apt/sources.list

#####
sudo sh -c "apt-get update;apt-get dist-upgrade;apt-get autoremove;apt-get autoclean"  
####
sudo sh -c "apt-get update;apt-get autoclean;apt-get clean;apt-get autoremove"
####
sudo dpkg --remove -force --force-remove-reinstreq package_name
####



### Clean APT Key

$ apt-key list
	pub   4096R/563278F6 2016-04-08 [expires: 2018-04-08]
	uid                  Foreman Automatic Signing Key (2016) <packages@theforeman.org>

$ apt-key del 563278F6  <= key_name

# how to remove -file
To remove a file whose name starts with a '-', for example '-foo',

$ rm --help
use one of these commands:
	rm -- -foo
	rm ./-foo


  

63. How do I fix a “Problem with MergeList” or “status file could not be parsed” error when trying to do an update?


sudo rm /var/lib/apt/lists/* -vf
sudo apt-get update



64.  AWS AMI MYSQL setup root pw issue 

1. apt-get/yum install mysql-server
2. /usr/bin/mysql_secure_installation
3. service mysqld stop 
4. rm -rf /var/lib/mysql/*
5. service mysqld start  <= recreate tables 
6. /usr/bin/mysqladmin -u root password 'Your-New-Passwd'  <= Your mysql root's pw

	mysql remote host login
	$ mysql -u webadmin –h 65.55.55.2 –p
	$ mysql -ID_admin -p'password'  -hlocalhost

65. Swappiness

	Swappiness is a Linux kernel parameter that controls the relative weight given to 
	swapping out runtime memory, as opposed to dropping pages from the system page cache. 
	Swappiness can be set to values between 0 and 100 inclusive. A low value causes the 
	kernel to avoid swapping, a higher value causes the kernel to try to use swap space. 
	The default value is 60, and for most desktop systems, setting it to 100 may affect 
	the overall performance, whereas setting it lower (even 0) may decrease response latency.

Value	Strategy
vm.swappiness = 0	The kernel will swap only to avoid an out of memory condition.
vm.swappiness = 60	The default value.
vm.swappiness = 100	The kernel will swap aggressively.
To temporarily set the swappiness in Linux, write the desired value (e.g. 10) to /proc/sys/vm/swappiness using the following command, running as root user:


	# Out of meory
	$ grep oom /var/log/*
	$ grep total_vm /var/log/*

	http://unix.stackexchange.com/questions/128642/debug-out-of-memory-with-var-log-messages
	http://unix.stackexchange.com/questions/92525/can-linux-run-out-of-ram/92544#92544
	
	$ grep oom /var/log/*
	/var/log/messages:Jan  5 18:41:12 app-03 kernel: httpd invoked oom-killer: gfp_mask=0x200da, order=0, oom_score_adj=0
	/var/log/messages:Jan  5 18:41:14 app-03 kernel: [<ffffffff8116d24e>] oom_kill_process+0x24e/0x3b0
	/var/log/messages:Jan  5 18:41:17 app-03 kernel: [ pid ]   uid  tgid total_vm  rss nr_ptes swapents oom_score_adj name
	
	pid 			<= The process ID.
	uid 			<= User ID.
	tgid 			<= Thread group ID.
	total_vm 		<= Virtual memory use (in 4 kB pages)
	rss 			<= Resident memory use (in 4 kB pages)
	nr_ptes			<= Page table entries
	swapents 		<= Swap entries
	oom_score_adj 	<= Usually 0; a lower number indicates the process will be less likely to die when the OOM killer is invoked.
	
	
	
# Set the swappiness value as root
  $ echo 10 > /proc/sys/vm/swappiness
 
# Alternatively, run this 
  $ sysctl -w vm.swappiness=10   # sysctl - configure kernel parameters at runtime
 
# Verify the change
  $ cat /proc/sys/vm/swappiness
  10

 
# Alternatively, verify the change
  sysctl vm.swappiness
  vm.swappiness = 10
  Permanent changes are made in /etc/sysctl.conf via the following configuration line     
  inserted, if not present):
  vm.swappiness = 10

  
  
  
66. rpm package 
	### install 
    $ rpm -ivh foo-xxx.rpm <= install
    $ rpm -Uvh foo-xxx.rpm <= upgrade

    ### Remove
	$ rpm -qa | grep -i webmin
	$ rpm -e <package name>

	### Search installed sw
	$ rpm -qa | grep mysql
	$ /sbin/ldconfig -v | grep mysql

  
67. nmap  <= Short for Network Mapper 
    It is an open source security tool for network exploration, security scanning and     
	auditing. However, nmap command comes with lots of options that can make the utility     
	more robust and difficult to follow for new users.
	
	$ nmap -sT ip_address  ( s-scan, T-TCP)


68. pgrep, pkill - look up or signal processes based on name and other attributes


69. rthunter(Root Kit Hunter)


70. 
	
		
# hanging terminal during the SSH connection.	
http://askubuntu.com/questions/344863/ssh-new-connection-begins-to-hang-not-reject-or-terminate-after-a-day-or-so-on	
A problem can arise when you are trying to connect from behind a NAT router using OpenSSH. 
During session setup, after the password has been given, OpenSSH sets the TOS (type of service) 
field in the IP datagram. Some routers are known to choke on this. The effect is that your 
session hangs indefinitely after you gave your password. Here is the example output from such 
an ssh session:		
$ ssh -vvv id@x.com	
$ ssh -o 		"ProxyCommand nc %h %p" id@x.com
	  ^option	
		

		
71.  Transfer file using SCP + pem key

	$ scp -i /path/to/key.pem     /path/to/file.jpg    user_id@ip_/home/id/folder
	
	## -i Identity_file such as pem key
	$ scp -i /AWS-OC-Key/oc-prod.pem /AWS-OC-Key/oc-sDATEe.pem ec2-user@ip_address:/home/ec2-user/.cert

	### copy a File to Remote host 
	$ scp -pv file.zip noza@192.168.221.128:/home/noza/download
	
	$ scp -i .ssh/oc.pem  ami.zip ubuntu@10.10.1.89:/home/ubuntu/

	### install unzip
	$ unzip file.zip -d /tmp/name			<= -d destin directory
    
	### Make a directory remotely
	
	$ ssh apark@remote_ip "mkdir /path/to/dir"

	
	### copy file / folder
	
	# Local to Remote copy
	$ scp -r /home/id/.ssh     id@ip:/home/id/			<= -r recursive

	# Remote to Local copy 
	$ scp -r id@ip:/home/id/.ssh    /home/id/.ssh2
	

	
72. Combine Xargs with Find command 
	$ ls
	one.c  one.h  two.c  two.h

	$ find . -name "*.c" | xargs rm -rf

	$ ls
	one.h  two.h

	Use fgrep in order to match for plain ., or escape the dots.

	$ find . -type f | xargs fgrep '...'   <= or if you still want to use grep :
	$ find . -type f | xargs grep '\.\.\.' <= And if you only want the current directory 	and not its subdirs :
	$ find . -maxdepth 1 -type f | xargs fgrep '...'
	
	Finding directory files with word
	$ grep -Ril "finding_word" /target/dir  <= R-recursive, i-ignore case, l-show file name
	
	
72.	Find the command location
	$ locate   command_name
	$ whereis  command_name
	$ which    command_name
	$ find     command_name
	
	Find string (word)
	$ grep -i finding_word *
	$ grep -i finding_word */*
	$ grep -i finding_word */*/*    <= continue to sub directories
	
	
	$ grep 'influx\|grafana'		<= grep 2
	

73. 
	iptables -L
	iptables -L -n
	iptables -nvL

	Ubuntu	iptables-restore < /etc/iptables/rules.v4
	
	CentOS	iptables-restore < /etc/sysconfig/iptables

	iptables-restore and ip6tables-restore are used to restore
	IP and IPv6 Tables from data specified on STDIN.  Use  I/O
        redirection(> <) provided by your shell to read from a file
	(< /etc/iptables/rules.v4)

	








75. Error 
	1>&2    <= STD output should go to the same place as STD error is going.

76. Backtick
	Command within backticks is evaluated (executed) by the shell before the main command (like chown in your examples), 
	and the output of that execution is used by that command, just as if you'd type that output at that place in the command line.

	$ sudo chown `id -u` /somedir
	$ sudo chown 1000 /somedir



77. Hostname Change

Ubuntu
/etc/hosts
/etc/hostname


CentOS6.x file location =>  
	$ cat /etc/hostname 
	$ hostname 
	$ echo new_name > /etc/hostname 
	
	$ systemctl restart systemd-hostnamed
	
	$ cat /etc/sysconfig/network
	
	$ hostnamectl status
	
	
###########################################
#	$ hostnamectl set-hostname new_name   # 
###########################################
	
	
	$ timedatectl status 
	$ timedatectl list-timezones
	$ timedatectl set-timezone UTC
	$ timedatectl set-timezone America/New_York
	
	Ubuntu timezone change
	$ sudo dpkg-reconfigure tzdata	
	
	
	
	
78. CenOS7 eno****** to eth0

	Step1#
	$sudo vi etc/sysconfig/grub	<= add "net.ifnames=0 biosdevname=0"
GRUB_CMDLINE_LINUX="rd.lvm.lv=centos/swap vconsole.font=latarcyrheb-sun16 rd.lvm.lv=centos/root crashkernel=auto  vconsole.keymap=us rhgb quiet net.ifnames=0 biosdevname=0"

	Step2#
	$sudo grub2-mkconfig  -o /boot/grub2/grub.cfg
	<= Using “grub2-mkconfig” command to re-generate a new grub configuration file

	Step3#
	$sudo mv /etc/sysconfig/network-scripts/ifcfg-eno16777736  /etc/sysconfig/network-scripts/ifcfg-eth0
		<= Rename “Eno” network file using”mv”command


	Step4#$
	su vi /etc/sysconfig/network-scripts/ifcfg-eth0 <= configuration file and set 
							the value of “Name” field to “eth0".
	<strong>NAME=eth0</strong>

	Step5# 
	reboot system, after rebooting system, using “ifconfig” command check 
	network interface information again.
	
	


80.  Ubuntu apt-get update fail some of packages.
	W:Failed to fetch bzip2:
     	E:Some index files failed to download. 

	sudo rm -rf /var/lib/apt/lists/*
	sudo apt-get update



81. Check Kernel Version

	$ ls -al /etc/ld.so.conf.d

	kernel-2.6.32-431.11.2.el6.x86_64.conf
	
	$ uname -a				<= all info 
	
	$ uname -r 
	2.6.32-431.1.2.0.1.el6.x86_64
	
	
	# Result is all same	
	$ cat /boot/config-$(uname -r)  
	$ cat /boot/config-`uname -r`
	$ cat /boot/config-4.2.0-27-generic 
	

	# Ubuntu
    $ sudo dpkg -l | grep linux-headers | grep ii
    $ sudo dpkg -l | grep linux-headers | grep ii | awk '{print $3}'
	

82. rsync
	rsync -av iptables.vm5-sgas sg-rog:/etc/sysconfig
	rsync -av rog sgas sg-rog:/home

    rsync -av /etc/hosts sg-eu:/etc
    rsync -av /etc/sysconfig/iptables sg-eu:/etc/sysconfig
    
	rsync -av lobby1:/root/ /root --exclude ".ssh"

	ssh sg-eu "service iptables restart"
    
	ssh sg-eu "cat .ssh/*.pub"




	
	
### IFS (Internal Field Separator)###
http://unix.stackexchange.com/questions/16192/what-is-ifs-in-context-of-for-looping
IFS isn't directly related to looping, it's related to word splitting. IFS indirectly determines how 
the output from the command is broken up into pieces that the loop iterates over.
When you have an unprotected variable substitution $foo or command substitution $(foo), there are 
two cases:
•	If the context expects a single word, e.g. when the substitution is between double quotes "$foo", 
	or in a variable assignment x=$foo, then the string resulting from the substitution is used as-is.
•	If the context expects multiple words, which is the case most of the times, then two further .
	expansions are performed on the resulting string:
•	The string is split into words. Any character that appears in $IFS is considered a word separator. 
	For example IFS=":"; foo="12:34::78"; echo $foo prints 12 34  78 (with two spaces between 34 and 
	78, since there's an empty word).
•	Each word is treated as a glob pattern and expanded into a list of file names. For example, 
	foo="*"; echo $foo prints the list of files in the current directory.

	For loops, like many other contexts, expect a list of words. So
	for x in $(foo); do …
	breaks $(foo) into words, and treats each word as a glob pattern. The default value of IFS isspace,
	tab and newline, so if foo prints out two lines hello world and howdy then the loop body is 
	executed with x=hello, then x=world and x=howdy. If IFS is explicitly changed to contain a newline 
	only, then the loop is executed for hello world and howdy. If IFS is changed to be o, then the 
	loop is executed for hell,  w, rld␤h (where ␤ is a newline character) and wdy.

	





### SSH TUNNEL
# orgin
	$ ssh -N -R 8822:localhost:22 remote.host.com
	The optional -N says "don't execute a command" (helpful to prevent accidents caused by leaving 
	remote shells laying around.)

	Now from remote, you can SSH to host1 like this: (The remote port 8822 forwards to host1, but 
	only on the loopback interface.)

# remote
	$ ssh -p 8822 localhost
	For extra credit, you can export the forwarding to the whole world, allowing anyone get to host1 
	by hitting remote's port 8822. (Note the extra initial colon)

# orgin
	$ ssh -N -R :8822:localhost:22 remote.host.com



	
	
############### 
# System Info #
###############

1. ssh -i .ssh/pem_key 		user_id@IP_Address

   ssh -p <port>       		user_id@IP_Address

   ssh-copy-id 	       		user_id@IP_Address

   # CentOS sudo without passwd
   
	$ sudo visudo   (old way   vi /etc/sudoers)
	  nozatech ALL=(ALL:ALL) NOPASSWD:ALL
	  %sudo    ALL=(ALL:ALL) NOPASSWD:ALL
	
	# Run specific 'script' without password prompt! 
	$ username  ALL=(ALL) NOPASSWD: /home/username/pydatertc.sh	
	
	# Ubuntu sudo without passwd
	$ sudo visudo
 		# User privilege specification
		root    ALL=(ALL:ALL) ALL
		apark   ALL=(ALL:ALL) NOPASSWD:ALL									<= Add NOPASSWD
		# Members of the admin group may gain root privileges
		%admin ALL=(ALL) NOPASSWD:ALL										<= Add NOPASSWD
		# Allow members of group sudo to execute any command
		%sudo   ALL=(ALL:ALL) NOPASSWD:ALL									<= Add NOPASSWD

	
	
	
	
	# Allowing/Denying User-Level Cron
	$ /etc/cron.allow

2 Processor
	killall <process name>
    kill -9 
	kill 15
	pkill <pid>  		<		= Kill all child processes, too.
	ps -u <user_id>				<= u (user)
		
		
3. Linux CentOS or Ubuntu Version check	
    $ cat /etc/*rel*
    $ uname -a				<= all include kernel, processor(32/64bit), hostname, 
    $ uname -r 				<= kernel version
    $ cat /proc/version

	###Install lsb_release on CentOS7
	$ yum provides */lsb_release
	$ yum install redhat-lsb 
	$ lsb_release -a
   

    $ yum provides
	When you install sw and got a message saying that "xxx requires xxx library and xxx are missing."
	Then run 'yum provides "xxx"' to check dependencies.
	
	

   
4. ulimit 		
	system wide, user resource limits
	
	OS needs memory to manage each open file
	$ ulimit -n 						<= Find open files limit per process
	$ lsof | wc -l						<= Count all opened files by all process
	$ cat /proc/sys/fs/file-max			<= Get maximum open files count allows
	$ cat /proc/sys/fs/file-nr			<= to get the current number of open files from the Linux kernel's point of view
	
	Example: This server has 40096 out of max 65536 open files, although lsof reports a much larger number:
	$ cat /proc/sys/fs/file-max
		65536
	$ cat /proc/sys/fs/file-nr 
		40096   0       65536
	$ lsof | wc -l
		521504
	
	### System-wide File Descriptors (FD) Limits  ###
	
	The Number Of Maximum Files Was Reached, How Do I Fix This Problem?????
	$ cat /proc/sys/fs/file-max
	  65536
	
	$ sudo sysctl -w fs.file-max=800000             <= change from 65536 to 800000
	# (or) $ echo 800000 > /proc/sys/fs/file-max
	
	$ cat /proc/sys/fs/file-max
	  800000
	
	$ vi /etc/sysctl.conf 
		fs.file-max=800000			<= append so that after reboot, still load 800000
	#(or) $ echo 'fs.file-max=800000" >> /etc/sysctl.conf
	#exit & relogin
	
	$ sudo sysctl -p
	
	$ cat /proc/sys/fs/file-max
		800000
	# (or)$ sysctl fs.file-max
		fs.file-max = 800000
	
	### User Level FD Limits
	$ cat /etc/security/limits.conf
	# for user 
	apark  -  nofile    64000
	
	# for all users
	*    soft    nofile 8192
	*    hard    nofile 8192
	
#---------------------------------------------------------------------------------------------------------	
	There are two limits in play: 
#-----------------------------------------------------------------------------------------------------
	1) the maximum number of open files the OS kernel allows (fs.file-max) and
	2) the per-user limit (ulimit -n). The former(Kernel) must be higher than the latter(user).
	
	
	If you are getting error “Too many open files (24)” then your application/command/script is hitting max 
	open file limit allowed by linux. You need to increase open file limit as below:
	
	### Check
	$ ulimit -a			<= All current limits
	$ ulimit -n			<= maximum number of open file
	=> 1024
	
	$ ulimit -Sn		<= soft limit number
	$ ulimit -Hn		<= hard limit number
	
	
	$ cat /proc/sys/fs/file-max
	 784694	
	
	### setting a new value in kernel variable /proc/sys/fs/file-max
	$ sysctl -w fs.file-max=100000   					 <= -w write
	
	$ echo "fs.file-max = 100000"  >>  /etc/sysctl.conf
	
	$ sysctl -p		( or --load)						 <= load 
	  fs.file-max = 100000
	
	$ # cat /proc/sys/fs/file-max 						 (or  $ sysctl fs.file-max )
	

	### Specific user limits
	$ vi /etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf/etc/security/limits.conf		<= add this lines
	###
	*       soft     nproc          65535		<= * all users but no root
	*       hard     nproc          65535
	*       soft     nofile         65535
	*       hard     nofile         65535
	apark    -       nofile         65535		<= apark specific
	root     -       nofile         65535		<= root specific
	###
	
	### Per-User Limit
	$ cat /etc/security/limits.conf
	#<domain>      <type>  <item>         <value>
	#root            hard    core            100000		<=100k files to open
	#@student        hard    nproc           20			<= only 20 files to open
	
	### pam-limits							<= session-related modules common to all services
	$ cat /etc/pam.d/common-session 		
	
	### System-Wide Limit
	$ echo 'fs.file-max = 2097152' >> /etc/sysctl.conf 			<= configure kernel parameters at runtime
	
	# configure kernel parameters at runtime
	$ sysctl -p 												<= -p Load in sysctl settings from the file specified or /etc/sysctl.conf 
											
	### Verify New Limits
	$ cat /proc/sys/fs/file-max 			<= max limit of file descriptors:
	2097152
		
	### Check limit for other user
	$ su - www-data -c 'ulimit -aHS' -s '/bin/bash'
	$ su - noza -c 'ulimit -aHS' -s '/bin/bash'
		core file size          (blocks, -c) 0
		data seg size           (kbytes, -d) unlimited
	
	
	### Check limits of a running process:
	$ cat /proc/Process_ID/limits
		Limit                     Soft Limit           Hard Limit           Units
		Max cpu time              unlimited            unlimited            seconds
	
	
	####
	$ echo 'fs.inotify.max_user_watches=100000' | sudo tee -a /etc/sysctl.conf; 			<=tee -a(append)
	$ sudo sysctl -p
	####
	
	https://help.ubuntu.com/community/RootSudo
	$ ls | sudo tee -a /root/somefile 
	
	# pass the whole command to a shell process run under sudo to have the file written to with root permissions
	$ sudo sh -c "ls > /root/somefile"
	


	
#### UBUNTU  ###	
http://stackoverflow.com/questions/21515463/how-to-increase-maximum-file-open-limit-ulimit-in-ubuntu

echo "* soft nofile 102400" > /etc/security/limits.d/*_limits.conf && \
echo "* hard nofile 102400" >> /etc/security/limits.d/*_limits.conf 
####	
	
	$ man bash  					
	/ulimit  				<= ulimit is built in shell
	-S   Change and report the soft limit associated with a resource. 
    -H   Change and report the hard limit associated with a resource. 
    -a   All current limits are reported. 
    -c   The maximum size of core files created. 
    -d   The maximum size of a process's data segment. 
    -f   The maximum size of files created by the shell(default option) 
    -l   The maximum size that can be locked into memory. 
    -m   The maximum resident set size. 
    -n   The maximum number of open file descriptors. 
    -p   The pipe buffer size. 
    -s   The maximum stack size. 
    -t   The maximum amount of cpu time in seconds. 
    -u   The maximum number of processes available to a single user. 
    -v   The maximum amount of virtual memory available to the process. 
	###
	
#-------------------------------------------------------------------------------------------------------------
	
	
5. sysctl  			<= Configure kernel parameters at runtime
	The ulimit and sysctl programs allow to limit system-wide resource use. 
	This can help a lot in system administration, e.g. when a user starts too 
	many processes and therefore makes the system unresponsive for other users.

	
	
	
	
6. Swap space swapon
	swapon -s 		<= Summary
		   -a 		<= All
	
	sudo dd if=/dev/zero of=/var/swapfile bs=1M count=2048
	sudo chmod 600 /var/swapfile
	sudo mkswap /var/swapfile
	echo /var/swapfile none swap defaults 0 0 | sudo tee -a /etc/fstab
	sudo swapon -a

  
	sudo dd if=/dev/zero of=/mnt/{filename}.swap bs=1M count={swap_size}
	sudo mkswap /mnt/{filename}.swap
	sudo swapon /mnt/{filename}.swap
	sudo vi /etc/fstab
	Add the following text at the end of the file, 
	/mnt/{filename}.swap  none  swap  sw  0 0
  

  ### Create a swap space script ### 
	#!/bin/sh
	# make swap space s
	dd if=/dev/zero of=/swapfile bs=1024 count=8388608
	mkswap /swapfile
	echo '/swapfile         swap            swap    defaults 0 0' >> /etc/fstab
	swapon -a
	swapon -s
	###
  
  ### creating random data for 10MB # dd <= covert and copy a file
    dd if=/dev/urandom of=foo bs=1000 count=10000     
		^ if=FILEread from FILE instead of stdin
		                ^ of=FILE, write to FILE instead of stdout
							  ^ bs=bytes, read and write up to BYTES bytes at a time



  
7. Start Up service for Ubuntu
  
	sudo mv /filename /etc/init.d/
	sudo chmod +x /etc/init.d/filename 
	sudo update-rc.d filename defaults 
	
	
	
  
8. UUID  <= identifier for block devices. 
	UUIDs are 128 bit long numbers represented by 32 hexadecimal digits and which are used in 
	software development to uniquely identify information with no further context. 
	
	#Linux implementation and generation
	 In Linux UUIDs are generated in /drivers/char/random.c?id=refs/DATEs/v3.8, and you can generate new ones via proc:
	# Usage in fstab
	As mentioned UUIDs are most often used in Linux to identify block devices. Imagine, you have a couple of hard disks 
	attached via USBs, than there is no persistent, reliable naming of the devices: sometimes the first USB hard disk is 
	named “sda”, sometimes it is named “sdb”. So to uniquely address the right disk for example in your /etc/fstab, you have to add an entry like:
	
	UUID=9043278a-1817-4ff5-8145-c79d8e24ea79 /boot ext3 defaults 0 2

	For the block device itself, the uuid is stored in the superblock.
	Beware however that UUIDs should not be used in fstab when you work with LVM snapshots(no 2 device using same uuid). 
	
	
	
	$ cat /proc/sys/kernel/random/uuid
		eaf3a162-d770-4ec9-a819-ec96d429ea9f
	
	# There is also the library libuuid which is used by uuidgen and especially 
		by the ext2/3/4 tools E2fsprogs to generate UUIDs:
	$ uuidgen 
	f81cc383-aa75-4714-aa8a-3ce39e8ad33c
	
	# bash style
	$ls -l /dev/disk/by-uuid
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 116a4716-c5ec-4ce5-aea4-9fea29d78f76 -> ../../dm-1
	lrwxrwxrwx. 1 root root  9 Apr 13 03:17 2015-12-09-23-03-16-00 -> ../../sr0
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 3479cc28-9e7d-4798-abef-a50bafef8761 -> ../../dm-0
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 62f1fbbf-e39b-4140-90ed-3d88a7988fa5 -> ../../sda1

	
	$sudo blkid /dev/sda1
	/dev/sda1: UUID="62f1fbbf-e39b-4140-90ed-3d88a7988fa5" TYPE="xfs"
	
	$ udevadm info -q all -n /dev/sda1|grep uuid
	S: disk/by-uuid/62f1fbbf-e39b-4140-90ed-3d88a7988fa5
	E: DEVLINKS=/dev/disk/by-path/pci-0000:00:10.0-scsi-0:0:0:0-part1 /dev/disk/by-uuid/62f1fbbf-e39b-4140-90ed-3d88a7988fa5
	
	### When not to use UUID ###
	Since it is not possible to mount two file systems with the same UUID, extra care need to be taken 
	when LVM snapshots (or cloned disks) are used in an environment: mounting might fail due to duplicate UUIDs.
	
	XFS: Filesystem dm-2 has duplicate UUID – can’t mount
	
	One way to deal with this is by the way to change the UUID during creation or afterwards, 
	another way is to mount with the nouuid option.

### Package List
	#Ubuntu
	$ dpkg -l | grep mysql
	
	# CentOS
	$ rpm -qa | grep mysql


#########################
# 5. PROCESS HANDLING.  #
#########################

To suspend a job, type CTRL+Z while it is running. You can also suspend a job with CTRL+Y. 
This is slightly different from CTRL+Z in that the process is only stopped when it attempts to 
read input from terminal. Of course, to interupt a job, type CTRL+C.


myCommand &  # runs job in the background and prompts back the shell

jobs         # lists all jobs (use with -l to see associated PID)

fg           # brings a background job into the foreground
fg %+        # brings most recently invoked background job
fg %-        # brings second most recently invoked background job
fg %N        # brings job number N
fg %string   # brings job whose command begins with string
fg %?string  # brings job whose command contains string

kill -l      # Options, returns a list of all signals on the system, by name and number
kill PID     # terminates process with specified PID
killall
pkill

$ pgrep nginx		# pgrep, pkill - look up or signal processes based on name and other attributes
$ pkill nginx		# kill all child process that spawned from nginx process id

$ pgrep -u apark	# process grep all apark's process
$ pkill -u apark	# kill all apark's process


kill 	PID			# same as 'kill 15'
kill 15 PID			# gracefully shutdown (terminate)
kill 9	PID			# force to kill
killall name 		#




### kill all processes relate to same process name
	$ pkill -f s3cmd
	$ ps -ef | grep myProcessName | grep -v grep | awk '{print $2}' | xargs kill -9


ps           # prints a line of information about the current running login shell and any processes running under it
ps -a        # selects all processes with a tty except session leaders


ps -ef	vs ps aux
ps aux 				<= BSDsystem
ps -ef 				<= System V system


ps aux | grep RRServer | grep -v grep | awk '{print $2}'



###  How to kill zombie process
		A zombie is already dead, so you cannot kill it. To clean up a zombie, it must be waited on by its parent, 
		so killing the parent should work to eliminate the zombie. (After the parent dies, the zombie will be 
		inherited by init or systemD (process ID 1), which will wait on it and clear its entry in the process table.) 
		If your daemon is spawning children that become zombies, you have a bug. Your daemon should notice 
		when its children die and wait on them to determine their exit status.

		Example command:
		kill $(ps -A -ostat,ppid | awk '/[zZ]/{print $2}')

			
###	
	Zombie processes (also show as <defunct>), aren't real processes at all. They are just entries in the kernel process table. 
	This is the only resource they consume. They do not consume any CPU or RAM. 
	*** The only danger of having zombies, is running out of space in process table!!!***
	
	$ cat /proc/sys/kernel/threads-max 
		30112
		
	They appear only when their parent process (i.e. process which fork()'ed them) is alive, but did not yet call wait() 
	system function. Once parent dies, the zombies are wait()'ed for by init and disappear.
###


trap cmd sig1 sig2  # executes a command when a signal is received by the script
trap "" sig1 sig2   # ignores that signals
trap - sig1 sig2    # resets the action taken when the signal is received to the default

disown <PID|JID>    # removes the process from the list of jobs

wait                # waits until all background jobs have finished



6. TIPS AND TRICKS.


# to quickly go to a specific directory
cd; nano .bashrc
> shopt -s cdable_vars
> export websites="/Users/mac/Documents/websites"

source .bashrc
cd websites
cd *		<= if there is only one folder


############################################
###
############################################
1. Find and Sort Files Based on Modification Date and Time
	A. List Files Based on Modification Time
		ls -lt
	B. List Files Based on Last Access Time
		


1. Grep 
grep  pattern  file_name   #search for pattern in files
grep -i			# Case insensitive search
grep -r 		# Recursive
	$ grep -r "port 80" /etc/httpd/*   <= search subdirectories with key words port 80.


	$ grep -v httpd			# Inverted search (everything but httpd)
	$ grep -o				# Show matched part of file only
	$ grep -v grep    		# don't match the grep word
    To remove the grep in search word, use "grep -v grep" 2nd grep is a word to -v <= don't match word .
	
	$ ps aux | grep some_PID => out puts grep word in list, and it counts the line as 1 + 1 actual process name some_PID
	



#######################
#!/bin/bash
# Check and put ok status if the RRS game service is running for Lobby 7504 & 7505 services

if (( $(ps aux | grep ./RRServer_0617_7504 | grep -v grep | wc -l) > 0  &&  \
$(ps aux | grep ./RRServer_0617_7505 | wc -l) > 0 ));
        then
        echo OK > /var/www/html/status/index.html;
else rm /var/www/html/status/index.html > /dev/null 2>&1;
fi
#################

### 
#Find biggest file Sizes top 5 list Only
###
	### NCurses Disk Usage
	$ ncdu /

	$ ncdu -x /    <= scan a full filesystem
		# Since scanning a large directory may take a while, you can scan a directory and export the results for later viewing:
	
	$ ncdu -1xo- / | gzip > ncdu-list.gz
	# ...some time later:
	
	$ zcat export.gz | ncdu -f-
	
	
    $ find / -type f -exec du -Sh {} + | sort -rh | head -n 5
	
	$ find / -type f -exec du -Sh {} + | sort -rh | head -n 20  <= Top 20 list
	
	$ find /*/* -type f -exec du -Sh {} + | sort -rh | head -n 5
	
	$ find /dir/ -user user_ID			#Find files owned by name in dir
	$ find /dir/ -name file_name
	$ find /dir/ -iname fine_name
	
	$ whereis 					<= Find binary /source / manual for command
	$ locate file_name		 	<= DB based Find file 
	
	$ find /etc | grep -e ulimit -e 4096 -e nofile   		<= e <-pattern (multi pattern search )


find /lib64 | grep mysql
/lib64/libmysqlcppconn.so
/lib64/libmysqlcppconn.so.7.1.1.3
/lib64/libmysqlclient.so.18.2.0
/lib64/libmysqlcppconn.so.7
/lib64/libmysqlclient.so.18






####
Tmux  Terminal Multiplexer

ctrl-b <command>
ctrl-b c - new window


#####
Ubuntu Package Installed 
$ dpkg -l | grep mongo

root@puppet:~# dpkg --get-selections
root@puppet:~# dpkg --get-selections | grep puppet
puppet-common                                   install
puppetlabs-release                              install
puppetmaster-common                             install
puppetmaster-passenger                          install



####
sed (stream editor)
sed -i 's/START=no/START=yes/g'    /etc/default/puppet
          ^exiting ^change to		^ location of file	

	#hostname change from u14 to rieman in hosts file
	$ sed -i 's/u14/rieman/g' /etc/hosts
		  
		  
	$ sed -i '/extern int errno/{s/^/\/* /;s/$/ *\//;G;s/$/#include <errno.h>/;}' src/error.h
	
	
	
		  
### Add some rows and text
$ cat << EOF >> /etc/puppet/puppet.conf
[agent]
server = puppetmaster.domain.tld
EOF


The cat <<EOF Bash syntax is very useful when one needs to work with multiline strings in Bash, 
	eg. when passing multiline string to a variable, file or a piped command.
1. Passing multiline string to a variable:
	$ sql=$(cat <<EOF
	> SELECT foo, bar FROM db
	> WHERE foo='baz'
	> EOF
	> )
	
noza@fm:~$ echo $sql
SELECT foo, bar FROM db WHERE foo='baz'

noza@fm:~$ echo -e $sql
SELECT foo, bar FROM db WHERE foo='baz'

noza@fm:~$ echo -e "$sql"
SELECT foo, bar FROM db
WHERE foo='baz'
The $sql variable now holds newlines as well, you can check it with echo -e "$sql" cmd.

2. Passing multiline string to a file:

$ cat <<EOF > print.sh
#!/bin/bash
echo \$PWD
echo $PWD
EOF
The print.sh file now contains:

#!/bin/bash
echo $PWD
echo /home/user

3. Passing multiline string to a command/pipe:

$ cat <<EOF | grep 'b' | tee b.txt | grep 'r'
foo
bar
baz
EOF
This creates b.txt file with both bar and baz lines but prints only the bar.


"EOF" is known as a "Here DATE". Basically << Here tells the shell that you are going to enter a multiline string until the "DATE" Here. You can name this DATE as you want, it's often EOF or STOP.
	- The DATE can be any string, uppercase or lowercase, though most people use uppercase by convention.
	- The DATE will not be considered as a Here DATE if there are other words in that line. In this case, it will merely be considered part of the string. The DATE should be by itself on a separate line, to be considered a DATE.
	- The DATE should have no leading or trailing spaces in that line to be considered a DATE. Otherwise it will be considered as part of the string.
example:

$ cat >> test <<HERE
> Hello world HERE <--- Not the end of string
> This is a test
>  HERE <-- Leading space, so not end of string
> and a new line
> HERE <-- Now we have the end of the string
More: http://stackoverflow.com/questions/2500436/how-does-cat-eof-work-in-bash




####
Install Fabric, boto, yaml

   1) Install fabric, boto and yaml

         wget https://bootstrap.pypa.io/get-pip.py
         sudo python get-pip.py
         sudo pip install fabric
         sudo pip install boto
         sudo pip install PyYaml

		 



		 
#################	 
### Security  ###
#################

1.Verifying Which Ports Are Listening
CentOS
#which ports are listening for TCP connections from the network:
	nmap -sT -O localhost   # -sT <= scan TCP  -O <= OS

#To check if the port is associated with the official list of known services	
	cat /etc/services | grep unknown_port

	# check for information about the port	
	netstat -anp | grep unknown_port     # -anp  <= All, no dns lookup(faster), program
	
	lsof -i | grep port_number		<= internet connection open
	lsof -p                      	<= process_number
	
	
	
2. Changing SSH port from default #22 to 2222 
	$ vi /etc/ssh/sshd_config
		#22 to 2222 
	$ /etc/init.d/sshd restart
	
	Update the IPTables
	$ iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
	
	Allow all from x.x.x.x IP 
	-A INPUT -m state -s x.x.x.x --state NEW -j ACCEPT

###
Allow everything from single IP using cmd	
###------------------------------------------------------------------
$ iptables -A INPUT -m state -s 65.87.26.0/24 --state NEW -j ACCEPT			<= BNEA Corp IP

$ iptables -A INPUT -m state -s 64.95.137.0/24 --state NEW -j ACCEPT		<= BNEA PacCity WIFI Comcast IP

$ iptables -A INPUT -m state -s 73.231.235.144/32 --state NEW -j ACCEPT   	<= Apark Home
###	-----------------------------------------------------------------

# iptables -t [filter|nat|mangle] [-A|-D|-L|-F] [INPUT|OUTPUT|FORWARD] -p [tcp|udp] -m [tcp|udp] -s [sourceip] -d [destip] --dport [port] -j [DROP|REJECT|ACCEPT] 

# iptables -t [filter] [-A] [INPUT   ]   -p [tcp] -m [tcp] -s [sourceip] -d [destip] --dport [port] -j [ DROP   ] 	
			   nat	    -D   OUTPUT          udp      udp												 REJECT
			   mangle	-L   FORWARD																     ACCEPT
						-F

# http://www.iana.org/assignments/icmp-parameters/icmp-parameters.xhtml#icmp-parameters-codes-8
# allow ping from IPs
$ iptables -A INPUT -s x.x.x.x -p ICMP --icmp-type 8 -j ACCEPT

# block all ping
$ iptables -A INPUT -p ICMP --icmp-type 8 -j DROP


### Ubuntu Port Forwarding using Iptables

	# Using Linux iptables for port 80 -> 8080
	
	This enables port forwarding of traffic between ports 80 and 8080. 
	You can keep Jenkins on the default port 8080.
	
	$ sudo vi /etc/rc.local
	Then add the following just before the exit 0
	
	#Requests from outside
	iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080
	
	#Requests from localhost
	iptables -t nat -I OUTPUT -p tcp -d 127.0.0.1 --dport 80 -j REDIRECT --to-ports 8080
	
	Now reboot or run sudo /etc/rc.local to enable port forwarding. 
	Additional info: https://gist.github.com/m5m1th/6870a54717c0387468c3








###########
### SSL ###
###########

DSA is faster in signing, but slower in verifying. A DSA key of the same strength as RSA (1024 bits) 
generates a smaller signature. An RSA 512 bit key has been cracked, but only a 280 DSA key. 
Also note that DSA can only be used for signing/verification, whereas RSA can be used for 
encryption/decrypt as well.


SSL has been around for long enough you'd think that there would be agreed upon container formats. And you're right, there are. Too many standards as it happens. So this is what I know, and I'm sure others will chime in.

.csr This is a Certificate Signing Request. Some applications can generate these for submission to certificate-authorities. The actual format is PKCS10 which is defined in RFC 2986. It includes some/all of the key details of the requested certificate such as subject, organization, state, whatnot, as well as the public key of the certificate to get signed. These get signed by the CA and a certificate is returned. The returned certificate is the public certificate (which includes the public key but not the private key), which itself can be in a couple of formats.
.pem Defined in RFC's 1421 through 1424, this is a container format that may include just the public certificate (such as with Apache installs, and CA certificate files /etc/ssl/certs), or may include an entire certificate chain including public key, private key, and root certificates. Confusingly, it may also encode a CSR (e.g. as used here) as the PKCS10 format can be translated into PEM. The name is from Privacy Enhanced Mail (PEM), a failed method for secure email but the container format it used lives on, and is a base64 translation of the x509 ASN.1 keys.
.key This is a PEM formatted file containing just the private-key of a specific certificate and is merely a conventional name and not a standardized one. In Apache installs, this frequently resides in /etc/ssl/private. The rights on these files are very important, and some programs will refuse to load these certificates if they are set wrong.
.pkcs12 .pfx .p12 Originally defined by RSA in the Public-Key Cryptography Standards, the "12" variant was enhanced by Microsoft. This is a passworded container format that contains both public and private certificate pairs. Unlike .pem files, this container is fully encrypted. Openssl can turn this into a .pem file with both public and private keys: openssl pkcs12 -in file-to-convert.p12 -out converted-file.pem -nodes
A few other formats that show up from time to time:

.der A way to encode ASN.1 syntax in binary, a .pem file is just a Base64 encoded .der file. OpenSSL can convert these to .pem (openssl x509 -inform der -in to-convert.der -out converted.pem). Windows sees these as Certificate files. By default, Windows will export certificates as .DER formatted files with a different extension. Like...
.cert .cer .crt A .pem (or rarely .der) formatted file with a different extension, one that is recognized by Windows Explorer as a certificate, which .pem is not.
.p7b Defined in RFC 2315, this is a format used by windows for certificate interchange. Java understands these natively. Unlike .pem style certificates, this format has a defined way to include certification-path certificates.
.crl A certificate revocation list. Certificate Authorities produce these as a way to de-authorize certificates before expiration. You can sometimes download them from CA websites.
In summary, there are four different ways to present certificates and their components:

PEM Governed by RFCs, it's used preferentially by open-source software. It can have a variety of extensions (.pem, .key, .cer, .cert, more)
PKCS7 An open standard used by Java and supported by Windows. Does not contain private key material.
PKCS12 A private standard that provides enhanced security versus the plain-text PEM format. This can contain private key material. It's used preferentially by Windows systems, and can be freely converted to PEM format through use of openssl.
DER The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. Not routinely used by much outside of Windows.



#####################################################
###  mistakenly deleted /boot folder and rebooted ###
#####################################################
1. Bootup with Live CD.

2. Find the drive/partition where you have installed your root filesystem. 
	To do this you can open a terminal and run either 
	$ sudo parted -l     # parted  is  a  disk  partitioning  and  partition resizing program.o
	$ sudo fdisk -l 	 #  a menu-driven program for creation and manipulation of partition tables.

3. Assuming that your root partition that you found from the last step is /dev/sda1
	Since Linux 2.4.0 it is possible to remount part of the file hierarchy somewhere else. The call is 'mount --bind old_dir new_dir'

	$ mkdir mnt
	$ sudo mount /dev/sda1 mnt
	$ sudo mount --bind /dev /mnt/dev
	$ sudo mount --bind /proc /mnt/proc
	$ sudo mount --bind /sys /mnt/sys
	$ sudo chroot mnt						<= chroot - run command or interactive shell with special root directory

4. You will now be inside a chroot environment meaning that running commands here is equivalent to running them on your installed system. 
	The first thing you want to do is reinstall GRUB2 to the device so that it copies the correct files into the /boot folder. 
	To do this run the following with the drive that your root partition is on (ie /dev/sda1 ):

	$ grub-install /dev/sda
	You now want to find out which packages you have installed that have files in the boot directory and reinstall them. 
	This will replace the kernel images that have been deleted among other things. The command to find the packages is:

5. $ dpkg -S /boot
		memtest86+, linux-image-3.13.0-32-generic, base-files: /boot
		 And to reinstall them:

6. $ apt-get --reinstall install linux-image-3.13.0-32-generic
	
	This step will probably require internet access (unless the packages are already in the cache), so make sure you are connected if there is an problem.

	Since you will have deleted your kernels and reinstalled them, this should have triggered a GRUB2 update automatically. But just in case they haven't, you can run:

7. $ update-grub
	Reboot and things should now be fixed. 
	One issue that I had the last time I did something similar was that Windows installs where not found by update-grub 
	One issue that I had the last time I did something similar was that Windows installs where not found by update-grub 
	when run in the chroot due to a bug in os-prober. If this is an issue, just run sudo update-grub again in the repaired system.

	
CentOS package update vs upgrade	
yum upgrade and yum update will perform the same function that update to the latest current version of package.

But the difference is Upgrade will delete obsolete packages, while update will preserve them.


### CHROOT
	Using chroot cmd to switch different file systems
	e.g. Host OS kernel, module, drivers on host file system1(Ubuntu /dev/sda1) can modify on file system2(Debina /dev/sda2) 

Ubuntu 
$nmap 			<- works
$ cd /debian
$ chroot . bash
Debian 			<- Using host kernel but running on Debian file system to modify
$ nmap 			<- need to install
$ exit


### Byobu multi screen utility ###
# CentOS 7
$ sudo yum install --enablerepo=epel byobu

	$ byobu

	F2 <= New terminal   
	Moving      F3 <=   => F$
	
	Shift+F2  <= Split 1/2 Vertically
	Ctrl +F2  <= Split 1/2 Horizontally

	# Make 4 Split screens with in 1 terminal
	Shift+F2 => Ctrl+F2 => Shift+4 => Ctrl+F2
	
	# Moving around
	Shift+F4
	
	# Too kill
	Ctrl+d    <=Exit
	
# Ubuntu 14
    $ byobu-enable/disable
	$ ctrl+F2
	
DOS - Remove all files including subdirectory
# Remove All directory with data and sub
rmdir "c:/temp/1" /s/q  







################################################################################################################
### Memcache  ##################################################################################################
################################################################################################################

$ watch -n1 'memcached-tool localhost stats'
















################################################################################################################
### Nginx  ##################################################################################################
################################################################

1. Nginx log rotate

$ mv /path/to/access.log /path/to/access.log.0
$ kill -USR1 `cat /var/run/nginx.pid`
$ sleep 1
[ post-rotation processing of old log file ]
	# rotates the logs is "kill -USR1 /var/run/nginx.pid". This does not kill the Nginx process, 
	# but instead sends it a signal causing it to reload its log files. This will cause new 
	# requests to be logged to the refreshed log file.


	
	
	
###
#  If the Nginx is running with PID, it copies to the /var/run/nginx.pid 	
$ ps aux | grep nginx
$ cat /var/run/nginx.pid














##################
### Log rotate ###
##################
to check logrotate, run cmd

$ logroate

### Configurations and default options
/etc/logrotate.conf

### Application-specific log file information (to override the defaults) 
$ ll /etc/logrotate.d/
-rw-r--r-- 1 root root 185 Feb  4  2016 httpd
-rw-r--r-- 1 root root 871 May 11 02:31 mysqld
-rw-r--r-- 1 root root 136 May  5 01:27 newrelic-sysmond


#### The logrotation for dpkg monitors the /var/log/dpkg.log file and does this on a 
/var/log/dpkg.log {
	monthly					# monthly basis - this is the rotation interval.
	size 100M				#### <===logs are rotated once the file size reaches 100M and this need not wait for the monthly cycle.
	rotate 12				# 12 days worth of logs would be kept.
	compress				# logfiles can be compressed using the gzip format by specifying
	delaycompress			# will work only if 'compress' option is specified.
	missingok				# avoids halting on any error and carries on with the next log file.
	notifempty				# avoid log rotation if the log file is empty.
	create 644 root root	# create <mode> <owner> <group> creates a new empty file with the specified properties after log-rotation.
}
####
	# Though missing in the above example, 'size' is also an important setting if you want to control the sizing of the logs growing in the system.

### Example ###	
$ cat /etc/logrotate.d/httpd
####
/var/log/httpd/*log {
    missingok
    notifempty
    sharedscripts
    delaycompress
    postrotate
        /sbin/service httpd reload > /dev/null 2>/dev/null || true
    endscript
}
####



### using Cron Job ###
	You can also set the logrotation as a cron so that the manual process can be avoided and this is taken care of automatically. 
	By specifying an entry in /etc/cron.daily/logrotate , the rotation is triggered daily.

	$ /etc/cron.daily/logrotate
###
#!/bin/sh

/usr/sbin/logrotate /etc/logrotate.conf
EXITVALUE=$?
if [ $EXITVALUE != 0 ]; then
    /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"
fi
exit 0
###	
	
	

##########################################
### CUSTOM LOG FILE Rotation logrotate ###
##########################################

$ vi /etc/logrotate.d/rrserver

#####
/RRLinux/*log {
    weekly
    size 1G
    rotate 10
    missingok
    notifempty
    delaycompress
    postrotate
        /RRLinux/restart-eu-s1.sh > /dev/null		#switch to new log file
    endscript
}

#####
	

### Check log 	
$ cat /var/lib/logrotate.status

"/RRLinux/run.log" 2016-10-24     <=== New logrotate creation for /etc/logrotate.d/rrserver




### Testing logrotate.conf ###

$ logrotate -vdf /etc/logrotate.conf
		verbose flag, “-v”  
		debug flag,   “-d” 
		force flag,   "-f"

		
rotating pattern: /RRLinux/*log  forced from command line (10 rotations)
empty log files are not rotated, old logs are removed
considering log /RRLinux/error.log
  log does not need rotating

considering log /RRLinux/RidgerRacer_Server_vSlave.log
  log needs rotating

considering log /RRLinux/run.log
  log needs rotating
running postrotate script
running script with arg /RRLinux/error.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
rotating log /RRLinux/RidgerRacer_Server_vSlave.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
glob finding old rotated logs failed

renaming /RRLinux/RidgerRacer_Server_vSlave.log to /RRLinux/RidgerRacer_Server_vSlave.log-20161024
creating new /RRLinux/RidgerRacer_Server_vSlave.log mode = 0644 uid = 0 gid = 0

running postrotate script
running script with arg /RRLinux/RidgerRacer_Server_vSlave.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
rotating log /RRLinux/run.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
glob finding old rotated logs failed

renaming /RRLinux/run.log to /RRLinux/run.log-20161024
creating new /RRLinux/run.log mode = 0644 uid = 0 gid = 0
running postrotate script
running script with arg /RRLinux/run.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"

	


### httpd logrotate check ###

$ logrotate -vdf /etc/logrotate.d/httpd

##########
reading config file /etc/logrotate.d/httpd
reading config info for /var/log/httpd/*log

Handling 1 logs

rotating pattern: /var/log/httpd/*log  forced from command line (no old logs will be kept)
empty log files are not rotated, old logs are removed
considering log /var/log/httpd/access_log
  log does not need rotating
considering log /var/log/httpd/error_log
  log does not need rotating
not running postrotate script, since no logs were rotated
########	
	
	
	
### specific logrotate run ###	

$ logrotate -vdf /etc/logrotate.d/rrserver


####	
reading config file /etc/logrotate.d/rrserver
reading config info for /RRLinux/*log
Handling 1 logs

#---------------------------------------------------------------------------------------
rotating pattern: /RRLinux/*log  forced from command line (10 rotations)
empty log files are not rotated, old logs are removed
considering log /RRLinux/error.log
  log does not need rotating
considering log /RRLinux/RidgerRacer_Server_vSlave.log
  log needs rotating
considering log /RRLinux/run.log
  log needs rotating
running postrotate script
running script with arg /RRLinux/error.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"

#---------------------------------------------------------------------------------------
rotating log /RRLinux/RidgerRacer_Server_vSlave.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
renaming /RRLinux/RidgerRacer_Server_vSlave.log.10 to /RRLinux/RidgerRacer_Server_vSlave.log.11 (rotatecount 10, logstart 1, i 1                                                              0),
.....                                                          
renaming /RRLinux/RidgerRacer_Server_vSlave.log.0 to /RRLinux/RidgerRacer_Server_vSlave.log.1 (rotatecount 10, logstart 1, i 0),                                                              
renaming /RRLinux/RidgerRacer_Server_vSlave.log to /RRLinux/RidgerRacer_Server_vSlave.log.1
running postrotate script
running script with arg /RRLinux/RidgerRacer_Server_vSlave.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
removing old log /RRLinux/RidgerRacer_Server_vSlave.log.11
error: error opening /RRLinux/RidgerRacer_Server_vSlave.log.11: No such file or directory


#---------------------------------------------------------------------------------------
rotating log /RRLinux/run.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
renaming /RRLinux/run.log.10 to /RRLinux/run.log.11 (rotatecount 10, logstart 1, i 10),
....
renaming /RRLinux/run.log.0 to /RRLinux/run.log.1 (rotatecount 10, logstart 1, i 0),
renaming /RRLinux/run.log to /RRLinux/run.log.1
running postrotate script
running script with arg /RRLinux/run.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
removing old log /RRLinux/run.log.11
error: error opening /RRLinux/run.log.11: No such file or directory
####	
	
	
73. Logrotate	
	Configurations and default options 
	$ /etc/logrotate.conf
	
	Application-specific log file information
	$ /etc/logrotate.d/
	
	 /etc/logrotate.d/dpkg. One of the entries in this file would be:

	/var/log/dpkg.log {
			monthly						<= this on a monthly basis - this is the rotation interval.
			rotate 12					<= 'rotate 12' signifies that 12 days worth of logs would be kept.
			compress					<= logfiles can be compressed using the gzip format by specifying 'compress'
			delaycompress				<= 'delaycompress' delays the compression process till the next log rotation. 'delaycompress' will work only if 'compress' option is specified.
			missingok					<= 'missingok' avoids halting on any error and carries on with the next log file.
			notifempty					<= 'notifempty' avoid log rotation if the logfile is empty.
			create 644 root root		<= 'create <mode> <owner> <group>' creates a new empty file with the specified properties after log-rotation.
	}

	Though missing in the above example, 'size' is also an important setting if you want to control the sizing of the logs growing in the system.
	A configuration setting of around 100MB would look like:
	size 100M
	
	# Cron Job setup
	0 0 * * * (or @daily)  /etc/cron.daily/logrotate
	$ cat /var/lib/logrotate/status 
	


	
	

74.  Apache Log

Check config file

	$ apachectl configtest
	https://httpd.apache.org/docs/2.4/programs/apachectl.html
	
	
$ while true; do tail -n0 -f /var/log/apache2/access.log > /tmp/tmp.log & sleep 2; kill $! ; wc -l /tmp/tmp.log | cut -c-2; done 2> /dev/null


	A. View Apache requests per minute

$ grep "16/Mar/2015:06" example.com | cut -d[ -f2 | cut -d] -f1 | awk -F: '{print $2":"$3}' | sort -nk1 -nk2 | uniq -c | awk '{ if ($1 > 10) print $0}'

	# grep "16/Mar/2015:06" example.com	
	  <= Use the grep command to only show hits from today during the 06th hour 
		from our Apache access log.
	# cut -d[ -f2 | cut -d] -f1	
	  <=Use the cut command with the -delimiter set to an opening bracket [ and print 
		out the -field of data that shows up 2nd, then use the cut command again 
		with the -delimter set to a closing bracket ] and print out the -field of 
		data that shows up 1st which gives us just the time stamp.
	# awk -F: '{print $2":"$3}'	
	  <=Use the awk command with the -Field delimiter set to a colon :, then print out 
		the $2nd column which is the hour, followed by the $3th column which is 
		the minute.
	# sort -nk1 -nk2 | uniq -c	
	  <= Sort the hits numerically by the 1st column which is the hour, then by the 
		2nd column which is the minute.
	# awk '{ if ($1 > 10) print $0}'	
	  <= Finally use the awk command with an if statement to only print out data when 
		the $1st column which is the number of hits in a minute is greater than 10.	
	

	
	
	
	
	
	
	
	
	

	
	
	
	
	
	
	
	
### Awk ### 
	Built-in Variables – FS, OFS, RS, ORS, NR, NF, FILENAME, FNR
	http://www.thegeekstuff.com/2010/01/8-powerful-awk-built-in-variables-fs-ofs-rs-ors-nr-nf-filename-fnr/?ref=binfind.com/web
	
	1. Awk FS Example: Input field separator variable.
	
	
	
	
	
	
	
	
	
	
### What is a .h file?
Files that contain the .h file extension are normally header files used with the C or C++ programming languages. 
.h files are commonly known by programmers as "header files". They may contain constants, function prototypes 
and external variable definitions.

The header files are included by source code files that need to use the constants or functions defined in 
the header file. For example, if you define a text string with your company name in a header file, 
this constant can be used everywhere you need to display or otherwise use the company name in your source code. 
Should you later decide to change the name, you only have to do it in the header file and recompile to make the 
change take effect everywhere.	
	
	
	
	
### Network Trobleshooting
	# Wireshark Filter <= https://www.youtube.com/watch?v=68t07-KOH9Y
	filters are here:
	ip.addr == 10.0.0.1
	tcp or dns
	tcp.port == 443
	tcp.analysis.flags
	!(arp or icmp or dns)
	follow tcp stream
	tcp contains facebook
	http.response.code == 200
	http.request
	tcp.flags.syn == 1
	
	### Tshark ### 
	# Dump and analyze network traffic for CLI environment and READ from GUI WIRESHARK!!!! 
	https://linuxsimba.com/tshark-examples
	
	$ yum install wireshark
	$ tshark -D  				<= Print a list of the interfaces on which TShark can capture
	
	$ tshark -w /tmp/capture.log -f "port 22" -i eth0 -P     
			# -w  <= Write raw packet data to outfile 
			# -f  <= Set the capture filter expression.
			# -i  <= Network interface 
			# -P  <= Decode and display the packet summary
			
	# http://explainshell.com/explain?cmd=tshark+-w+%2Ftmp%2Fdhcp.pcap+-f+%22port+67+or+port+68%22+-i+eth1+-P
	
	$ tshark -r /tmp/capture.log
	
	$ tshark -D
		1. eth0
    $ tshark -i 1 -a duration:10 -w /tmp/10secs.pcap
	$ tshark -r /tmp/10secs.pcap
	
			
	##  ldd 
		prints the shared objects (shared libraries) required by each program or shared object specified 
		on the command line.  An example of its use and output is the following:
	$ ldd /usr/sbin/sshd
	
	## Debug
	$ yum groupinstall -y 'Development Tools'
	$ debuginfo-install openssh-server-6.6.1p1-33.el7_3.x86_64
	$ yum info openssl
	
	$rpm -q --changelog openssl | grep CVE-2015-319
		- fix CVE-2015-3197 - SSLv2 ciphersuite enforcement
		- fix CVE-2015-3194 - certificate verify crash with missing PSS parameter
		- fix CVE-2015-3195 - X509_ATTRIBUTE memory leak
		- fix CVE-2015-3196 - race condition when handling PSK identity hint
	
	# change MTU size
	$ ifconfig eth0 mtu 1000
	
	
	
	### Ping
	- Is the remote host alive?		=> Host reachability
	- Is the network speed good? 	=> Network congestion
	- Is the remote host far? 		=> Travel length
	
	# Ping MTU discovery
	http://muzso.hu/2009/05/17/how-to-determine-the-proper-mtu-size-with-icmp-pings
	
	https://openmaniak.com/ping.php
	
	$ ping -M  do -s 1500 remote_host
			-M pmtudisc_opt
              Select  Path  MTU Discovery strategy.  pmtudisc_option may be either 
				do   <-(prohibit fragmentation, even local one), 
				want <-(do PMTU discovery, fragment locally when packet size is large),
                dont <-(do not set DF flag).
			-s packetsize
              Specifies  the number of data bytes to be sent.  The default is 56, 
			  which translates into 64 ICMP data bytes when combined with the 8 bytes 
			  of ICMP header data.
			1500 MTU size
			
			
	$ ping -M do -s 1500 45.55.5.69
	PING 45.55.5.69 (45.55.5.69) 1500(1528) bytes of data.
	ping: local error: Message too long, mtu=1500

	$ ping -M do -s 1400 45.55.5.69
	PING 45.55.5.69 (45.55.5.69) 1400(1428) bytes of data.
	1408 bytes from 45.55.5.69: icmp_seq=1 ttl=52 time=9.05 ms
	
	#------------------------------------------------------------------------------------
	#### Works solution ###
	$ vi /etc/ssh/ssh_config
	# uncomment on Ciphers and MACs under 
	#Host *
	Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc
	MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160
	#------------------------------------------------------------------------------------
	
	
	# TTL (Time TO LIVE)
	CentOS 	 64 TTL
	Windows 128 TTL
	Ubuntu  192 TTL
	
	### ICMP Packet 
	
	<----------><---------><-----------><--------->
	MAC Header 	 IP Header  ICMP Header  ICMP Data
	14-bytes	  20-bytes	 8-bytes
	<--------------------------------------------->     <= Ethernet Frame
				<--------------------------------->		<= IP Packet
				           <---------------------->		<= ICMP Packet
						   
						   
						   
						   
						   
						   
						   
						   
### tput 
		(http://linuxcommand.org/lc3_adv_tput.php)
		# When we start a terminal session on our Linux system, the terminal emulator sets the TERM environment 
		  variable with the name of a terminal type.	
		https://en.wikipedia.org/wiki/Tput		  
		# tput is a standard Unix operating system command which makes use of terminal capabilities.
		  Depending on the system, tput uses the terminfo or termcap database, as well as looking into the environment for the terminal type.
						   
						   
	### infocmp - compare or print out terminfo descriptions
		infocmp  can  be used to compare a binary terminfo entry with other terminfo entries, rewrite a
       terminfo description to take advantage of the use= terminfo field,  or  print  out  a  terminfo
       description  from  the  binary  file (term) in a variety of formats.  In all cases, the boolean
       fields will be printed first, followed by the numeric fields, followed by the string fields.				
						   
	$ echo $TERM
	$ infocmp screen
		#       Reconstructed via infocmp from file: /usr/share/terminfo/s/screen
		screen|VT 100/ANSI X3.64 virtual terminal,

	$ logname
	$ whoami
	
	The tput command can be used to test for a particular capability or to output the assigned value.
	$ tput logname
	xterm terminal emulator (X Window System)
	"screen" used by terminal multiplexers such as screen and tmux.
						   

						   
						   
						   
### ETL https://en.wikipedia.org/wiki/Extract,_transform,_load
	is short for extract, transform, load, three database functions that are combined into one tool to 
	pull data out of one database and place it into another database. Extract is the process of reading data from a database.						   
	
	
### UML	https://en.wikipedia.org/wiki/Unified_Modeling_Language
	The Unified Modeling Language (UML) is a general-purpose, developmental, modeling language in the field 
	of software engineering, that is intended to provide a standard way to visualize the design of a system.
	
### RSS https://en.wikipedia.org/wiki/RSS
	(Rich Site Summary; originally RDF Site Summary; often called Really Simple Syndication) uses a family of 
	standard web feed formats[2] to publish frequently updated information: blog entries, news headlines, 
	audio, video. An RSS document (called "feed", "web feed",[3] or "channel") includes full or summarized 
	text, and metadata, like publishing date and author's name.
	RSS feeds enable publishers to syndicate data automatically. A standard XML file format ensures 
	compatibility with many different machines/programs. RSS feeds also benefit users who want to receive 
	timely updates from favourite websites or to aggregate data from many sites
	
	
### Nexus or Artifictory for a Maven Repo	
	http://stackoverflow.com/questions/364775/should-we-use-nexus-or-artifactory-for-a-maven-repo
	
	
### /dev diretory 
	http://unix.stackexchange.com/questions/18239/understanding-dev-and-its-subdirs-and-files
	crw-r-----. 1 root kmem 1, 1 Feb  2 17:50 /dev/mem
	^ Character devices
	There are two types of device files: block devices (indicated by b as the first character in the output of ls -l), 
	and character devices (indicated by c).
	- Block Devices (indicated by b as the first character in the output of ls -l), and character 
	devices (indicated by c). The distinction between block and character devices is not 
	completely universal. Block devices are things like disks, which behave like large, 
	fixed-size files: if you write a byte at a certain offset, and later read from the device 
	at that offset, you get that byte back. 
	- Character devices are just about anything else, where writing a byte has some immediate 
	effect (e.g. it's emitted on a serial line) and reading a byte also has some immediate 
	effect (e.g. it's read from the serial port).	
	
	
### /var/log 
	http://superuser.com/questions/565927/differences-in-var-log-syslog-dmesg-messages-log-files

	Log files from the system and various programs/services, especially login (/var/log/wtmp, which logs all 
	logins and logouts into the system) and syslog (/var/log/messages, where all kernel and system program 
	message are usually stored). Files in /var/log can often grow indefinitely, and may require cleaning at 
	regular intervals. Something that is now normally managed via log rotation utilities such as 'logrotate'. 
	This utility also allows for the automatic rotation compression, removal and mailing of log files. 
	Logrotate can be set to handle a log file daily, weekly, monthly or when the log file gets to a certain 
	size. Normally, logrotate runs as a daily cron job. This is a good place to start troubleshooting general 
	technical problems.

/var/log/messages 	– Contains global system messages, including the messages that are logged during system startup. 
					  There are several things that are logged in /var/log/messages including mail, cron, daemon, kern, auth, etc.
/var/log/dmesg   	– Contains kernel ring buffer information. When the system boots up, it prints number of messages 
					  on the screen that displays information about the hardware devices that the kernel 
					  detects during boot process. These messages are available in kernel ring buffer and 
					  whenever the new message comes the old message gets overwritten. You can also view the 
					  content of this file using the dmesg command.
/var/log/auth.log 	– Contains system authorization information, including user logins and authentication machinsm that were used.
/var/log/secure

/var/log/boot.log 	– Contains information that are logged when the system boots
/var/log/daemon.log – Contains information logged by the various background daemons that runs on the system
/var/log/dpkg.log   – Contains information that are logged when a package is installed or removed using dpkg command
/var/log/kern.log   – Contains information logged by the kernel. Helpful for you to troubleshoot a custom-built kernel. */var/log/lastlog – Displays the recent login information for all the u sers. This is not an ascii file. You should use lastlog command to view the content of this file.
/var/log/mail.log   – Contains the log information from the mail server that is running on the system. For example, sendmail logs information about all the sent items to this file
/var/log/user.log   – Contains information about all user level logs
/var/log/Xorg.x.log – Log messages from the X
/var/log/alternatives.log – Information by the update-alternatives are logged into this log file. On Ubuntu, update-alternatives maintains symbolic links determining default commands.
/var/log/btmp 		– This file contains information about failed login attemps. Use the last command to view the btmp file. For example, “last -f /var/log/btmp | more”
/var/log/cups 		– All printer and printing related log messages
/var/log/anaconda.log – When you install Linux, all installation related messages are stored in this log file
/var/log/yum.log 	– Contains information that are logged when a package is installed using yum
/var/log/cron 		– Whenever cron daemon (or anacron) starts a cron job, it logs the information about the cron job in this file
/var/log/secure 	– Contains information related to authentication and authorization privileges. For example, sshd logs all the messages here, including unsuccessful login.
/var/log/wtmp   	– Contains login records. Using wtmp you can find out who is logged into the system. 
/var/log/utmp		  who command uses this file to display the information.
/var/log/faillog 	– Contains user failed login attemps. Use faillog command to display the content of this file. Apart from the above log files, /var/log directory may also contain the following sub-directories depending on the application that is running on your system.
/var/log/httpd/ 
/var/log/apache2 	– Contains the apache web server access_log and error_log
/var/log/lighttpd/ 	– Contains light HTTPD access_log and error_log
/var/log/conman/ 	– Log files for ConMan client. conman connects remote consoles that are managed by conmand daemon.
/var/log/mail/ 		– This subdirectory contains additional logs from your mail server. For example, sendmail stores the collected mail statistics in /var/log/mail/statistics file
/var/log/prelink/ 	– prelink program modifies shared libraries and linked binaries to speed up the startup process. /var/log/prelink/prelink.log contains the information about the .so file that was modified by the prelink.
/var/log/audit/ 	– Contains logs information stored by the Linux audit daemon (auditd).
/var/log/setroubleshoot/ – SELinux uses setroubleshootd (SE Trouble Shoot Daemon) to notify about issues in the security context of files, and logs those information in this log file.
/var/log/samba/ 	– Contains log information stored by samba, which is used to connect Windows to Linux.
/var/log/sa/ 		– Contains the daily sar files that are collected by the sysstat package.
/var/log/sssd/ 		– Use by system security services daemon that manage access to remoA servlet is simply a class which responds to a particular type of network request - most commonly an HTTP request. Basically servlets are usually used to implement web applications - but there are also various frameworks which operate on top of servlets (e.g. Struts) to give a higher-level abstraction than the "here's an HTTP request, write to this HTTP response" level which servlets provide.




### Servlets 
	A servlet is simply a class which responds to a particular type of network request - most commonly an HTTP request. 
	Basically servlets are usually used to implement web applications - but there are also various frameworks which 
	operate on top of servlets (e.g. Struts) to give a higher-level abstraction than the "here's an HTTP request, 
	write to this HTTP response" level which servlets provide.

	Servlets run in a servlet container which handles the networking side (e.g. parsing an HTTP request, 
	connection handling etc). One of the best-known open source servlet containers is Tomcat.	
	
	
	
### Binary releases - contain computer OS readable version of the application, meaning it is compiled. 
### Source releases - contain human readable version(code) of the application, meaning it has to be compiled 
					  before it can be used.	
	
	
### primitive types are the basic types of data
		byte, short, int, long, float, double, boolean, char
		primitive variables store primitive values
		
### reference types are any instantiable class as well as arrays
		String, Scanner, Random, Die, int[], String[], etc.
		reference variables store addresses	
	
	
### Directory . and .. means	
	. means the current directory.
	.. means the parent directory.
	e.g. when you type cd .. , you will move one level up. When you type cp /etc/resolv.conf  .  , 
		 you will copy file /etc/resolv.conf in the current directory.	
	
	