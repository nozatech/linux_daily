### LINUX ### 

removing empty space with ()
rm terraform_0.14.7_darwin_amd64\ '(1)'.zip

Bash Command List https://ss64.com/bash/

------------------------------------------------------------
# HELP 
------------------------------------------------------------
# apropos(~에 관하여) 			<= Search the manual page names and descriptions
  
  $ apropos egrep
		bzgrep (1)           - search possibly bzip2 compressed files for a regular expression
		egrep (1)            - print lines matching a pattern
		xzegrep (1)          - search compressed files for a regular expression

  $ apropos -r REGEXofUNIXCOMMAND or Description
								<= -r  Interpret  each  keyword as a regular expression. 

$ whatis(무엇에 관하여)			<= Searches a set of database files containing short descriptions
	$ whatis grep

$ help
	$ grep --help
	
$ info				<= Longer document
	$ info grep

$ man grep | less -p '-a'				<=  shows '-a, --text' option
	
$ man bash | less -p '-A action$'		<=  -p Pattern  '-A action$' <- $ end with 'n'
$ man bash | less -p '-c$'

$ man -k REGEX or UNIXCOMMAND

## Build-in Bash uses MAN		e.g.  $ man bash
## 3rd Utils uses     --help    e.g.  $ kubectl --help

-----------------------------------------------------------------------------------	
### Command and file Locations ###
# Find the command location
-----------------------------------------------------------------------------------	 
	$ whereis  cmd				<= <= Find binary /source / manual for commandD
		$ whereis ls
			ls: /usr/bin/ls /usr/share/man/man1/ls.1.gz
		$ whereis ssh_config
			ssh_config: /usr/share/man/man5/ssh_config.5.gz
		
	$ which    cmd				<= shows the full path of (shell) commands.
		$ which ls
         alias ls='ls --color=auto'
         /usr/bin/ls
		
	$ find / -name  cmd
		$ sudo find / -name ls	<= SUDO requires to scan all /(root) subdirectories
		
	$ locate  cmd				<= CentOS7 $yum install mlocate then $updatedb
		$ updatedb
		$ locate ls
		  scan all files with 'ls' 
	
		$ locate ssh_config		 				<= DB based Find file 
		  /etc/ssh/ssh_config
		  /etc/ssh/ssh_config.rpmnew
		  /usr/share/man/man5/ssh_config.5.gz

-----------------------------------------------------------------------------------	
# Bash Shell Setting	 
-----------------------------------------------------------------------------------	
### Font Type    <= Lucia Console, 12pt
### Shell's Color, History, Time stamp, Size ### 

# PS1 (Prompt Setting 1~ 4) <=  Green 
$ PS1="#: " ; export PS1
-----------------------------------------------------------------------------------	
echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\] '"  >> ~/.bashrc && source ~/.bashrc 

# echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\] '"  >> ~/.bashrc &&   .  ~/.bashrc 
-----------------------------------------------------------------------------------	

### Green & Red promt
============================================================================================
# 'w' full directory
echo 'export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$ "' >> ~/.bashrc	&& source ~/.bashrc	 

# 'W' Reduce directory
echo 'export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\W$ "' >> ~/.bashrc	&& source ~/.bashrc	
============================================================================================

-----------------------------------------------------------------------------------	
# Reload .bashrc within current shell without re-login
-----------------------------------------------------------------------------------	
	$ source ~/.bashrc	


# Bash Moves

$ is 	 /var/log/msg
$^is^cat /var/log/msg			<= change is to cat command

	# visual block
	



-----------------------------------------------------------------------------------	
# History Control Options for Bash
https://www.shellhacks.com/tune-command-line-history-bash/
-----------------------------------------------------------------------------------	
echo 'export HISTCONTROL="ignorespace:erasedups:ignoredups"' >> ~/.bashrc	<= Ignore Duplication
echo 'export HISTIGNORE="history*:ll*:ls*:"' >> ~/.bashrc					<= Ignore list
echo 'export HISTTIMEFORMAT="%F > "' >> ~/.bashrc							<= History time stamp(man date)
echo 'export HISTSIZE=10000' >> ~/.bashrc									<= Increate to 10k from Default set 1000


# History with time stamp
	export into ~/.bash_profile as well as /root/.bash_profile
	$ export HISTTIMEFORMAT="%F %T "
	
	# Modify .bashrc Path and reload
	http://docs.aws.amazon.com/cli/latest/userguide/awscli-install-linux.html#awscli-install-linux-path
	
	$ export PATH=~/.local/bin:$PATH		<= export  - Modify
	$ source ~/.bash_profile				<= source  - reLoad
	$ . ~/.bash_profile						<= source  - reLoad
	$ source .bashrc						<= reload .bashrc
-----------------------------------------------------------------------------------	
# Reload Profile
	Issue occurs after uninstalling kubectl but new binary file is in /usr/local/bin
	bash: /usr/bin/kubectl: No such file or directory
	
	Reload profile without log off
	$ .      ./bash_profile
	$ source ./bash_profile

-----------------------------------------------------------------------------------	
# Bash Log Out Message
# .bash_logout
-----------------------------------------------------------------------------------		
	$ vi ~/.bash_logout			<= user account
		echo "You are logging out! GoodBye~~"
	$ vi /etc/bash.bashrc

$ git help everyday
	No manual entry for giteveryday
https://codingbee.net/centos/man-how-to-fix-the-no-manual-entry-for



	
-----------------------------------------------------------------------------------
# Copy & Paste or create a 'shellenv.sh' script and PSSH to all servers 
	<< EOF here document method  << STDIN >> STDOUT 
-----------------------------------------------------------------------------------
cat << EOF >> ~/.bashrc
	export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$ "			# Color
	export HISTCONTROL="ignorespace:erasedups:ignoredups"		# Ignore space, dups, 
	export HISTIGNORE="history*:ll*:ls*:"						# Ignore ls ll
	export HISTTIMEFORMAT="%F > " 								# Date format
	export HISTSIZE=10000
	EOF

	# Reload .bashrc 
	$ source ~/.bashrc 
	
	# Copy to servers using PSSH, 
	$ pssh -O StrictHostKeyChecking=no -h serverList.txt -l apark -A -I < ./shellenv.sh
	
-----------------------------------------------------------------------------------
# Color Codes( 9 )	for Fonts
-----------------------------------------------------------------------------------
	Reset   = 0
	Black   = 30
	Red     = 31
	Green   = 32
	Yellow  = 33
	Blue    = 34
	Magenta = 35
	Cyan    = 36
	White   = 37
	
	$ echo -e "\e[1;31m This is red text. \e[0m"	<- \e[1;31m (\)escape string sets(-e ON) color
		This is red text.							<- \e[0m reset to default color
	
	$ echo -e '\e[1;31m	"Red text" \e[0m'
	
  # Color Codes	for Colored Background
	Coloring and Styling

  NCSA Escape code - Sequence
	$ bash -e			<= enable, echo with Escaping

	$ echo -e "\033[34;42mThis is green background\033[0m"
		This is green background

	# Colored test(ANSI)
	-----------------------------------------------
	Color		Foreground 	Background
	-----------------------------------------------
	Black			30			40
	Red				31			41
	Green			32			42
	Yellow			33			43
	Blue			34			44
	Magenta(분홍)	35			45
	Cyan(하늘색)	36			46
	White			37			47
-----------------------------------------------	
# .bashrc vs .bash_profile	
https://apple.stackexchange.com/questions/51036/what-is-the-difference-between-bash-profile-and-bashrc
https://superuser.com/questions/409186/environment-variables-in-bash-profile-or-bashrc

	.bash_profile 	<= is executed for login shells
	
	.bashrc 		<= is executed for interactive non-login or new shells.

-----------------------------------------------------------------------------------------
# BASH SHELL - Movement
https://www.cheatography.com/citguy/cheat-sheets/bash-shortcuts/
https://blog.m157q.tw/posts/2014/10/16/bash-cheat-sheet/
https://ss64.com/bash/syntax-keyboard.html

# Move
	Ctrl + a	<= go to the start of the command line
	Ctrl + e	<= go to the end   of the command line
	
	Ctrl + <-	<= < left  Arrow go to Left Front of word	
	Ctrl + ->	<= > right Arrow go to Right  End   of word	
	

# Clean / Stop 
	Ctrl + l	<= clear screen
	Ctrl + s	<= Stop verbose output

# Control
	Ctrl + c	<= terminate command
	Ctrl + z	<= suspend comman­d/send to background (use fg to bring forward)

# Cut / Paste / Delete 
	Ctrl + u	<= from cursor, delete to start of line     <= Delete delete...|Ok
	Ctrl + k	<= from cursor, delete to end of line		Ok|Delete delete ... =>
	
	Ctrl + y	<= Paste
	
	Ctrl + w	<= Delete word 
	Alt  + d	<= delete word
	
	

# !(bang) Commands
	!!			<= Run last command
	!number		<= History then run from history command number
	!$ or Alt+.	<= the last word of the previous command
	
-----------------------------------------------------------------------------------------	
# vim file explorer


	# netrw
		Netrw is file explorer plugin for vim and comes installed on Vim 7.0 and 
		higher (netrw v155)
		https://shapeshed.com/vim-netrw/
		https://www.youtube.com/watch?v=3lqzc77carU
		https://shapeshed.com/vim-netrw/
	  Network oriented reading, writing, and browsing (keywords: netrw ftp scp) 
	
	$ vi .ssh
	# quit  
	  :q
		
		
#########################################################################################
#   Boot Process   
#########################################################################################

Firmware(POST or UEFI) => Boot loader => Kernel => Initialization Stages

1. POST(UEFI | BIOS) => Exec Boot Loader

2. Boot Loader(GRUB2) => load Kernel
	BIOS: /boot/grub2/grub.cfg
	UEFI: /boot/efi/EFI/rehat/grub.cfg

3. Kernel
	Loads the RAMDISK(temp root file system) to RAM
	Loads device drivers and config files from RAMDISK
	Unmounts RAMDISK and mounts root filesystem
	Starts the initialization stage

4. Initialization Stages
	Kernel starts the 1st process ( init(oldest) => upstart => systemd(latest) )
	Systemd starts system services
	Systemd starts Login shells and GUI interface

5. OS is ready to be used
-----------------------------------------------------------------------------------------
	
# CentOS 7 uses the Systemd Targets
	Target is a specific configuration(run level)
	Default target is graphical.target
	Systems can be booted into different targets. 
		e.g. rescue mode
		
-----------------------------------------------------------------------------------------
# GRUB2 - CentOS 7  
-----------------------------------------------------------------------------------------
$ cat /boot/grub2/grub.cfg				<= BIOS system
$ cat /boot/efi/EFI/centos/grub.cfg		<= UEFI system

$ file   /etc/grub2.cfg					<= use 'file' command to find out the link
/etc/grub2.cfg: symbolic link to `../boot/grub2/grub.cfg'

$ sudo less -N /boot/grub2/grub.cfg

   # It is automatically generated by grub2-mkconfig using templates
   # from /etc/grub.d and settings from /etc/default/grub

### BEGIN /etc/grub.d/00_header ###  	<= grub menu header
### BEGIN /etc/grub.d/10_linux ###		<= Actual Kernel Choice

linux16 /boot/vmlinuz-3.10.0-693.17.1.el7.x86_64 root=UUID=b3...aa 
ro console=tty0 console=ttyS0,115200n8 crashkernel=auto console=ttyS0,115200 LANG=en_US.UTF-8

#Init ramdisk
initrd16 /boot/initramfs-3.10.0-693.17.1.el7.x86_64.img

### BEGIN /etc/grub.d/40_custom ###
# This file provides an easy way to add custom menu entries.  Simply type the
# menu entries you want to add after this comment.  Be careful not to change
# the 'exec tail' line above.
### END /etc/grub.d/40_custom ###


$ cat /etc/default/grub
GRUB_TIMEOUT=1
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=saved					<= Default Kernel is saved in another file.
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL="serial console"
GRUB_SERIAL_COMMAND="serial --speed=115200"
GRUB_CMDLINE_LINUX="console=tty0 crashkernel=auto console=ttyS0,115200"   <=KERNEL OPTIONS
GRUB_DISABLE_RECOVERY="true"

$ cat /boot/grub2/grubenv
# GRUB Environment Block
saved_entry=CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core)
###############################################################  <= CAN'T READ AWK to print out

### Check boot up list menu
$ awk -F\' '/menuentry/{print $2}' /boot/grub2/grub.cfg

CentOS Linux 7 Rescue 08...04 (3.10.0-693.17.1.el7.x86_64)		<= 0
CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core)				<= 1
CentOS Linux (3.10.0-693.11.6.el7.x86_64) 7 (Core)				<= 2
CentOS Linux (0-rescue-60...9c)           7 (Core)              <= 3

# Change menu
$ grub2-set-default 1

# Check back in 
$ /etc/default/grub

GRUB_CMDLINE_LINUX="console=tty0 crashkernel=auto console=ttyS0,115200" <= Kernel option
	# change with " " 

# Reconfigure GRUB
$ grub2-mkconfig -o /boot/grub2/grub.cfg			<= BIOS
$ grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg   <= UEFI


#### Custom GRUB menu

$ sudo cat /etc/grub.d/40_custom  /boot/grub2/grub.cfg > ~/40_custom
$ vi ~/40_custom     <= edit
$ sudo cp ~/40_custom   /etc/grub.d/40_custom




 
 
 
 



# Discovering Which Bootloader the Droplet Uses
https://www.digitalocean.com/community/tutorials/how-to-update-a-digitalocean-server-s-kernel
### Grub 2 ###
Debian 7, 8
Ubuntu 12.04, 14.04, 16.04
CentOS 7
Fedora 23
### Grub 1 ###
CentOS 5, 6





-----------------------------------------------------------------------------------------	
########################
### System services  ### 
########################

	SysV init process tree( from ATT Linux 80's)

	init --> abrtd
		  |-> agetty
		  |--> auditd --{gdbud}
		  
	Runlevel 3	  
		cups, httpd, iptables  
	
	Cons
		slow startup
		No service dependencies
		No persistent network
	to => Launchd & Upstart but ==> SystemD
	
	
-----------------------------------------------------------------------------------------	
### SystemD ###
-----------------------------------------------------------------------------------------
	https://www.digitalocean.com/community/tutorials/how-to-use-systemctl-to-manage-systemd-services-and-units
	https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files	
	https://askubuntu.com/questions/795226/how-to-list-all-enabled-services-from-systemctl
	
	# Units(service name) <= target of most of actions which are resources, defined by unit files
 	
	$ systemctl						
	
	$ systemctl list-units			<= Listing only active units
		UNIT: The systemd unit name
		LOAD: Whether the unit's configuration has been parsed by systemd. 
				The configuration of loaded units is kept in memory.
		ACTIVE: A summary state about whether the unit is active. 
				This is usually a fairly basic way to tell if the unit has started 
				successfully or not.
		SUB: This is a lower-level state that indicates more detailed information 
			about the unit. This often varies by unit type, state, and the actual 
			method in which the unit runs.
		DESCRIPTION: A short textual description of what the unit is/does.
	
	# Check Enabled list
	$ systemctl list-unit-files | grep enabled
		jenkins_proxy.service     enabled
	
	$ systemctl list-units	--all
	
	**$ systemctl list-unit-files  <= displaces all 
	
	$ systemctl list-unit-files -at service
		UNIT FILE                             STATE
		arp-ethers.service                    disabled
		atd.service                           enabled

	$ systemctl list-units -at service
		UNIT                                LOAD      ACTIVE   SUB     DESCRIPTION
		atd.service                         loaded    active   running Job spooling tools
		auditd.service                      loaded    active   running Security Auditing Service
	
	$ systemctl list-units -t service --state running
		UNIT                     LOAD   ACTIVE SUB     DESCRIPTION
		atd.service              loaded active running Job spooling tools
		auditd.service           loaded active running Security Auditing Service
		nginx.service            loaded active running The nginx HTTP and reverse proxy server
		ntpd.service             loaded active running Network Time Service
	
	$ systemctl enable | disable | status iptables
		# /etc/systemd/system/basic.target.wants/iptables.service
	
	$ systemctl cat sshd				<= 'cat' concatenate the Unit file for 'sshd.service'
		# /usr/lib/systemd/system/sshd.service
		[Unit]
		Description=OpenSSH server daemon
			$ systemctl is-enabled nginginx 
		disabled
	
	$ sudo systemctl enable nginx
		Created symlink from '/etc/systemd/system/multi-user.target.wants/nginx.service' to 
			'/usr/lib/systemd/system/nginx.service.'
	
	$ cat /etc/systemd/system/mucat lti-user.target.wants/nginx.service
		[Unit]
		Description=The nginx HTTP and reverse proxy server
		After=network.target remote-fs.target nss-lookup.target

		[Service]
		Type=forking
		PIDFile=/run/nginx.pid
	
	$ systemctl is-enabled sshd	
	$ systemctl is-active  sshd 
	$ systemctl is-failed  sshd 
	$ systemctl reload-or-restart sshd.service
	
	$ systemctl list-dependencies sshd.service
	
	$ ls -l /etc/systemd/system/multi-user.target.wants
		lrwxrwxrwx. 1 root root 36 Jan 25 10:14 sshd.service -> /usr/lib/systemd/system/sshd.service

	$ systemctl list-units --all --state=active
	$ systemctl list-units --all --state=inactive
	$ systemctl list-units --type=service
	
	# low level
	$ systemctl show sshd.service
	
	# Masking and unmasking units( making disable from start up by linking to /dev/null)
	$ systemctl mask | unmask sshd.service  		<= start will fail due to masked

	# Edit Unit file
	$ systemctl edit sshd.service
			
	$ sudo systemctl edit nginx
	Editing "/etc/systemd/system/nginx.service.d/override.conf" <= precedence over nginx.conf
	
	$ sudo systemctl edit --full nginx
	# to Remove
	$ sudo rm -r /etc/systemd/system/nginx.service.d	<= snippet
	$ sudo rm /etc/systemd/system/nginx.service			<= full

  
  # systemctl Manager Lifecycle Commands <= from man page
   
       systemctl daemon-reload
           Reload systemd manager configuration. This will rerun all generators (see systemd.generator(7)), 
		   reload all unit files, and recreate the entire dependency tree. While the daemon is being 
		   reloaded, all sockets systemd listens on behalf of user configuration will stay accessible.

           This command should not be confused with the reload command.

       systemctl daemon-reexec
           Reexecute the systemd manager. This will serialize the manager state, reexecute the process and
           deserialize the state again. This command is of little use except for debugging and package upgrades.
           Sometimes, it might be helpful as a heavy-weight daemon-reload. While the daemon is being reexecuted,
           all sockets systemd listening on behalf of user configuration will stay accessible.


	# Restore default
**  $ sudo systemctl daemon-reload						<= Restore to system default
	
	### Retart all daemon
	$ systemctl daemon-reexec
	
	
	
	
	
	
	### Journald ###
	$ journalctl			<= log information from applications and the kernel
		$ vi /etc/systemd/journald.conf
	
	$ journalctl -b					<= current boot
	$ journalctl -k	  <=> dmesg     <= Kernal
	$ journalctl -xe				<= -x  Augment log lines with explanation texts from the message 
										   catalog.
									<= -e  jump to the end of the journal 
	
	
# SystemD create startup Script
https://stackoverflow.com/questions/39284563

# SystemD Service and Log files

$ sudo vi /etc/systemd/system/jenkins_proxy.service
#or# 
$ systemctl edit --full jenkins_proxy.service
--------------------------------------------------------------
[Unit]
Description=Jenkins Proxy Server

[Service]
ExecStart=/usr/bin/node index.js > /var/log/jenkins-proxy.log			
Restart=always

#RestartSec=90
#StartLimitInterval=400
#StartLimitBurst=3

User=node
Group=node
Environment=PATH=/usr/bin:/usr/local/bin
Environment=NODE_ENV=production
WorkingDirectory=/var/app/jenkins-proxy				

[Install]
WantedBy=network.target
--------------------------------------------------------------

# Creating App Directory and Change Permission 
$ mkdir /var/app
$ sudo chmod -R a+w /var/app/			<= a <-all, w <-write, R <- Recursive


# Creating Index_test.js
$ vim /var/app/index_test.js
-------------------------------------------------------------------------------
// Load the http module to create an http server.
var http = require('http');

// Configure our HTTP server to respond with Hello World to all requests.
var server = http.createServer(function (request, response) {
  response.writeHead(200, {"Content-Type": "text/plain"});
  response.end("Hello World\n");
});

// Listen on port 8000, IP defaults to 127.0.0.1
var port = 3000;
server.listen(port);

// Put a friendly message on the terminal
console.log("Server running on port", port);
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
Package Management
-------------------------------------------------------------------------------
Ubuntu Package Manager
-------------------------------------------------------------------------------
apt   <= provides a high-level commandline interface for the package
	     management system. It is intended as an end user interface and enables
         some options better suited for interactive usage by default compared to
         more specialized APT tools like apt-get(8) and apt-cache(8).
	$ apt-get
			  update (apt-get(8))
				   update is used to download package information from all configured
				   sources. Other commands operate on this data to e.g. perform
				   package upgrades or search in and display details about all
				   packages available for installation.
			  upgrade (apt-get(8))
				   upgrade is used to install available upgrades of all packages
				   currently installed on the system from the sources configured via
				   sources.list(5). New packages will be installed if required to
				   satisfy dependencies, but existing packages will never be removed.
				   If an upgrade for a package requires the remove of an installed
				   package the upgrade for this package isn't performed.	   
			  install
			  remove
			  purge
			  autoremove (apt-get(8))
				   autoremove is used to remove packages that were automatically
				   installed to satisfy dependencies for other packages and are now no
				   longer needed as dependencies changed or the package(s) needing
				   them were removed in the meantime.

	$ apt-cache
			    search (apt-cache(8))
				   search can be used to search for the given regex(7) term(s) in the
				   list of available packages and display matches. This can e.g. be
				   useful if you are looking for packages having a specific feature.
				   If you are looking for a package including a specific file try apt-
				   file(1).

	

-------------------------------------------------------------------------------
CentOS Package Manager
-------------------------------------------------------------------------------
  yum  is an interactive, rpm based, package manager. It can automatically perform 
	   system updates, including dependency analysis  and  obsolete  processing
       based on "repository" metadata. It can also perform installation of new packages, 
	   removal of old packages and perform queries  on  the  installed  and/or
       available  packages  among  many  other commands/services (see below). yum is
       similar to other high level package managers like apt-get and smart.

	    * install package1 [package2] [...]
        * update [package1] [package2] [...]
        * update-to [package1] [package2] [...]
        * update-minimal [package1] [package2] [...]
        * check-update
        * upgrade [package1] [package2] [...]
        * upgrade-to [package1] [package2] [...]
        * distribution-synchronization [package1] [package2] [...]
        * remove | erase package1 [package2] [...]
        * autoremove [package1] [...]
        * list [...]
        * info [...]
        * provides | whatprovides feature1 [feature2] [...]
        * clean [ packages | metadata | expire-cache | rpmdb | plugins | all ]
        * makecache [fast]
        * groups [...]
        * search string1 [string2] [...]
        * shell [filename]
        * resolvedep dep1 [dep2] [...]
           (maintained for legacy reasons only - use repoquery or yum provides)
        * localinstall rpmfile1 [rpmfile2] [...]
           (maintained for legacy reasons only - use install)
        * localupdate rpmfile1 [rpmfile2] [...]
           (maintained for legacy reasons only - use update)
        * reinstall package1 [package2] [...]
        * downgrade package1 [package2] [...]
        * deplist package1 [package2] [...]
        * repolist [all|enabled|disabled]
        * repoinfo [all|enabled|disabled]
        * repository-packages   <enabled-repoid>   <install|remove|remove-or-rein‐
			stall|remove-or-distribution-synchronization> [package2] [...]
        * version [ all | installed | available | group-* | nogroups* | grouplist|groupinfo ]
        * history [info|list|packages-list|packages-info|summary|addoninfo|redo|undo|rollback|new|sync|stats]
        * load-transaction [txfile]
        *  updateinfo  [summary  | list | info | remove-pkgs-ts | exclude-updates |
			exclude-all | check-running-kernel]
        * fssnapshot [summary | list | have-space | create | delete]
        * fs [filters | refilter | refilter-cleanup | du]
        * check
        * help [command]

-----------------------------------------------------------------------------------------	
Python3 brakes Yum
https://stackoverflow.com/questions/11213520/yum-crashed-with-keyboard-interrupt-error/11220305
-----------------------------------------------------------------------------------------
	# Problem occur
	$ yum
	  File "/usr/bin/yum", line 30
	   except KeyboardInterrupt, e:
	   
	   
	# checky python version
	This issue happens when user upgrades to python3, 
	Just simply edit the file --> /usr/bin/yum and change to first line to --> "#!/usr/bin/python2"
	
	The above solution wouldn't solve the all yum dependency problems, its better to run the below commands.
	sudo ln -s  /usr/local/bin/python3 /usr/bin/python3 (Mark latest python as python3)
	sudo ln -sf /usr/bin/python2.7     /usr/bin/python  (make 2.7 as default python)
	
-----------------------------------------------------------------------------------------	
# PROCESS
-----------------------------------------------------------------------------------------
	$ ps 				<= Unix, BSD, GNU types
		1 UNIX 		<= ps -ef (Unix/POSIX)    which may be grouped and must be preceded by a dash.
		2 BSD 		<= ps aux (Ubuntu/Debian) which may be grouped and must not be used with a dash.
		3 GNU 		<= ps -ef (CentOS)		  Standard, long options, which are preceded by two dashes.
	$ ps -e			<= everything
	$ ps -eH		<= Hierachical 
	$ ps ax | ps -ef 		<= BSD|Unix for every process (GNU e.g. CentOS)
	$ ps aux		<= every process + -u user (BSD e.g. Ubuntu)
	$ ps -elF		<= Long, 15 columes of options	
	
	$ ps -e --format uid,pid,ppid,%cpu,cmd  <= use '--format' for custom option
	$ ps -G  group_name
	$ ps -C nginx --format pid,ppid,%cpu,cmd --sort %cpu
		PID  PPID %CPU CMD
		23976 23968  0.0 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf
		23984 23976  0.0 nginx: worker process

	$ ps -e --format uid,pid,ppid,tty,%cpu,cmd --sort %cpu
	$ ps -e --format uid,pid,tty,rss,cmd --sort rss   <= memory usage, cmd sorted
	$ ps -U root				<= -u & -U same(user)
	$ ps -U apark --format %mem | awk '{memory +=$1};END {print memory}'  	 <= user memory usage
	
	### Process tree format
	$ ps axjf  		<= BSD PS tree format
	$ ps -ejH		<= CentOS 
	
	$ ps auxf
-----------------------------------------------------------------------------------------
# pstree
-----------------------------------------------------------------------------------------
	https://www.cyberciti.biz/faq/unix-linux-pstree-command-examples-shows-running-processestree/
	$ yum -y install psmisc
	$ pstree
	$ pstree pid
	$ pstree user
	
	$ pstree -p					<= show PIDS for each process name
		systemd(1) ─┬─agetty(1166)
					├─agetty(1167)
					├─auditd(435)───{auditd}(436)

	$ pstree -n				<= To sort processes with the same ancestor by PID instead of by name 
	$ pstree -np				i.e. numeric sort, pass the -n options as follows:
		systemd(1) ─┬─systemd-journal(357)
					├─auditd(435)───{auditd}(436)
					├─systemd-logind(552)
					├─dbus-daemon(553)───{dbus-daemon}(554)

	
-----------------------------------------------------------------------------------------	
# Process Niceness/Priority
-----------------------------------------------------------------------------------------
	$ nice -2 top			<= if only 1 '-' +2,  '--' -2
	$ ps -C top --format pid,nice,cmd,%cpu		<= -C  command name
		PID  	NI CMD
		21112   2  top
	$ sudo nice --2 top		<= If no sudo, it will set to nice value '0'<- max permission
	$ sudo renice -5 top	<= sudo requires

-----------------------------------------------------------------------------------------	
# Process Jobs manage
-----------------------------------------------------------------------------------------
	$ watch -n 5 'ps -C dd --format pid,cmd,%cpu'		<= Term1    
	
	$ dd if=/dev/zero of=/dev/null						<= Term2    
	
	$ Ctrl+z											<= Term2 "Stopping process"    
		$ jobs
			[1]+  Stopped                 dd if=/dev/zero of=/dev/null
		$ bg 1
			[1]+ dd if=/dev/zero of=/dev/null &
		$ jobs
			[1]+  Running                 dd if=/dev/zero of=/dev/null &
		$ fg
			dd if=/dev/zero of=/dev/null
		$ Ctrl+z
		$ bg
		$ Ctrl+c
		$ dd if=/dev/zero of=/dev/null &
			[1] 24933
		$ killall dd

-----------------------------------------------------------------------------------------
# pidof
-----------------------------------------------------------------------------------------
	pidof -- find the process ID of a running program.
	$ pidof systemd
		1

	
	
-----------------------------------------------------------------------------------------	
# Kill Signal
-----------------------------------------------------------------------------------------
	$ kill -l 			<=list of singals
	kill, killall, pkill
	http://www.thegeekstuff.com/2009/12/4-ways-to-kill-a-process-kill-killall-pkill-xkill/
	https://unix.stackexchange.com/questions/317492/list-of-kill-signals
	
	$ pidof    sleep
	$ pgrep    sleep
	$ kill     15558 15559 15560		<= same as kill -15(greacefull)
	$ kill -9  15558 15559 15560		
	$ pkill   sleep
	$ killall sleep					
	$ killall -u user_name				<= Kill all related to user
	
-----------------------------------------------------------------------------------------	
# Monitoring 
# TOP
-----------------------------------------------------------------------------------------
	$ top
			l 	<= load avg & Uptime
			1 	<= All CPU usage
			t 	<= toggle CPU states
			m	<= toggle betwen memory states
			f	<= FIELD management
				select and spacebar to enable
			c	<= toggle between cmd name & cmdline
			u	<= user name
			k	<= kill task
			r	<= renice,  choose PID, and value
			
		shift + m	<= Memory usage
		shift + p	<= Process usage
		shift + t	<= Time usage
		shift + n	<=(PID Number)
		
	### Monitoring Scripts		
	https://bash.cyberciti.biz/shell/monitoring/

	
-----------------------------------------------------------------------------------------	
# wait: wait [id]
-----------------------------------------------------------------------------------------
    Wait for job completion and return exit status.
	https://unix.stackexchange.com/questions/42287/terminating-an-infinite-loop/121391#121391
	$ wait $!

-------------------------------------------------	
#!/bin/bash
# Thhis script uses trap to catch ctrl-c (or SIGTERM), kills off the command 
# use sleep  as a test and exits.

cleanup (){
kill -s SIGTERM $!
exit 0
}

trap cleanup SIGINT SIGTERM

while [ 1 ]
do
    sleep 60 &
    wait $!
done
-------------------------------------------------





--------------------
### BASH Startup ###
--------------------
# Login Shell Process when everytime login to shell
----------------------------------------------------------------------------------------------------
# Login Shell Process
	### Steps of Loading profiles ###
	1. /etc/profile 			<= systemwide env and shell vars(Sys Admin)
	2. /etc/profile.d/*.sh  	<= systemwide env and shell vars
	
	3. ~/.bash_profile			<= User env and shell vars
	4. ~/.bashrc				<= Execute /etc/bashrc (within shell)
	
	5. /etc/bashrc				<= Systemwide alias and shell functions
	6. ~/.bashrc				<= Execute /etc/bashrc
	### User Successfully LOGIN to the SHELL ###

# Non-Login Shell Login Process
	1. ~/.bashrc				<= Execute /etc/bashrc
	2. /etc/bashrc				<= Systemwide alias and shell functions
	3. ~/.bashrc				<= Execute /etc/bashrc

# Variables for ALL USERS add to 
	/etc/profile 				<= systemwide env and shell vars
	/etc/profile.d/*.sh  		<= systemwide env and shell vars

# Variables for individual USERS 
	~/.bash_profile				<= User env and shell vars






----------------------------------------------------------------------------------------------------
$ /home/apark/.bash_profile  <= settings, environment profile
$ /home/apark/.bashrc		 <= current & new vars  e.g. alias

	# .bash_profile <= is read when bash is invoked as login shell.
	# .bashrc       <= is executed when a new shell is started.

PATH=$PATH:/usr/local/bin

Keyboard => Terminal => SHELL	=> |Kernel|
screen 	 <= Terminal => SHELL	<= |Kernel|

1. Shell checks if it's a built-in command.
2. Shell checks if it's an alias of a command.
3. Shell checks if the command is on the hard disk.
sh(Bourne Shell)
	- basic shell
	- POSIX-compliant shell
	- Expose array indices
	- RegEx 
	- Increment assignment operator
	
Other shells
zsh, ksh, csh

-----------------------------------------------------------------------------------
https://mywiki.wooledge.org/BashGuide/CommandsAndArguments

# Alias  
	a word that is mapped to a string. Whenever that word is used as a command, 
	it is replaced by the string it has mapped.

# Function 
	a name that is mapped to a set of commands. Whenever the function is used as a 
	command, it is called with the arguments following it. Functions are the basic 
	method of making new commands.

# Builtin
	certain commands have been built into Bash. These are handled directly by the 
	Bash executable and do not create a new process.

# Executable 
	a program that can be executed by referring to its file path (e.g. /bin/ls), 
	or simply by its name if its location is in the PATH variable.


-----------------------------------------------------------------------------------
### Vi environment set-up (setting)
# vimrc
-----------------------------------------------------------------------------------
  # Open two files Right | Left
	$ vi -o  file1 file2
	
  # Open tow files Up | Down
	$ vi -O  file1 file2

  # Save all files
    :wqall   <-Write and Quit All files


# Cywin
Copy vimrc_expample.vim file from Ubuntu14 and create a ~/.vimrc file in Windows.
Ubuntu 14's vimrc_expample.vim file location is /usr/share/vim/vim74/vimrc_example.vim


http://vimdoc.sourceforge.net/htmldoc/syntax.html#%3ahighlight

Create a .vimrc file in home directory


# vimrc setup (vim editor environment)	
$ vi ~/.vimrc
----------------------------------------------------
:set nu
:set tabstop=4
syntax on
colorscheme desert
----------------------------------------------------
	# To check
	$ vi test				
		# from cmd mode
		:echo $MYVIMRC			<=Vim variable $MYVIMRC
			/home/userId/.vimrc	<= output should be this


	----------------------------------------------------
	####	Vim color scheme enable   ###
	----------------------------------------------------
	http://www.server-world.info/en/note?os=CentOS_7&p=initial_conf&f=7

	$ yum -y install vim-enhanced 

	2. E dit profile(global profile configuration for all users)
	$ vi /etc/profile
	# add at the last line
	$ alias vi='vim'

	3. reload without relogin
	$ source /etc/profile 

	4.Configure vim. 
		( Apply to a user below. If you apply to all users, Write the same settings in 
		'/etc/vimrc', some settings are applied by default though. )

	# Add following 
		syntax on
		colorscheme desert
	---------------------------------------------------------------------------------------------------
	esc highlight

----------------------------------------------------
# VI - number line and tab stop
----------------------------------------------------
	$ vi -V 			<= version check
	:set nu 			<= set number On
	:set nu!   			<= set number Off
	:e file_name        <= open & create a new file
 	:w new_name 	    <= save as to new_name
	
	:set tabstop=4		<= Tab will stop at 4 spaces
	
	# Cut and Paste(v/V-d/y-p/P)

		 a. Position the cursor where you want to begin cutting.
	V    b. Press v to select characters (or V - whole lines).
		 c. Move the cursor to the end of what you want to cut.
	d    d. Press d to cut (or y to copy).
		 e. Move to where you would like to paste.
	p    f. Press P to paste before the cursor, or p to paste after.

	yy 	<=	shift+v to copy 
	pp	<=	shift+p to paste


	# Undo 
	  Esc + u
	
	# Redo
	  Ctrl + r
	
	# Change String like "SED" stream editor
	http://vim.wikia.com/wiki/Search_and_replace
	
	# within VI, change 'foo' to 'bar'
	:%s/foo/bar/gc				<= c <- confirm first
		
	# Substitude(Replace) from domain1 to domain2	
    :%s/domain1.com/domain2.com/g
	FROM: ssl_certificate         /etc/nginx/ssl/domain1.com/server.crt;
	TO:   ssl_certificate         /etc/nginx/ssl/domain2.com/server.crt;
	
	
	
	
	
	
	# Search with in VI
	/search_word
	 n 					<- next word

----------------------------------------------------------------------------------------------------	
# Environment variable check
# env
# set
# unset
	
	https://www.digitalocean.com/community/tutorials/how-to-read-and-set-environmental-and-shell-variables-on-a-linux-vps
----------------------------------------------------
	### to check the environment variables
	$ printenv     			
	$ env					
	### env == printenv ### they are same
	$ set	
	
	
	$ envsubst 		<=  substitutes environment variables in shell format strings
	
	
----------------------------------------------------	
# Check $SHELL variable	
----------------------------------------------------
	$ printenv SHELL
	$ printenv | grep SHELL
	$ env | grep SHELL
	$ echo $SHELL		  	
	$ set | grep SELL					<= check current SHELL's variables
	=> they all same '/bin/bash <= 

	
	$ env <= modified the environment variables
		-  Set each NAME to VALUE in the environment and run COMMAND.
		
----------------------------------------------------	
# set and unset
----------------------------------------------------
http://linuxcommand.org/lc3_man_pages/seth.html
	<= Set or unset values of shell options and positional parameters(NOT Environment).	
	Set is a SHELL command to set the value of a shell attribute variable; 
	these are internal variables used by the SHELL, Not Environment.
	- 'Set' or 'unset' values of shell options and positional parameters.
	- Change the value of shell attributes and positional parameters, or
		display the names and values of shell variables.
	
	# set variable $i
		$ i=variable
		$ set i
		$ echo $i
		  variable
		$ set | grep i
		  i=variable


	# Unset
		$ unset i
		$ echo $i
		
		
	$ foo=(4 5 6)
	$ foo[2]=
	$ echo ${#foo[*]}
		3
		
	$ unset foo[2]
	$ echo ${#foo[*]}	
		2	

	/home/user1> var=""
	/home/user1> echo $var

	/home/user1> set -u
	/home/user1> echo $var

	/home/user1> unset var
	/home/user1> echo $var
	>>> -bash: var: unbound variable <<<

	
----------------------------------------------------------------------------------------------		
# set can see Shell-LOCAL variables, env cannot.
----------------------------------------------------------------------------------------------
	https://www.cyberciti.biz/faq/linux-list-all-environment-variables-env-command/
	
	Shells can have variables of 2 types: 
	
	locals, <= which are only accessible from the current shell,  and (exported)
	environment variables, <= which are passed on to every executed program.

	Since set is a built-in shell command, it also sees sees shell-local variables (including 
	shell functions). env on the other hand is an independent executable; it only sees the 
	variables that the shell passes to it, or environment variables.

	When you type a line like "a=1" then a local variable is created (unless it already 
	existed in the environment). Environment variables are created with "export a=1"
	
	env
	set
	printenv
	

----------------------------------------------------------------------------------------------
# printenv <= print all or part of environment
----------------------------------------------------------------------------------------------
	$ set -o 				<= -o Options,  Inherite from BASH or other Shells
	$ set -o posix			<= Turn on POSIX, see shell variables, remove all shell functions
	$ set +o posix			<= +o removes POSIX mode

----------------------------------------------------------------------------------------------
# Environment VS SHELL Variables 	
----------------------------------------------------------------------------------------------
	1. Environment Variables
		Variables that are defined for the current shell and are inherited by any child shells
		or processes
		
	2. SHELL Variables 
		Variables that are contained exclusively within the shell in which they were defined.

----------------------------------------------------------------------------------------------
# export 	<= (default same as '-p')Set export attribute for Shell Variables.
----------------------------------------------------------------------------------------------
			Marks each NAME for automatic export to the environment of subsequently
			executed commands.  If VALUE is supplied, assign VALUE before exporting.
			Options:
			-f   shell functions
			-n   remove the export property from each NAME
			-p   display a list of all exported variables and functions

	$ VAR=TEST				<= creating variable 'VAR'
	$ set | grep VAR		
	  VAR=TEST
	
	$ unset VAR				<= Removing 'VAR'
	
	$ env | grep VAR
		** No Result
	
	$ printenv | grep VAR
		** No Result 			<= VAR is not part of environment
----------------------------------------------------	
# Env variables, use 'export'
----------------------------------------------------
	$ export VAR			
	$ env | grep VAR		
	  VAR=TEST
	$ export -n VAR			<= 	


	
	
	
	
	
----------------------------------------------------	
# socat - Multipurpose relay (Socket CAT)
----------------------------------------------------
https://medium.com/@copyconstruct/socat-29453e9fc8a6
	utility for data transfer between two addresses.





----------------------------------------------------
# Bash options
----------------------------------------------------
# set vs. shopt 

	$ set -o					<=inherited from other Bourne-style shells (mostly ksh)
	(https://www.tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_06.html)
	
	
#	$ shopt( Bash 'sh'ell 'opt'ions)	<= specific options for BASH Shell
										   list of Bash  Shell Option
	$ shopt -s extglob					<= turn set  (on)  extended glob
	$ shopt -u extglob					<= turn unset(off) extended glob
	$ shopt | grep extglob
	
	
	
	$ env
	
	### Environment Variable
	# Value pair with = to SORTING
	# var=value
	The environment variables of a process exist at runtime, and are not stored in some file or so. 
	They are stored in the process's own memory (that's where they are found to pass on to children). 
	But there is a virtual file in
	/proc/pid/environ

	This file shows all the environment variables that were passed when calling the process 
	(unless the process overwrote that part of its memory — most programs don't). The kernel makes 
	them visible through that virtual file. One can list them. For example to view the variables 
	of process 3940, one can do

	$ cat /proc/3940/environ | tr '\0' '\n'   <= 'tr' replaces the 'zero' into a newline.
		Each variable is delimited by a 'binary zero' from the next one. 'tr' replaces the zero into a newline.
	
	# Example
	$ pgrep bash
		3179
	$ cat /proc/3179/environ
	USER=aparkLOGNAME=aparkHOME=/home/aparkPATH=/usr/local/bin:/usr/binMAIL=/var/mai
	l/aparkSHELL=/bin/bashSSH_CLIENT=10.100.5.190 23898 22SSH_CONNECTION=10.100.5.19
	0 23898 10.100.5.155 22SSH_TTY=/dev/pts/0TERM=xtermSELINUX_ROLE_REQUESTED=SELINU
	X_LEVEL_REQUESTED=SELINUX_USE_CURRENT_RANGE=XDG_SESSION_ID=14840XDG_RUNTIME_DIR=	
	
	$ cat /proc/3179/environ | tr '\0' '\n'  <= 'tr' translate(replaces) the binary zero into a newline.
											 <=	'\n' <- new line
											 <= '\0' <- NULL value, no space???
		USER=apark
		LOGNAME=apark
		HOME=/home/apark
		PATH=/usr/local/bin:/usr/bin
		MAIL=/var/mail/apark
		SHELL=/bin/bash
		......


	### Environment var search the PATH
	$ echo $PATH
	$ env | grep PATH
	$ set | grep PATH
	
	$ PATH="$PATH:/home/apark/bin
	$ export PATH
	$ echo $PATH


--------------------------------------------------------------------------
### Invocation(호출, 발동) Modes
https://mywiki.wooledge.org/bash/invocation/mode
--------------------------------------------------------------------------	
MODE				BASH								POSIX Shell
Login Shell			-bash[options]						-sh
					bash -l[options]					sh -l
					bash --login[options]				sh --login
	
Command String		bash -c 'command'[Options]			sh [options] -c 'command
	
Interactive Shell	bash -i								sh -i	
Shell Script		#!/bin/bash [option]				#!/bin/sh [option]
Command File		bash [options] file [args]			sh [options] file [args]
Command Stream		bash [options] -s [SHELL [ARGS]]	sh [options] -s [SHELL [ARGS]]	
	
--------------------------------------------------------------------------	
# Switch user
--------------------------------------------------------------------------
	$ su   user_id					<= just switch user but same current directory
	$ su - user_id					<= go to user's HOME directory
--------------------------------------------------------------------------	


--------------------------------------------------------------------------	
# CD command
# Tilde( ~ ) extension
--------------------------------------------------------------------------	
	$ cd .. 		<= Parent  directory
	$ cd .			<= Current directory
	$ cd ~			<= User's home directory
	$ cd -			<= Previous directory
	$ cd ~-			<= goto last location
	$ echo ~-		<= Print out last location
	
	#e.g.
	$ mkdir delme && cd delme && mkdir -p 1/2/3			<= creating Subdirectories
	$ cd 1/2/3
	$ pwd
	  /home/apark/delme/1/2/3

	$ cd ../..						<= from /1/2/3  =>  /1   	
	$ pwd
	  /home/apark/delme/1
	$ cd -							<= move to previous directory
	$ cd ~-							<= move to previous directory
	
	$ echo ~-						<= Print previous directory
	

--------------------------------------------------------------------------
# History	
--------------------------------------------------------------------------	
	$ ls /var/log
	$ cd  !*						<= cd into last command directory
		$ cd /var/log
	
	$ !!   or	UP arrow key 		<= Previous command
	
	$cat /etc/shadow
		cat: /etc/shadow: Permission denied
		
	$ sudo !!
	
	$ crtl+r 			<= Reverse i search  
	
	$ cat vs tac
	$ tac <= concatenate and print files in reverse
		
	
	# Not recrod if there is a sapce at front
	$ export HISTCONTROL=ignorespace
	$ <space> clear						<= not record in history
	$ export HISTCONTROL=ignoredups
	$ export HISTCONTROL=erasedups 		<= delete all duplicated 
	
	*** add all in one line
	$ export HISTCONTROL="ignorespace:erasedups:ingreodups"
	$ echo 'export HISTCONTROL="ignorespace:erasedups:ingreodups"' >> ~/.bashrc
	
	$ export HISTIGNORE="history*:ll*:ls*" 
	$ echo 'export HISTIGNORE="history*:ll*:ls*:"' >> ~/.bashrc
	*** History Time stamp
	$ export HISTTIMEFOREMAT="%F>"
	$ echo 'export HISTTIMEFOREMAT="%F>"' >> ~/.bashrc
	*** History size
	$ export HISTSIZE=10000
	$ echo "export HISTSIZE=10000" >> ~/.bashrc
	
--------------------------------------------------------------------------	
# Touch 
--------------------------------------------------------------------------	
Creating an 'empty' file
  $ touch this_is_empty_file.txt
  $ echo "this is test" >> this_is_empty_file.txt

Update the time stamp		<= Just update the time stamp
  $ touch this_is_empty_file.txt
  $ echo this_is_empty_file.txt
	this is test
	
	
	
--------------------------------------------------------------------------	
# Brace expansion
--------------------------------------------------------------------------	
	
	$ touch apple banana cherry				<= creating multiple files
	$ touch {apple,banana,cherry}			<= !NO SPACE!creating multiple files
	
	$ touch file_{01..1000}					<= adding 0 to fixes the file name issue
		file_0001 .....file_10000
	
	$ touch cherry_{01..100}{w..d}.txt
	
	$ echo {1..10..2}						<= Prints 1~10 every 2nd number 
	  1 3 5 7 9
	$ echo {A..z..2}
	  A C E ... U W Y [ ] _ a c e ... u w y
	
	$ echo {A..Z}						<= Prints A to Z
	  A .... Z
	  
	$ echo {A..z}						<= Prints Capital A-Z, then a-z
		A B C D E F G H I J K L M N O P Q R S T U V W X Y Z  [  ] ^ _ ` <= This also prints out
		a b c d e f g h i j k l m n o p q r s t u v w x y z
	
	$ echo {{a..z},{A..Z},{1..10}}  	<= Double braces for combine

	$ echo {w..d..2}					<= Backward + every 2nd letter
--------------------------------------------------------------------------	
# Brace combination
--------------------------------------------------------------------------
	$ echo {{A..Z},{a..z}}				<= without printing  [  ] ^ _ ` 
	  A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 
		a b c d e f g h i j k l m n o p q r s t u v w x y z

	--------------------------------------------------------------------------
	
	$ echo {A..Z}{a..z}				<= Aa Ab Ac Ad Ae Af Ag Ah Ai ....
	$ echo {A..Z},{a..z}			<= A,a A,b A,c A,d A,e A,f A,g ....
	$ echo {{A..Z}{a..z}}			<= {Aa} {Ab} {Ac} {Ad} {Ae} {Af} {Ag} {Ah} {Ai}
	--------------------------------------------------------------------------
	$ echo {{A..Z},{a..z}}			<= A B C ... x y z  <-This is What we want!!!
	--------------------------------------------------------------------------

	# Counting how many outputs
	$ for i in $(echo {{A..Z},{a..z}}); do echo $i; done | wc -l	
		52
	$ for i in $(echo {A..Z}{a..z}); do echo $i; done | wc -l
		676
--------------------------------------------------------------------------	
# Brace Expansion - Variables 
--------------------------------------------------------------------------
	fruit="apple banana banana cherry"

	$ echo ${fruit/banana/mango}				<= change 1st banana to mango 
		apple mango banana cherry
	
	$ echo $fruit
		apple banana banana cherry				<= same 
	
	$ echo ${fruit//banana/mango}				<= // change all banana to mango 
		apple mango mango cherry
	
	$ echo ${fruit/#banana/mango}				<= # ONLY FIRST list item change
		mango banana banana cherry
	
	$ echo ${fruit/%cherry/mango}				<= % only the END of string
		apple banana banana mango
	
	$ echo ${fruit/*/mango}						<= * Matching
		cherry
	
--------------------------------------------------------------------------
# nl <= number line
--------------------------------------------------------------------------	

	$ nl file.txt
	$ cat file.txt | nl 

	$ grep -i 'tcp' /etc/services | awk '{print $1}' | sort | less

	
	### mkfifo - make FIFOs (named pipes)
	From 1st shell
	$ mkfifo named_pipe 
	$ echo "hi" > named_pipe
	
	From 2nd shell
	$ cat named_pipe
	# meta data
	prw-rw-r--. 1 apark apark 0 Jun  1 15:10 named_pipe

--------------------------------------------------------------------------	
### STDIN(0), STDOUT(1), STDERR(2)
--------------------------------------------------------------------------	
		Type		  Symbol
	-------------------------------------
	standard input		0<
	standard output		1>
	standard error		2>
	-------------------------------------
	
	$ sort < unsorted.txt > sorted.txt

	$ find /etc |sort|tee etcSort.txt|wc -l
	$ find /etc 2> etcErr.txt |sort|tee etcSort.txt|wc -l
	$ find /etc &> /dev/null   			<= NO OUT PUT
	
	$ my_prog < inputfile 2> errorfile | grep XYZ
	
	-----------------------------------------------------------------------	
  # Redirection 
    # Redirect outpu
	
	> 	<= redirect standard output (implicit 1>)
	& 	<= what comes next is a file descriptor, not a file (only for right hand side of >)
	2 	<= stderr file descriptor number
	
	
  	>	 Redirect_output	<= Stdout + Stderr 
	
	1>   success.log	<= Only Stdout 
	2>     error.log	<= Only Stderr 
	&>       redirect 	<= both stdout and stderr
	
	
	1>&2 	 all.log	<= Stdout + Stderr 
	2>&1	 all.log	<= Stderr + Stdout (stderr to stdout)
	>&		 all.log	<= Stdout + Stderr (both)	
	>&2		 all.log	<= Stdout + Stderr
	
	
  # close file descriptors for instance
  https://www.linuxunit.com/io-redirection-stdin-stdout-stderr-streams/
	n<&- 	, closing the input file descriptor n.
	0<&-, <&- 			Closing stdin.
	-----------------------------------------------------------------------
	
--------------------------------------------------------------------------
### '\' back slash
--------------------------------------------------------------------------	
	long command using  \  for extent command
	1. escape character \
	2. Long command 
		$ echo "test"\
		> "test"			<= No Space in front
		testtest			<= No Space



--------------------------------------------------------------------------	
### SSH 
--------------------------------------------------------------------------	
	-v  <=verbose 
	-V  <=Version 
	
	$ ssh -vvvv nozatech@192.168.221.129						<= Max verbose debugging mode
    
	OpenSSH_6.7p1, OpenSSL 1.0.1k 8 Jan 2015
    debug1: Reading configuration data /etc/ssh_config
    debug1: Connecting to 192.168.221.129 [192.168.221.129] port 22.
    debug1: Connection established.
    debug1: identity file /home/apark/.ssh/id_rsa type 1

	### SSL is TLS ###
	SSL(Secure Socket Layer) is the old name. 
	TLS(Transport Layer Security) is for now a days.
	https://curl.haxx.se/docs/sslcerts.html
	
--------------------------------------------------------------------------
### SSH, Putty, mRemote, Cygwins, Shell, Terminal ###
--------------------------------------------------------------------------	
	
3. SSH connection login
	$ ssh -v root@172.16.248.xx > result.txt					<= -v debugging mode
	$ ssh -v root@172.16.248.xx 2>&1 > result.txt	
	$ ssh id@x.x.x.x -p 2222									<= different port 2222
	$ ssh -l ubuntu ip_address (or hostname)					<= -l login user name
	
--------------------------------------------------------------------------	 
### sshd ### 
--------------------------------------------------------------------------	
	$ vi /etc/ssh/sshd_config
		RSAAuthentication 			yes
		PubkeyAuthentication		yes
		AuthorizedKeysFile 			.ssh/authorized_keys
		PasswordAuthentication 		no	<= NO!

	$ /etc/init.d/sshd restart							<= Restarts the sshd   service
	$ systemctl restart sshd 	
	
--------------------------------------------------------------------------
# ssh using  Alias vs ~/.ssh/config
									
	https://www.cyberciti.biz/faq/create-ssh-config-file-on-linux-unix/
	
	$ vi ~/.ssh/config
	-------------------------------------------------------------------------------------------
	Host netops
		HostName 45.55.5.69
		User apark
		IdentityFile ~/.ssh/id_rsa
		
	Host puppet
		HostName 45.55.5.69
		User apark
		Port 22
		IdentityFile ~/.ssh/id_rsa
	
	# SSH Connection 
	$ ssh apark@netops			<= from .ssh/config file
	
--------------------------------------------------------------------------	
### Login to internal lan server 192.168.0.251 via our public UK office ssh based gateway using ##
	## $ ssh uk.gw.lan ##
	Host uk.gw.lan uk.lan
		HostName 192.168.0.251
		User nixcraft
		ProxyCommand  ssh nixcraft@gateway.uk.cyberciti.biz nc %h %p 2> /dev/null
	-------------------------------------------------------------------------------------------
	
	
	 
	ProxyCommand : Specifies the command to use to connect to the server. The command string extends to the end
				   of the line, and is executed with the user’s shell. In the command string, any occurrence 
				   of %h will be substituted by the host name to connect, %p by the port, and %r by the remote 
				   user name. The command can be basically anything, and should read from its standard input 
				   and write to its standard output. This directive is useful in conjunction with nc(1) and 
				   its proxy support. For example, the following directive would connect via an HTTP proxy at 
				   192.1.0.253:  ProxyCommand /usr/bin/nc -X connect -x 192.1.0.253:3128 %h %p

--------------------------------------------------------------------------
# SSH Ciphers option
--------------------------------------------------------------------------
https://www.digitalocean.com/community/tutorials/understanding-the-ssh-encryption-and-connection-process

	$ ssh -c aes256-ctr apark@45.55.5.69
	$ ssh -c 3des-cbc   apark@45.55.5.69
	$ ssh -Q [cipher | cipher-auth | mac | kex | key]
	
	$ vi .ssh/config
	----------------------
	Host *
	Ciphers aes256-ctr
	----------------------
	


--------------------------------------------------------------------------	
# Set SSH debugging mode on server
--------------------------------------------------------------------------
	https://en.wikibooks.org/wiki/OpenSSH/Logging_and_Troubleshooting
	
	$ systemctl stop sshd 					<= your connection is still Alive
	$ /usr/sbin/sshd -ddd					<= running debugging mode on screen	

	## Client can't connect
		error msg: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
		Authentication refused: bad ownership or modes for directory .ssh
	
	# Resolution: change permission on user .ssh folder to 700
	$ chown 700 -R .ssh
	

	# MD5 (Message-Digest algorithm 5)
	https://help.ubuntu.com/community/HowToMD5SUM
	
	$ md5sum file_name
	When one has downloaded an ISO file for installing or trying Ubuntu, it is recommended to test 
	that the file is correct and safe to use. The MD5 calculation gives a checksum (called a hash value), 
	which must equal the MD5 value of a correct ISO.
	
	 md5sum - compute and check MD5 message digest
	
	
--------------------------------------------------------------------------	
###  basename 		<= 	strip directory and suffix from filenames

--------------------------------------------------------------------------	
	basename에 경로 이름을 지정하면 마지막 슬래시('/') 전까지의 앞 글자들을 지우고 결과를 반환한다.
	
	basename prints filename NAME with any leading directory components removed. 
	It can optionally also remove any trailing suffix.
	
	
	EXAMPLES
       basename /usr/bin/sort
              => "sort"

       basename include/stdio.h .h
              => "stdio"

       basename -s .h include/stdio.h					<-  -s, --suffix=SUFFIX
			  => "stdio"								remove a trailing SUFFIX		

            
       basename -a any/str1 any/str2					<- -a, --multiplemultiple arguments 
              => "str1" followed by "str2"				support multiple arguments and treat each as a NAME
			
	
	http://bahndal.egloos.com/595136
	특정 파일에 대해 작업을 할 때, 해당 파일의 디렉토리는 제외하고 파일명만 추출해야 할 필요가 
	있는 경우가 생기면 basename으로 간편하게 해결할 수 있다. 아래의 예시를 보자.

	# /home/john/Documents/my_doc.txt에서 파일명만 추출
	basename "/home/john/Documents/my_doc.txt"
	my_doc.txt

	만약 파일명 맨 뒤에 있는 확장자를 제거하고 싶다면 -s 옵션을 추가해 주자.

	# .txt 확장자 제거
	basename -s ".txt" "/home/john/Documents/my_doc.txt"
	my_doc

	여러개의 인자를 사용하고 싶다면 -a 옵션을 사용하면 된다.

	basename -a "/home/john/Documents/my_doc.txt" "/home/john/Downloads/my_data.bin"
	my_doc.txt
	my_data.bin

	예를 들어 디렉토리 정보가 포함된 특정 파일이 있을 때, 해당 파일을 임시 디렉토리에 복사하여 
	뭔가 작업을 하는 상황을 생각해 보자. 아래의 간단한 예시를 보면 금방 감이 잡힐 것이다.

	# 임시 디렉토리를 지정하여 변수 tmp_dir에 저장
		tmp_dir="/tmp/"
	
	# 파일 정보를 사용자 입력으로 받아 변수 file_info에 저장
		read -p "작업할 파일을 지정해 주세요: " file_info
	
	# 임시 디렉토리에 파일 복사
		cp "$file_info" "$tmp_dir"
	
	# 파일명만 추출해서 변수 f_name에 저장
		f_name=`basename "$file_info"`
	
	# 복사된 파일에 대한 grep 명령을 실행, 문자열 "ABCD"가 있는지 확인
		grep "ABCD" "$tmp_dir/$f_name"

--------------------------------------------------------------------------
### System OS start up  
--------------------------------------------------------------------------
	Check list
	$ ls -l  /etc/init.d/
	
	# Modify
	### CentOS 6
	$ chkconfig --list
	$ chkconfig ssh on
	$ chkconfig ssh off
	
	
	# Ubuntu Upstar(Debian)
	$ apt-get install openssh-server openssh-client
	$ update-rc.d ssh defaults			<= put it into start up
	$ update-rc.d ssh enable 			# sets the default runlevels to on 
	$ update-rc.d ssh disable 			# sets all to off
	

--------------------------------------------------------------------------	
### User management	
--------------------------------------------------------------------------
# useradd vs adduser
--------------------------------------------------------------------------	
	https://www.tecmint.com/add-users-in-linux/
	# https://askubuntu.com/questions/345974/what-is-the-difference-between-adduser-and-useradd
	
	**$ addduser 			<= Perl script that creates /home/user & ssh in shell
	**$ deluser 	     	    add user with full profile and info (pass, quota, permission, etc.)
	
		# System ID creation
		$ useradd  
		$ userdel  
		$ usermod 			<= low level utilities at add user name Only, No Shell, 
		# https://www.tecmint.com/add-users-in-linux/
	
	
	# Create system user accounts using 	--no-create-home 
											--shell /bin/false 
		options so that these users can't log into the server.
		
	$ useradd --no-create-home --shell /bin/false prometheus		<- System id
	$ useradd --no-create-home --shell /bin/false node_exporter		<- System id
	$ useradd --no-create-home -s      /bin/false alertmanager		<- System id

--------------------------------------------------------------------------		
# Change user name
--------------------------------------------------------------------------	
	https://askubuntu.com/questions/34074/how-do-i-change-my-username
	
	$ sudo usermod -l    newUsername   oldUsername
	
	$ sudo usermod -d   /home/newHomeDir  -m   newUsername
	
	$ ln -s    /home/newname   /home/oldname   
	

--------------------------------------------------------------------------		
# userdel: user is currently used by process 749 
--------------------------------------------------------------------------
	$ ps aux | grep 749
	$ kill -15 749
	$ killall -TERM -u userName
	$

--------------------------------------------------------------------------		
# sudo
--------------------------------------------------------------------------	
$ sudo -l			<= If no command is specified, list the allowed (and forbidden) 
						commands for the invoking user (or the user specified by 
						the -U option) on the current host. 
$ sudo -k	


	

### FROM CLIENT ###
### Add user without password but ssh key only
https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2

$ cat ~/.ssh/id_rsa.pub | ssh id@ip   "mkdir -p ~/.ssh && chmod 700 ~/.ssh && cat >>  ~/.ssh/authorized_keys"
$ cat ~/.ssh/id_rsa.pub | ssh apark@162.243.144.182  \
		"mkdir -p ~/.ssh && \
		chmod 700 ~/.ssh && \ 
		cat >>  ~/.ssh/authorized_keys"

		
		

## FROM SERVER ###
https://unix.stackexchange.com/questions/210228/add-a-user-wthout-password-but-with-ssh-and-public-key
# login to the remote server first
$ sudo su -
$ useradd -m -d /home/ipetcu -s /bin/bash  apark2			<= new user name 'apark2'
					-m --create-home		<= Create the user's home directory if it does not exist. 
					-d --home-dir			<= The new user will be created using HOME_DIR as the value 
												for the user's login directory.
					-s --shell				<= default SHELL 'BASH'

### Login as apark, switch user to ROOT.
					
	$ useradd -m -d /home/ipetcu -s /bin/bash ipetcu	&& cd /home/ipetcu
	$ mkdir .ssh && chmod 700 .ssh  && chown ipetcu: .ssh 
	$ touch /home/ipetcu/.ssh/authorized_keys && chmod 600 .ssh/authorized_keys && chown ipetcu:  .ssh/authorized_keys
	$ vi .ssh/authorized_keys

	$ cat id_rsa.pub >> .ssh/authorized_keys
	$ sudo visudo
	  ipetcu  ALL=(ALL)       NOPASSWD:ALL

	
	
--------------------------------------------------------------------------	
### /etc/passwd	
--------------------------------------------------------------------------	
	userID:pw:uid:gid:gecos:home             : shell
	icinga:x:992:987:icinga:/var/spool/icinga2:/sbin/nologin
	
	# Get GECOS 
	:FullName,RomAddress,WorkPhone,HomePhone,Others:
	https://superuser.com/questions/1031615/the-other-finger-gecos-fields-at-etc-passwd
	$ awk -F ":" '{print $5}' /etc/passwd	
	
	



	
	
--------------------------------------------------------------------------	
### PSSH tool includes parallel versions of OpenSSH and related tools such as:

	pssh – is a program for running ssh in parallel on a multiple remote hosts.
	
	pscp – is a program for copying files in parallel to a number of hosts.
		   Copy/Transfer Files Two or More Remote Linux Servers
	
	prsync – is a program for efficiently copying files to multiple hosts in parallel.
	
	pnuke – kills processes on multiple remote hosts in parallel.
	
	pslurp – copies files from multiple remote hosts to a central host in parallel.

	
	$ yum install python-pip
	$ pip install pssh


	
		
----------------------------------------------------------------------------------------------------
### Run command on remote
# 
http://unix.stackexchange.com/questions/19008/automatically-run-commands-over-ssh-on-many-servers
----------------------------------------------------------------------------------------------------
### Run command on remote
	$ ssh apark@192.241.190.57 	   'cat /etc/motd	'			<= w/o apark user id
	$ ssh puppet sudo              iptables -nL					<= using .ssh/config  file
	
	$ ssh apark@138.68.10.194 'sudo iptables -nL'				<= Doesn't work
		sudo: sorry, you must have a tty to run sudo	
	$ ssh -t apark@192.241.190.57  'sudo iptables -nL'			<= -t  Force pseudo-tty allocation
	


--------------------------------------------------------------------------		
### run script remotely
--------------------------------------------------------------------------	
	$ ssh apark@remote_server  'bash -s' < from_local_script.sh		<= run a script remotely from local using sudoer
	$ ssh apark@45.55.5.69     'bash -s' < ~/do/do_agent.sh              <= installing as sudo	

--------------------------------------------------------------------------		
### Create a server list text file and a update script first
--------------------------------------------------------------------------		
	$ for r in $(cat nrErrorServers.txt); do ssh -v $r 'sudo su && bash -s' < updateHosts.sh ; done
	
	$ for i in $(cat doUpdateList.txt); do ssh -t apark@$i 'sudo curl -sSL https://agent.digitalocean.com/install.sh | sh'; done
	$ for i in `cat doUpdateList.txt` ; do ssh -t apark@$i 'sudo curl -sSL https://agent.digitalocean.com/install.sh | sh'; done
	
### Ignoring 'adding remote host (yes/no)'
for i in $(cat doUpdateList.txt) ; do ssh -t -o "StrictHostkeyChecking no" apark@$i 'sudo curl -sSL https://agent.digitalocean.com/install.sh | sh'; done
for i in `cat do_server_list.txt`; do ssh -t -o "StrictHostkeyChecking no" apark@$i 'sudo yum remove do-agent -y && curl -sSL https://repos.insights.digitalocean.com/install.sh | sudo bash'; done


The authenticity of host '162.243.17.101 (162.243.17.101)' can't be established.
ECDSA key fingerprint is SHA256:6dyWrwSEQq0wgjRHRE+F95PsNsnNfr2ZquWA/qqnhJ8.
ECDSA key fingerprint is MD5:cf:42:fc:60:a0:80:ae:57:a7:bc:e6:4d:25:8a:11:6d.
Are you sure you want to continue connecting (yes/no)?


--------------------------------------------------------------------------		
### curl -FsSL link | sudo bash
--------------------------------------------------------------------------	
for i in `cat do_server_list.txt`; do ssh -t apark@$i 'sudo yum remove do-agent -y && curl -sSL https://repos.insights.digitalocean.com/install.sh | sudo bash'; done



RSA, DSA, and ECDSA keys that ssh uses
https://security.stackexchange.com/questions/178958/what-are-the-differences-between-the-rsa-dsa-and-ecdsa-keys-that-ssh-uses?noredirect=1&lq=1




--------------------------------------------------------------------------		
### PSSH ###
--------------------------------------------------------------------------	
	$ pssh -O StrictHostKeyChecking=no -h HostList.txt -l apark -A -i   -x '-t -t'    "sudo /etc/init.d/newrelic-sysmond restart"
 
 
 
--------------------------------------------------------------------------	
### PRSYNC ###	
--------------------------------------------------------------------------	
 	$ prsync -h /cdn/usEdgeServers.txt -l apark -v -x '-avz --delete' /cdn/data/ /cdn/data/
	
	http://unix.stackexchange.com/questions/87405/how-can-i-execute-local-script-on-remote-machine-and-include-arguments
	
	$ for host in $(cat host_list.txt); do ssh "$host" "$command" > "output.$host"; done
		Authenticating with name/password is really no good idea. 
		You should set up a private key for this:
	
	$ ssh-keygen && for host in $(cat hosts.txt); do ssh-copy-id $host; done
	
	
	
----------------------------------------------------------------------------------------------------	
4.  RSA KEY( 1024bit, 2k, 4k bit)
--------------------------------------------------------------------------	
	### You need to login to the server !!!first!!!
	$ ssh-keygen -t rsa							<= Generate  id_rsa (Private Key ) & id_rsa.pub (Public Key)	
	
    $ ssh-copy-id user_id@remote_server			<= Transferring Public Key
	
	$ cat ~/.ssh/id_rsa.pub | ssh user@ip "mkdir -p ~/.ssh && cat >>  ~/.ssh/authorized_keys"

	$ ssh remote_server "cat .ssh/*.pub"

	
	
	

	
	
----------------------------------------------------------------------------------------------------	
### Finding my ip (Gateway IP)
--------------------------------------------------------------------------	
	$ curl ifconfig.me
	$ curl ifconfig.me/all
	
	$ curl ifconfig.io	
	$ curl icanhazip.com

	https://opensource.com/article/18/5/how-find-ip-address-linux
	$ curl ifconfig.me
	$ curl -4/-6 icanhazip.com
	$ curl ipinfo.io/ip
	$ curl api.ipify.org
	$ curl checkip.dyndns.org
	$ curl ident.me
	$ curl bot.whatismyipaddress.com
	$ curl ipecho.net/plain
	
	$ dig +short myip.opendns.com @resolver1.opendns.com
	$ host myip.opendns.com resolver1.opendns.com
	

	### The following commands will get you the private IP address of your interfaces:
	$ ifconfig | grep -o '[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}'|head -1  <= RegEx 
	$ ifconfig -a
	$ ip addr 
	$ ip a  
	$ hostname -I | awk '{print $1}'
	$ ip route get 1 | awk '{print $NF;exit}'
	$ ip route get 8.8.4.4 | head -1 | awk '{print $7}'
	$ ip route get 8.8.4.4 | head -1 | cut -d' ' -f7
	
	# nmcli 	<= command-line tool for controlling NetworkManager
	$ nmcli -p device show
	
--------------------------------------------------------------------------
### jq - Command-line JSON processor
--------------------------------------------------------------------------	
	curl -s http://127.0.0.1:4567/clients | jq .   	<= -s slient 
	
---------------------------------------------------------------------------
### cURL vs wget  ### 
# powered by libcurl - a cross-platform library with a stable API 
https://daniel.haxx.se/docs/curl-vs-wget.html

Download a file into current folder
wget     http://file.com/get.zip
curl -O  http://file.com/get.zip			

Data Integrity  and Authenticity check using "SHA256SUM"
$ wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz
$ sha256sum       go1.10.3.linux-amd64 .tar.gz
	fa1b0........f901035  go1.10.3.linux-amd64.tar.gz  <= compare the value from downloaded site




--------------------------------------------------------------------------	
  # REST API Testing
	A. Content of the URL and display it in the STDOUT (i.e on your terminal)
		$ curl http://www.centos.org
	
	B. To store the output in a file, you an redirect it.
		$ curl http://www.centos.org > /tmp/centos-org.html  <= save to file
	
	C. Save the cURL Output to a file
		-o (lowercase o) saved in the file name e
		-O (uppercase O) the file name in the URL will be taken and it will be 
			used as the file name to store the result
		-l --list-only
		-L --location
	
	# -o 	<- saves the file with a predefined filename,
	  $ curl -o ~/tmp/<saving_name.html> http://www.gnu.org/text_info.html							
	  $ ls -l ~/tmp/
		text_info.html	
	
	# -O  	<- will save the file with its original filename
	  $ curl -O http://www.gnu.org/text_info.html  
	  $ ls -l 
		text_info.html   
	
	
	D.  Follow HTTP Location Headers with -L option  
		$ curl -L http://google.com
	
	E.	
		$ curl --limit-rate 1000B -O http:
	
	F. Proxy
		$ curl -p 
	
	G. $ curl -u username:password URL
	
	H. Verbose
	  $ curl -v http://google.com
	
	I. $ curl dict://dict.org/d:bash
	

	# to get the HEADER info	
	$curl -i http://address:3000/json-test 
  	
	HTTP/1.1 403 Forbidden
	Date: Wed, 19 Oct 2016 02:07:18 GMT
	Server: Apache/2.4.6 (CentOS)
	...
	Content-Length: 4897
	Content-Type: text/html; charset=UTF-8
	
	
	### Check multiple site
	$ curl -sSF http://site.{one,two,three}.com

--------------------------------------------------------------------------
*C*R*U*D*
--------------------------------------------------------------------------	

POST method
	$ curl -d "first=Alber&last=park" http://address:port/method_name

PUT method
	$ curl -X PUT -d "first=Alber&last=park" http://address:port/method_name

Delete
	$ curl -X DELETE  http://address:port/method_name

	$ curl -u 

	$ curl -X GET -H 
	
--------------------------------------------------------------------------	
https://docs.microsoft.com/en-us/appcenter/transition/moving/bulk

$ curl -X POST \												# -X <- request
    -F "apps[]=69e641a599f64e4c8adbb221b05a5ab1" \				# -F <-form
    -F "apps[]=096dea13e6244155b0a7733328dcff57" \
    -H "X-HockeyAppToken: 182a66de22s040l591s014645b9d7c87" \ 	# -H	<- header
    https://rink.hockeyapp.net/api/2/apps/move

	
----------------------------------------------------------------------------------------------------
#  WGET to download a file using user_id & PW
# Wget is command line only. There's no library.
--------------------------------------------------------------------------	
	$ wget --user=user_id --password='my_passwd' http://download.com/foo.pdf
	$ wget http://www.gnu.org/software/gettext/manual/gettext.html  or file_name	
	$ time wget http://www.gnu.org/software/gettext/manual/gettext.html  			<= measuring download time

	# download web folder files
	$ wget -r --no-parent http://www.download.com/folder_name/

	$ sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo
	
--------------------------------------------------------------------------
# User-Agent
--------------------------------------------------------------------------	
	When a software agent operates in a network protocol, it often identifies itself, 
	its application type, operating system, software vendor, or software revision, 
	by submitting a characteristic identification string to its operating peer. 
	In HTTP,[3] SIP,[2] and NNTP[4] protocols, this identification is transmitted in 
	a header field User-Agent. Bots, such as Web crawlers, often also include a URL 
	and/or e-mail address so that the Webmaster can contact the operator of the bot.
	
	
----------------------------------------------------------------------------------------------------
# Curl(transfer a URL) can download or upload through any protocol, http, ftp, sftp, scp, ldap, telnet
https://curl.haxx.se/docs/httpscripting.html
--------------------------------------------------------------------------	

curl 					www.yahoo.com 		-o 		output.html					# -o output to
curl -sL 	https://deb.nodesource.com/setup_6.x -o nodesource_setup.sh			# -s(silence)L(location) -o(output)
curl -O 				www.yahoo.com/index.html
curl -# -u id:pw 		ftp://ftphost/fuzicast/sample.txt -o sample.txt 		# the -# adds progress bar
curl -r 0-99 -u id:pw 	ftp://ftphost/fuzicast/sample.txt						# get first 100 bytes
curl -r -500 -u id:pw   ftp://ftphost/fuzicast/sample.txt 						# get last 500 bytes
echo "Hello World" | curl -T - -u id:pw \
						ftp://ftphost/fuzicast/sample2.txt
curl -T UNIX-1.14 -u id:pw ftp://ftphost/fuzicast/unix
curl -T localfile1 servername/remotefile1 -T localfile2 servername/remotefile2
curl -T UNIX-1.14 -u id:pw -a ftp://ftphost/fuzicast/unix 						# append to FTP file
curl --ftp-create-dirs -T UNIX-1.14 -u id:pw ftp://ftphost/fuzicast/unix/test.txt
curl --limit-rate 10240 -u id:pw ftp://ftphost/fuzicast/sample.txt 				# limit number of bytes per second .curlrc # curl configuration file
curl -u id:pw -z sample.txt ftp://ftphost/fuzicast/sample.txt 					# download remotefile only if it's newer than localfile
curl -z "Jan 12 2012" -u id:pw  ftp://ftphost/fuzicast/sample.txt 				# download remote file only if it's newer than Jan 12 2012
curl -B -u id:pw 				ftp://ftphost/fuzicast/sample.txt 								# enforces ASCII transfer during FTP download 
curl -u id:pw 					ftp://ftphost/fuzicast/sample.txt --create-dirs -o sampledir/sample.txt		# create directory if not exist
curl --key id_rsa # use SSH key
curl -u id:pw ftp://ftphost -Q 'RNFR /fuzicast/sample.txt' -Q 'RNTO /fuzicast/sampleyue.txt'	 # rename a remote file in FTP protocol
curl -u id:pw ftp://ftphost -Q 'rename /fuzicast/sample.txt /fuzicast/sampleyue.txt' 			# rename in SFTP is different from FTP
curl -R -u id:pw ftp://ftphost/fuzicast/sample.txt -o output.txt # reserve original file timestamp
curl -l -u id:pw ftp://ftphost/fuzicast/ 										# list remote filenames
curl -m 1800 -Y 3000 -y 60 servername/filename 									# speed must be greater than 3000 bytes per second for a minute and 
																				download process must be completed within 1800 seconds, otherwise the 
								  												download will abort
### Verify the API Token
$ curl -H "X-HockeyAppToken: 756408....da91"   https://rink.hockeyapp.net/api/2/apps


Curl -fsSL  ('f'ail 's'ilently but 'S'how errors in this 'L'ocation"
$ curl -fsSL 	-f  (fail silently(no output)) with -s option
				-sS (silence, --show-error_)When used with -s it makes curl show an error message if it fails.
				-L  (location)	
				
# Download and Install 
 execute the following command as the superuser
 "Hey, take this stream of commands from this URL on the internet and run it on my computer with superuser access."
 
  $ curl -sSL https://repos.insights.digitalocean.com/install.sh | sudo bash				
				


Save the script		=> curl -sSL https://repos.insights.digitalocean.com/install.sh -o /tmp/install.sh   (-o output to /tmp/install.sh)
View the contents 	=> less /tmp/install.sh 
Run the script manually => sudo bash /tmp/install.sh 



				
----------------------------------------------------------------------------------------------------	
### text browser ###
https://www.tecmint.com/command-line-web-browser-download-file-in-linux/

	$ sudo apt-get install -y links
	$ links http://www.google.com 
	
	'q' to quit



	
----------------------------------------------------------------------------------------------------
### base64 - base64 encode/decode data and print to standard output
--------------------------------------------------------------------------	
	http://stackoverflow.com/questions/201479/what-is-base-64-encoding-used-for
	https://www.base64decode.org/
	
	# Encoding
	  $ printf id:pw | base64 > encoded_data.b64	<= on to Files
	  $ base64 data.txt > data.b64
	
	# Decoding using -d option
	  $ base64 -d encoded_data.b64 					<= on to SCREEN(STDOUT)
	  $ base64 -d data.b64 > data.txt				<= on to Files
		

	$ export GRAFANA_PASSWORD="$(echo -n 'your_grafana_password' | base64)"
	
----------------------------------------------------------------------------------------------------
### How to check SHELL type
--------------------------------------------------------------------------	
	$ echo $SHELL
		/bin/bash
	$ echo $0
		-bash
	$ env | grep SHELL
		SHELL=/bin/bash
	$ set | grep SHELL
		SHELL=/bin/bash
	$ ps | grep $$
		3179 pts/0    00:00:00 bash
	$ ps -ef | grep $$

	# checking which shell
	$ ps 		<= All shells PID
	$ ps -p $$  <= Current shell PID
 

	### install tcsh and login
	$ yum install tcsh -y
	$ tcsh
	$ echo $0
		tcsh
		


--------------------------------------------------------------------------	
### cp
--------------------------------------------------------------------------	


--------------------------------------------------------------------------	
### type
	https://www.howtogeek.com/426014/how-to-use-the-linux-type-command/
--------------------------------------------------------------------------	
	Find out if a command resolves to an alias, a disk file, a shell function, a built-in 
		command, or a reserved word. 
	Use type to discover how your Linux commands are executed and understand your system better.
	
	$ type date
		date is /bin/date
	$ type ls
		ls is aliased to `ls --color=auto'
	$ type pwd
		pwd is a shell builtin

	$ type elif
		elif is a shell keyword
	$ type date top ls
	
	$ type -t date top ls		<= -t 	option as standing for “terse,” you won’t be far wrong. 
		file							It reduces the responses from type to single word answers.
		file
		alias

--------------------------------------------------------------------------	
### mv 
--------------------------------------------------------------------------	
	https://superuser.com/questions/901183/who-deals-with-the-star-in-echo
	e.g. test.txt test1(dir) test2(dir) test3(dir)
	$ mv *
	$ ls
		test3(dir)   <= cuz 'mv' test.txt test1(dir) test2(dir) => test3(dir)
	
	
--------------------------------------------------------------------------	
### rename
--------------------------------------------------------------------------	
	$ rename	a to b
	$ mv		a to b
	

### yes 		
	$ yes Testing 				<= output a string repeatedly until killed




	
### rm 
	$ rm -rf data.{1..9}				<= delete data.1 ~ data.9
	$ rm "the file name.mp3"			<= if there are spaces in file name
	

--------------------------------------------------------------------------		
### grep
--------------------------------------------------------------------------	
	$ grep --color=auto apark /var/log/secure
	$ export GREP_OPTIONS='--color=auto'


	# Finding a string word using grep
	$ grep -i word *		<= current dir
	$ grep -i word */*		<= sub dirs
	$ grep -i word */*/*    <= sub sub dirs
	
	# Only first grep result 
	$ grep -i -m 1 'centos' /etc/*rel*
	$ cat /etc/*rel* |  grep -i -m 1 'centos'

--------------------------------------------------------------------------		
# File Globs
--------------------------------------------------------------------------	
	1) * (asterist)
		$ file.*			<= matches any number of any characters
		$ file*.txt
		$ *.jpg
	
	2) ? Matches one(1) of any character
		$ ls -l file?.txt
		$ ls -l ?.jpg
		
	3) [ ] Character Sets
		$ ls -l file[0-9].txt			<= file1.txt
		$ ls -l file[a-z].txt
		$ ls -l file[abc123].jpg		<= any One from [abc123];  a.jpg, b.jpg, 3.jpg
	
	4) [ - ] Matches a '-' (hyphen)
		$ file[-9-0].txt		
		  file-.txt 
		  file1.txt
	
	5) [! ] Negate(NOT) a match; Do Not Match 0-9.txt
		$ file[!0-9].txt		
		  filea.txt
		  fileb.txt
	
	6) [::] Matches on character of a certain type
		Claass		Match
		[:digit:]	Numbers
		[:uppper:]	Upper case characters
		[:lower:]	lower case characters
		[:alpha:]	Upper and lower case
		[:alnum:]	Upper and lower case plus numbers
		[:space:]	Space, tabs, and newlines
		[:graph:]	Printable characters, not including spaces
		[:print:]	Printable characters, including spaces
		[:punct:]	Punctuation
		[:cntrl:]	Non-printable control characters
		[:xdigit:]	Hexadecimal characters
		
	$ ls file[0-9].txt
	$ ls f*[[:digit:]].txt
	
	$ ls file[[:digit:][:space:].txt
	$ ls file[![:digit:][:space:].txt			<= ! Negate(but this)
	
	$ ls file[![:digit:]].txt
	$ ls file[![:digit:][:space:]].txt 
		
	$ ls {*.jpg,*.gif,*.png}		<= works on 2 or more
		a.jpg, b.jpg, c.png, d.gif
	$ ls -l {b*,c*}
	$ rm *[^1].txt					<= delete all but *1.txt
	$ rm *[!1].txt					<= range, delete all but *1.txt
	$ cat myfile | grep '^s.*n$'	<= end of the line




--------------------------------------------------------------------------		
### Extended Globs
--------------------------------------------------------------------------	
	$ shopt
	$ shopt -s extglob				<= put into .bashrc

	1) +(match)
		$ file+(abc).txt			<= fileabc.txt, fileabcabc.txt

	2) +(match|match)
		$ ls +(photo|Photo)*+(.jpg|.gif)		
			photo.jpg, Photo.gif
		
	3) *(match)
		$ ls test*(1).*				<= test1.jpg, testfile1.png, testUp1.txt
		$ ls test*1.*				<= test1.jpg, testFile1.png, testUp1.txt
		
	# Invert match	
	4) !(match)						<= Don't match 
		ls -l !(*.jpg|*.png)
		
	5) !(+(match)*+(match))			<= group match
		$ ls !(+(photo|file)*+(.jpg|.gif)) 	<= All files that do not have photo or file, and 
												don't end with jpg or gif.
--------------------------------------------------------------------------				
# Meta-Characters			
--------------------------------------------------------------------------	
	< >, $, *, {}, [], +, '', ^, ., ?, |, (), \, ""

	$ USER=apark
	$ echo  $USER
	$ echo  My name is $USER
	$ echo "My name is $USER"     	<= apark
	$ echo 'My name is $USER'		<= $USER
	
	
	### /etc/services http://www.penguintutor.com/linux/network-services-ports
	maps port numbers to named services(SSH 22/tcp).
	a simple database that associates a human friendly name to a machine friendly service port.

	$ grep '*./tcp' /etc/services | grep ssh
		ssh  22/tcp      # The Secure Shell (SSH) Protocol
	
	# Socket(IPP) is the combination of IP address, port and protocol.
	


	


	
--------------------------------------------------------------------------			
# free  
--------------------------------------------------------------------------		 
	### Most memory used by process
	
	$ ps aux --sort=-%mem | awk 'NR<=10{print $0}'	

		  <= Check memory
	
	### Displays the total amount of free and used physical and swap memory in the system, 
		as well as the buffers used by the kernel. The shared memory column should be ignored; it is obsolete.
		
	$ free -m | xargs | awk '{ print "Free/Total memory " $10 "/" $8 "MB" }'

	$ free -m | grep Mem | awk '{print $4 "/" $2 "MB free"}'

	$ free -tom
         -m MB 
         -t switch displays a line containing the totals.
         -o switch disables the display of a "buffer adjusted" line.  If the -o
            option is not specified, free subtracts buffer memory from the  used
            memory and adds it to the free memory reported.
	$ egrep --color 'Mem|Cache|Swap' /proc/meminfo

Swap file 
ubuntu-18-04/
	https://linuxize.com/post/how-to-add-swap-space-on-ubuntu-18-04/
	
--------------------------------------------------------------------------	
# vmstat   
--------------------------------------------------------------------------	
	# virtual memory statistics 
	# reports information about processes, memory, paging, block IO, traps, and cpu activity.	
	
	# Check memroy size first
	$ free -m		<=MB
					total        used        free      shared  buff/cache   available
		Mem:         7756         439        3244         148        4072        6744
		Swap:        7935           0        7935
	
	# top <- real time system monitoring
	$ top
	  # 1   <= show list all CPUs
		top - 11:39:06 up 91 days,  1:23,  1 user,  load average: 0.60, 0.29, 0.25
		Tasks: 197 total,   2 running, 195 sleeping,   0 stopped,   0 zombie
		%Cpu0  :  0.3 us,  0.7 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
		%Cpu1  :  0.0 us,  1.0 sy,  0.3 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
					user	system	nice	idle	wait time(i/o) 	
		KiB Mem :  7942812 total,  3318900 free,   453520 used,  4170392 buff/cache
		KiB Swap:  8126460 total,  8126460 free,        0 used.  6903032 avail Mem
	  
	  # shift + .  <= ( > ) top memory usage
	  # shift + ,  <= ( < ) top CPU usage	

	# Stress Testing CPU 1
	$ stress --help
		Example: stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 10s

	$ stress -c 1
	$ top
		1
	$ stress -m 3   <= 3GB  
	  
	$ vmstat 1 20 		<= one (1) second twenty (20) times:
	$ vmstat 30 		<= ongoing report for intervals of 30 seconds
	$ vmstat -S k 1 10  <= S-switch unit (k kilobyte, K, m, M)
	$ vmstat 10(sec) 6(times)  	
	$ vmstat -a (active)
		procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
		r  b   swpd   free  inact active   si   so    bi    bo   in   cs us sy id wa st
		1  0      0 600324 140380 175672    0    0     5     2    8   13  0  0 100  0  0
		
		procs	<= umber of processing jobs waiting to run
		memory  <= same as free -m
		swap	<= memory is sent to or retrieved from the swap system
		io		<= input and output activity per second  in terms of blocks read and blocks written
				   bi -block in, bo-block out
		system	<= number of system operations per second
		cpu		<= always add to 100 and reflect “percenDATEe of available time”.	
			
	$ vmstat -s    <=-s switch displays summary of various event counters and memory statistics.
      1009992 K total 	 memory
       410888 K used 	 memory
       176468 K active 	 memory
       140376 K inactive memory

	$ vmstat -d  		<=  -d option display all disks statistics.
	    disk- ------------reads------------ ------------writes----------- -----IO------
				total merged sectors      ms  total merged sectors      ms    cur    sec
		ram0       0      0       0       0      0      0       0       0      0      0
		ram1       0      0       0       0      0      0       0       0      0      0
		ram2       0      0       0       0      0      0       0       0      0      0

		



		
6. Creating swap space in CentOS7
	http://www.cyberciti.biz/faq/linux-add-a-swap-file-howto/
	
	$ swapon -s   <= -s, summary
	$ free -m
	
#### Create swap space	
https://www.2daygeek.com/add-extend-increase-swap-space-memory-file-partition-linux/#
#--------------------------------------------------------------------------------
#4GB with 1MB bitesize
dd if=/dev/zero of=/swapfile count=4096 bs=1MiB   

#2GB with 512k bitesize 
dd if=/dev/zero of=/swapfile bs=2048 count=512k		<- created 1GB, Not 2GB ???

$ sudo chmod 600 /swapfile
$ mkswap /swapfile
$ swapon /swapfile

# To enable it at boot up, add to /etc/fstab
$ echo "/swapfile  swap  swap  defaults  0 0"  >>  /etc/fstab   

#--------------------------------------------------------------------------------	
#### Increase the exisiting Swap space	




### How do I verify swap is activated or not?
	$ free -h
	$ swapon
	$ swapon --show
	$ less /proc/meminfo
	$ top           									<= check in KiB Swap
	$ cat /proc/swaps
	$ cat /proc/meminfo | grep -i swap
	
	
	### bug, not working	###
#	$ sudo fallocate -l 2G /swapfile   <= 2GB space for swap
#	$ sudo chmod 600 /swapfile
#	$ ls -lh /swapfile
#	$ sudo mkswap /swapfile
#	$ sudo swapon /swapfile
###	

### fallocate - preallocate space to a file

	$ fallocate -l 1G   1gb_file		<= 1078 size ??
	$ fallocate -l 1GB  1gb_file		<= 1000 size ??
	
	
### stat			<=display file or file system status

	$ stat file.name
	
	File: ‘file.name’
	Size: 80              Blocks: 8          IO Block: 4096   regular file
	Device: fd02h/64770d    Inode: 1074617621  Links: 1
	Access: (0664/-rw-rw-r--)  Uid: ( 1000/   apark)   Gid: ( 1000/   apark)
	Context: unconfined_u:object_r:user_home_t:s0
	Access: 2018-01-25 14:37:35.037545396 -0800
	Modify: 2018-01-25 14:37:32.473485310 -0800
	Change: 2018-01-25 14:37:32.483485544 -0800
	Birth: -
	
	$stat -f /
	File: "/"
    ID: fd0000000000 Namelen: 255     Type: xfs
	Block size: 4096       Fundamental block size: 4096
	Blocks: Total: 13100800   Free: 9197614    Available: 9197614
	Inodes: Total: 52428800   Free: 52251919

### Type command
	The type command can be used to get a description of the command type:

	$ type rm
		rm is hashed (/bin/rm)
	$ type cd
		cd is a shell builtin
	
	
7. sysstat	Linux Performance Monitoring package

	Collective CPU usage
	Individual CPU statistics
	Memory used and available
	Swap space used and available
	Overall I/O activities of the system
	Individual device I/O activities
	Context switch statistics
	Run queue and load average data
	Network statistics
	Report sar data from a specific time

	
	
$ iperf
	Measures TCP bandwidth; 
	Reports on maximum segment size and maximum transmission unit;
	Support for TCP Window size;
	Multithreaded for multiple simultaneous connections;
	Creates specific UDP bandwidth streams;
	Measures packet loss;
	Measures delay jitter;
	Runs as a service or daemon; and
	Runs under Windows, Linux OSX or Solaris.
	
	$ iperf -s
	$ iperf -c remote_server     				<= -c host


	
https://www.server-world.info/en/note?os=CentOS_7&p=sysstat
$ yum install -y sysstat
$ systemctl start sysstat
$ systemctl enable sysstat

$ sudo cat /etc/cron.d/sysstat
	# Run system activity accounting tool every 10 minutes
	*/10 * * * * root /usr/lib64/sa/sa1 1 1
	# 0 * * * * root /usr/lib64/sa/sa1 600 6 &
	# Generate a daily summary of process accounting at 23:53
	53 23 * * * root /usr/lib64/sa/sa2 -A
	
#	
#	$ vi /etc/default/sysstat   						<= change to true for data collection
# 	$ service sysstat restart
#	$ sar -A > $(date +`hostname`-%d-%m-%y-%H%M.log)    <=save the statistics
	
http://www.tecmint.com/sysstat-commands-to-monitor-linux/
	
	### iostat  <= displays CPU and I/O statistics of all partitions as shown below.
	$ iostat
		Linux 3.16.0-23-generic (puppet.nozatech.com)   03/24/2016      _x86_64_        (8 CPU)
		avg-cpu:  %user   %nice %system %iowait  %steal   %idle
				0.01    0.01    0.08    0.05    0.00   99.86
		Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
		sda               2.01        36.44        11.34     236588      73606
		dm-0              2.98        33.48        11.34     217373      73592
		dm-1              0.04         0.17         0.00       1080          0
	
	$ iostat -N 			<= With -N (Upper-case) parameter displays only LVM statistics as shown.
	$ iostat -p sda 		<= By default it displays statistics of all partitions, with -p and 
								device name arguments displays only disks I/O statistics for specific device only as shown.
	$ iostat -d				<= -d arguments displays only disks I/O statistics of all partitions as shown.
		
	$ mpstat -P ALL
	
	$ pidstat
	
	$ cifsiostat 
		
		
		
		
		
8.	slabinfo - kernel slab allocator statistics
	cat /proc/slabinfo
	Slab allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects.
	
	
	
	
9. Utility to check real time processing
    A. top 
        $ top -bn 1 			<= Top Batch Number 1			    
								<= capture into single page file  
								<= -b <= batch mode, n <= number, 1 <= count 
		
		$ top -bn 1 | awk "/$1/ {tot =+ \$6; n++} END {print tot\" \"n}"      <=??
		
		### A Top % memory usage app value
		$ top -o %MEM -bn 1 |  awk '8 <=NR && NR <=8' | awk '{print $10}'
		
		
		### Top 5 CPU processes
		
		### Using head & tail
		$ top -bn 1 | head -n 12 | tail -n 6  	<= head displays head of 12 lines 
												   tail from bottom 6 lines ( Displays 7,8,9,10,11,12 lines)
		  
		  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.74 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.13 kworker/u128:0

		### using sed
		$ $top -bn 1 | sed -n '6,12p'

			PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.74 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.14 kworker/u128:0
		
		### Using awk for specific line 6 to 12
		$ top -bn 1 | awk '6 <=NR && NR <=12'

			PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.76 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.15 kworker/u128:0

						
		### Top commands short cuts:
	***	1 – To display or hide all other CPU’s
		A - Split into multiple CPU screen
		b - bache mode sending output from top to other programs or to a file
		c – To display or hide command full(absolute) path
		d - display rate from 3sec to any sec
		n - number of iteration 
		l – To display or to hide load average line
		t – To display or to hide task/cpu line
		m – to display or to hide RAM and SWAP details
		s – To change the time interval for updating top results(value is in sec’s)
		R – To sort by PID number
		u — Press u then user name to get only that user process details
		P – To sort by CPU utilization
		M – To sort by RAM utilization

		r – To renice a process, press r then the PID no then the renice value to renice a process.
	***	k – To kill a process, press k then PID number then enter to kill a process
		w – To save the modified configuration permanently.
		
		q – To quit the top command.
	***	h – for getting help on top	

		
	shift + m 		<= list process according to memory usage
	shift + p		<= list the process according to cpu usage
	shift + w		<= for saving the top output in a file(/root/.toprc.)
	shfit + o		<= for sorting the process as per requirement
		
	# User process list	
	$ top -u apark
	
	# quit after 1 iteration
	$ top -n 1 	
		
	# Batch mode( capture as text file)
	$ top -bn 1	 > top_process.txt
	
	$ top -d300 -b > top_log.txt
	
	
		
	B. atop
		Atop is an ASCII full-screen performance monitor which can log and report 
		the activity of all server process up to 28days log.
	
		$ atop -a <= sort in order of most active resource.
		$ atop -c <= revert to sorting by cpu consumption (default).
		$ atop -d <= sort in order of disk activity.
		$ atop -m <= sort in order of memory usage
		$ atop -n <= sort in order of network activity
  
    C. htop
		Up/Down arrow keys to select a process, and then you can kill it with the F9 key

		
	## Memory Usage  
	https://unix.stackexchange.com/questions/128953/how-to-display-top-results-sorted-by-memory-usage-in-real-time
	$ top -o %MEM -bn 1 > topMemoryUsage.txt
	$ top -o %MEM -bn 1 | awk '8 <=NR && NR <=8
		
		
10. Disk usage / folder size / disk space
https://www.tecmint.com/find-top-large-directories-and-files-sizes-in-linux/

	$ du -hs * | sort -rh | head -5
	
### Find top 5 biggest files in MB/GB ###	

	$ find  /  -type f -exec du -Sh {} + | sort -rh | head -n 5
	
	
	#$ find  /  -type f -printf "%s %p\n" | sort -rn | head -n 5
	
	$ du -ah              					<= all total size files & directories, -a all, -h human 
	$ du -ah *   							<= No total size
	$ du -sh /home/noza*					<= total holder size including subfolders
	$ du -sh /home/noza*  | sort -nr		<= Folder list usage, not files
	$ du -sh /home/noza*  | sort -nr		<= Folder list usage, not files
	
######################################################################################
				total       used       free     shared    buffers     cached
	Mem:           482        304        178          0         51        137
	-/+ buffers/cache:        115      **367** <= Actual Free Memory


	$ free -m | awk 'NR==3 {print $4 " MB"}'
	#
	# When thinking about 'how much memory is really being used' :
	# 'used' - ('buffers' + 'cached')
	Actual Memory usage 116MB = 304-(51+137)
	#
	# When thinking about 'how much memory is really free' :
	# 'free' + ('buffers' + 'cached')
	Actual Free memory   336MB = 178+(51+137)
######################################################################################


	$ iotop  				<=check I/O usage


### exec
	https://askubuntu.com/questions/525767/what-does-an-exec-command-do
	exec serves to also replace current shell process with a command, 
	so that parent goes a way and child owns pid

	### Exectute and exit
	$ ps -p $$
	PID TTY          TIME CMD
	2441 pts/1    00:00:00 bash			<= current shell
	
	$ exec ps -p $$
	PID TTY          TIME CMD
	2441 pts/1    00:00:00 bash			
	Connection to 10.100.5.155 closed.	<= Exit right after execute from the current shell
	
	### Exec and Fork
	$ ps
		PID TTY          TIME CMD
		8386 pts/1    00:00:00 bash				<= only current shell
	$ bash										<= launch a new bash shell	
	$ ps -p $$
		PID TTY          TIME CMD
		8414 pts/1    00:00:00 bash				<= newly lauched bash shell
	
	$ exec > newShell.log						<= STDOUT redirected to newShell.log file, not on screen
	$ date										<= NO STDOUT
	$ exit
	$ ps -p $$
		2441 pts/1    00:00:00 bash				<= current shell
	$ cat newShell.log
	
### fork
	Normal programs are system commands that exist in compiled form on your system. When such a 
	program is executed, a new process is created. This child process has the same environment 
	as its parent, only the process ID number is different. This procedure is called forking.
	
	It provides a way for an existing process to start a new one. But there may be situation that 
	child process is not the part of same program as parent process is. In this case exec is used. 
	exec will replace the contents of the currently running process with the information from a 
	program binary.
	
	After the forking process, the address space of the child process is overwritten with the new 
	process data. This is done through an exec call to the system.
	
	
	
### Date format 
	$ date
	  Tue Jan 15 11:29:01 PST 2019
	  
	$ DATE=`date   +%H:%M:%S`			 <= `command.....`      <- Legacy way
	$ DATE=$(date  +%H:%M:%S)			 <= $(command....)
	$ DATE="$(date +%H:%M:%S)"			 <= $(command....)
		
		$ echo $DATE
		  22:18:35
	
	$ date +%F
	  2016-08-11
	
	$ timestamp=`date +%F`  			<= set variable name 'timestamp'
	$ timestamp=$(date +%F)  			<= set variable name 'timestamp'
	
	$ date +%Y-%m-%d-%H-%M-%S
	  2019-01-15-11-30-02

	$ DATE=`date +%Y-%m-%d-%H-%M-%S`  	 <= %Y is 2016, %y is 16
	$ echo $DATE
	  2016-03-24-22-15-39


	$ DATE='$(date +%H:%M:%S)'			<= NO Single Quote XXX
		$(date +%H:%M:%S)				<= Literal with Single Quotes

	 
	# time stamp for script
	$ date +%s                 			<= %s  seconds since 1970-01-01 00:00:00 UTC
	
	$ date +%S                 			<= from 60 seconds (0~60 sec)
	
	# milliseconds since 1-1-1970(Use %3N to truncate the nanoseconds to the 3 most significant digits)
	$ date +%s%3N
		1397392146866 
	 
	$ date +%Y-%m-%d-hour-%H:%M:%S
	
	$ date +Name:`hostname`/Date:%Y-%m-%d/hour:%H:%M:%S 
	  Name:PC_Name/Date:2016-10-28/hour:11:37:43
	  
	$ date +Name:`hostname`\|Date:%Y-%m-%d\|hour:%H:%M:%S
	  Name:i7|Date:2019-02-25|hour:14:42:34

	$ date +"%m-%d-%Y"   		<= 12-17-2014
	$ date +"%H:%M:%S"   		<= 10:20:18

	$ date +"Today's date is %m-%d-%Y and time is %H:%M:%S."
		Today's date is 02-25-2019 and time is 14:38:29.

	$ date +%T
	$ date +%D
	
	
	$ date +%F   				<= 2018-01-09  'F'ull date; same as %Y-%m-%d
	
	
	
###	ls
	http://unix.stackexchange.com/questions/21638/show-only-hidden-files-dot-files-in-ls-alias
# list directory
	$ ls -dl */					<= -d directory
	$ ls -Xl    <= sort alphabetically by entry extension
	$ ls -1     <= list one file per line <= number 1, showing names only 

# list hidden files in current directory  
	$ ls -d  .*	  				<= list hidden .files only
	$ ls -ld .*	  				<= list hidden .files only
	$ ls -l  .*?				<= list hidden .files only
	$ ls -Ad .*
	$ ls -a | grep "^\."    	<= show hidden files start with . file

	
	$ hidden() { ls -a "$@" | grep '^\.'; }		<= function
	$ alias hid="ls -a | grep '^\.'"			<= alias

	  
### Route or IP

	$ route
	$ ip route
	
	$ /sbin/route -n					<=Numeric value
	Kernel IP routing table
	Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
	0.0.0.0         192.168.1.1     0.0.0.0         UG    0      0        0 eth0
	192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0

	Add / set-up a new route
	
	$ route add default gw {IP_ADDRESS} {INTERFACE_NAME}
	$ route add default gw 192.168.1.254 eth0
	
	$ ip route add 192.168.1.0/24 dev eth0
	$ ip route add 192.168.1.0/24 via 192.168.1.254
	
------------------------------------------------------------------------------------------
### Compression 	
------------------------------------------------------------------------------------------

tar -cvzf => tar -xvzf

  ## tar ('T'ape 'Ar'chive) compression for backup --> 'tarball')

  # Creating tar file 
	# tar -cvpzf
			-c(create)
			 v(verbose)
			 p(preserve permission)
			 z(compression)
			 f(filename)
	
				    <tar name>			 <from what>
	$ tar -czvf   archive-name.tar.gz  directory-location	
	
	$ tar -czvf  backup.tar.gz /home/nozatech
	$ tar -cvpzf backup.tar.gz /(root)        --exclude=/mnt /
	$ tar -cvpzf backup.tar.gz /home/nozatech
  
  ----------------------------
  # Extract tar file
  ----------------------------
				<tar name>			<target location>	
  $ tar -xvzf   backup.tar.gz   -C  /recover
			-x(extract)
			v(verbose)
			p(permission)
			z(uncompress)
			f(file name) 
		-C (change to different directory) 
	
	$ tar -xvzf  backup.tar.gz /home/nozatech
	$ tar -xvpzf backup.tar.gz /home/nozatech	
	$ tar -xzvf nexus-3.2.0-01-unix.tar.gz   -C   /opt


	$ tar -czvf  <= create tar zip file
	$ tar -xzvf  <= extract tar zip file
	$ tar -tf    <= extract tar file
	
	#  Zipping or archiving
	$ bzip2 ls.txt &
	 [1] 28072
	$ jobs
	 [1]+  Done                    bzip2 ls.txt
	$ ll
	 -rw-r--r--. 1 root root  932 Jun  9 14:42 ls.txt.bz2
	

	
	
#-----------------------------------------------------------------------------------------------	
### Cron Jobs
	$ crontab -l (check teh crontab list)
	$ crontab -e (first time ask a editor - choose vim)
		* * * * *
		min         hr         day of month       month          day of month          command
		0-59 mins   0-23 hrs   1-31days           1-12           0(sun)-6(sat)
		30	        2     	   * 	              *              2(tuesday)
		
		
		
cron job search from command line
* * * * *  (*)<-Not used
| | | | | 	| 
| | | | | 	+-- Year            (range: 1900-3000)         
| | | | +---- Day of the Week   (range: 1(Monday)-7)
| | | +------ Month of the Year (range: 1-12)
| | +-------- Day of the Month  (range: 1-31)
| +---------- Hour              (range: 0-23)
+------------ Minute            (range: 0-59)

# examples 
	0 0 * * 1,3,5   			   <= 12AM at every Mon, Wed, Fri
	@reboot        				   <= Run once, at start up.  Put start up script!!!!
	0 0 1 1 *		@yearly        <= Run once a year
	@annually       @yearly 
	0 0 1 * *		@monthly       <= Run once a month
	0 0 * * 0		@weekly        <= Run once a week
	0 0 * * *		@daily         <= Run once a day
	@midnight       @daily 
	0 * * * *		@hourly        <= Run once an hour
	
	* * * * * /sbin/ping -c 1 192.168.0.1 > /dev/null
	0 0,12 1 */2 * 				   <= every 12am & 12pm on 1st day of every 2nd month
	30 08 10 06 * 				   <= June 10th 8:30AM (every year)
	00 09-18 * * 1-5 			   <= Everyday 9am to 6pm Mon ~ Fri
	
	
https://en.wikipedia.org/wiki/Cron
Entry	Description	Equivalent to
@yearly (or @annually)	Run once a year at midnight of 1 January		0 0 1 1 *
@monthly	Run once a month at midnight of the first day of the month	0 0 1 * *
@weekly		Run once a week at midnight on Sunday morning				0 0 * * 0
@daily		Run once a day at midnight									0 0 * * *
@hourly		Run once an hour at the beginning of the hour				0 * * * *
@reboot		Run at startup	N/A	
	

### Jenkins Cron jobs	
http://stackoverflow.com/questions/12472645/how-to-schedule-jobs-in-jenkins
15  13  *    *    *    				<= run 15th minute of the 13th hour of the day
min hr day month week

0 0 * * * for a dozen daily jobs will cause a large spike at midnight.
| |
H H * * * would still execute each job once a day, but not all at the same time, 
		  better using limited resources.
		The "H" symbol can be thought of as a random value over a range, but it actually is 
		a hash of the job name, not a random function, so that the value remains stable for 
		any given project.
H 8 * * *				<= run every 8AM
H 8 * * 0				<= run on Sunday 8AM
H/15 * * * * 			<= every fifteen minutes
H(0-29)/10 * * * *		<= every ten minutes in the first half of every hour 
							(three times, perhaps at :04, :14, :24)
H 9-16/2 * * 1-5 		<= once every two hours every weekday 
							(perhaps at 10:38 AM, 12:38 PM, 2:38 PM, 4:38 PM)
H H 1,15 1-11 * 		<= once a day on the 1st and 15th of every month except December	


0 12 * * 1-5			<= 12:00 every weekday (Mo-Fr)
H 2 * * *				<= Every 2am
ss


	
# Install crontab
	$ crontab cron_jobs.txt
	
	###
	$ cat cron_jobs.txt
	* * * * *         /sbin/ping -c 1 192.168.0.1  > /dev/null 2>&1
	* * * * *         /sbin/ping -c 1 192.168.0.1  2>&1 >> /tmp/log   <= save to log file
	
	0 0,12 1 */2 *    /cmd				    # every 12am & 12pm on 1st day of every 2nd month
	30 08 10 06 * 	  /cmd					# June 10th 8:30AM
	00 09-18 * * 1-5  /cmd			   		# Everyday 9am to 6pm Mon~Fri
	5 * * * *								# every 5th min each hour
	*/5 * * * *								# every 5 mins
	
	###
	
	$ crontab -l				<= current user's list of cron jobs 
	$ crontab -e				<= edit cron job
	
	### Finding cron job list for specific user ### 	
	$ crontab -u user_id -l					<= list jobs 
	$ crontab -u user_id -e					<= edit
	
	### Finding cron job list for all users ###
	####################################################################################
	###   $ for user in $(cut -f1 -d: /etc/passwd); do crontab -u $user -l; done     ###
	####################################################################################
	
	# Run as SUDOER for permission issue
	$ sudo "for user in $(cut -f1 -d: /etc/passwd); do echo $user; crontab -u $user -l; done"

	
	a. Getting all user id info from /etc/passwd using 
		$ cut -f1 -d:  /etc/passwd
    
	b. using crontab utility to check list of cron job list
		$ crontab -u $user -l

	##########################################
	# #!/bin/bash
	# for user in $(cut -f1 -d: /etc/passwd); 
	# do 	
	#    echo $user; crontab -u $user -l; 
	# done
    ##########################################
#-----------------------------------------------------------------------------------------------

	
	
15. List files by modified date 
    ls -atlh   			 	 <= -a all, -t modified time, -l per line 
    ls -atlhr   			 <= -r reverse
	
	list directory only
	$ echo */ 
	$ ls -d */
	$ ls -dl */
	$ ls -l | grep "^d"
	$ for i in $(ls -d */); do echo ${i%%/}; done
	$ for i in $(ls -d */); do echo ${i}; done

	$ ls -l
-rwxrw-r--    10    root   root 2048    Jan 13 07:11 afile.exe
	
?UUUGGGOOOS   00  UUUUUU GGGGGG ####    ^-- date stamp and file name are obvious ;-)
^ ^  ^  ^ ^    ^      ^      ^    ^
| |  |  | |    |      |      |    \--- File Size
| |  |  | |    |      |      \-------- Group Name (for example, Users, Administrators, etc)
| |  |  | |    |      \--------------- Owner Acct
| |  |  | |    \---------------------- Link count (what constitutes a "link" here varies)
| |  |  | \--------------------------- Alternative Access (blank means none defined, anything else varies)
| \--\--\----------------------------- Read, Write and Special access modes for [U]ser, [G]roup, and [O]thers (everyone else)
\------------------------------------- File type flag
	
	type_descriptor types:-
	-rw-rw-r--.    	<= f: regular file
	drw-rw-r--. 	<= d: directory
	lrw-rw-r--. 	<= l: symbolic link
	crw-rw-r--. 	<= c: character devices
	brw-rw-r--. 	<= b: block devices	

	
### Filesystem type  
	$ df -T						<= disk free
	$ df -Th
	$ df -Tha
		Filesystem              Type         Size  Used Avail Use% Mounted on
		rootfs                  rootfs        18G  1.1G   17G   7% /

###########################################
### Add new hard drive or partition
###########################################

	1. 	$ fdisk -l
		$ ls /dev/sd*      (Ubuntu /dev/xvd*)
		
	2. 	$ fdisk /dev/sdb			<= Create new partitions
		$ command(m for help): p
			p	 print the partition table
		   Device Boot      Start         End      Blocks   Id  System

		$ command(m for help): n
			n   add a new partition
			Partition type:
			p   primary (0 primary, 0 extended, 4 free)
			e   extended
			Select (default p): p
			Partition number (1-4, default 1): 1
			
		$ command(m for help): p
			Device Boot      Start         End         Blocks   Id  System
			/dev/sdb1        2048        10485759     5241856   83  Linux
		
		$ Expert command (m for help): w
		The partition table has been altered!
		
		
		$ fdisk -l
	    Device Boot      Start         End      Blocks   Id  System
		/dev/sdb1            2048    10485759     5241856   83  Linux
		
		$ file -sL /dev/xvd*
		  /dev/xvda1: Linux rev 1.0 ext4 filesystem data  					<= ext4
		
	3. 	$ mkfs.ext3 /dev/sdb1
	    $ mkfs.ext4 /dev/xvdb                                               <= ext4 

	4. 	$ mount -t ext3 /dev/sdb1 /mnt/mysql -rw
	    $ mount -t ext4 /dev/xvdb /puppet/ -rw
		# $ umount /mnt/mysql

	$ cat /etc/fstab					<= check hdd list
	5. 	$ echo '/dev/sdb1  /mnt/mysql  ext3  defaults 0 0' >> /etc/fstab
	
	
	6. $ mount | column -t				<= Check mount list
	   $ cat /proc/mounts
	
	# Mount recovery disk
	$ fdisk -l
	$ mount /dev/xvdf /mnt/dir or /tmp/dir
	
	# Unmount
	$ umount /mnt/dir
	 -l, --lazy              detach the filesystem now, and cleanup all later


	# tmpfs    /dev/shm 		<=Shared Memory, it is a file system, which keeps all files in virtual memory
	https://www.cyberciti.biz/tips/what-is-devshm-and-its-practical-usage.html
	/dev/shm (filesystem-level shared memory) also known as 'tmpfs'
	
	
	
	
	$ df -h
	tmpfs           246M     0  246M   0% /dev/shm
	
	
	$ mount -t tmpfs -o size=5G,nr_inodes=5k,mode=700 tmpfs /disk2/tmpfs

	-o opt1,opt2 	   : Pass various options with a -o flag followed by a comma separated string of options. 
	remount 		   : Attempt to remount an already-mounted filesystem. In this example, remount the system and increase its size.
	size=8G or size=5G : Override default maximum size of the /dev/shm filesystem. he size is given in bytes, 
						and rounded up to entire pages. The default is half of the memory. The size parameter 
						also accepts a suffix % to limit this tmpfs instance to that percentage of your pysical RAM: 
						the default, when neither size nor nr_blocks is specified, is size=50%. In this example 
						it is set to 8GiB or 5GiB. The tmpfs mount options for sizing ( size, nr_blocks, and nr_inodes) 
						accept a suffix k, m or g for Ki, Mi, Gi (binary kilo, mega and giga) and can be changed on remount.
	nr_inodes=5k 	   : The maximum number of inodes for this instance. The default is half of the number of your physical 
						 RAM pages, or (on a machine with highmem) the number of lowmem RAM pages, whichever is the lower.
	mode=700 	       : Set initial permissions of the root directory.
	tmpfs 			   : Tmpfs is a file system which keeps all files in virtual memory.
	
	# Permanent
	$ vi /etc/fstab
	none      /dev/shm        tmpfs   defaults,size=8G        0 0
	 
	$ man mount 
	nodev 	<= Do not interpret character or block special devices on the file system. 
	noexec 	<= Do not allow execution of any binaries on the mounted file system.
	nosuid 	<= Do not allow set-user-identifier or set-group-identifier bits to take effect. 
	 
	 
	File descriptor
	https://en.wikipedia.org/wiki/File_descriptor
	
---------------------------------------------------------------------	
# lsof and fuser
---------------------------------------------------------------------

	# fuser - which processes using files or sockets
			- user owning the process and the type of access
			- displays the process id(PID) of every process using the specified files or file systems.
	https://www.digitalocean.com/community/tutorials/how-to-use-the-linux-fuser-command
	
	$ fuser -l
	
	$ fuser -v .	(current dir)<= gives information about the USER, PID, ACCESS and COMMAND
        USER        PID ACCESS COMMAND
		/home/apark/localVariable:
        apark     10233 ..c.. bash
        apark     26462 ..c.. bash
	
	$ fuser -v    /  (root dir)
		USER        PID ACCESS COMMAND
		/:  root     kernel mount /
        apark     10028 .r... bash
			............
        apark     26462 .r... bash

	$ fuser -vm   /								<= -v verbose, -m mount
	
	# show all processes having open files on the filesystem mounted on / (root directory)
	$ fuser -vm / 2>&1 | awk '$3 ~ /f|F/' | less -N 		<=?????

	### Network 
	$ nc -l -p 80									<= Netcat -l listen -p port 
	$ fuser -v -n tcp 80							<= -n name space
					USER        PID ACCESS COMMAND
		80/tcp:   	apark     20168 F.... nc

	### Kill
	$ fuser -k -i 80/tcp						<= -i ask before kill
	
	### Find The Process Accessing A File System
	$ fuser -v -m /var/log/dmesg						<= -v verbose, -m mount
	
					USER        PID ACCESS COMMAND
	/var/log/dmesg: root     kernel mount /                
                    root          1 .rce. systemd
                    root          2 .rc.. kthreadd
                    root          3 .rc.. ksoftirqd/0
					............
					nginx     19176 Frce. nginx
                    root      19295 .rce. sshd
                    apark     19310 .rce. sshd
					icinga    26921 Frce. icinga2
                    root      27046 F.... python
                    root      27784 .rc.. kworker/u8:1

	
	
	
	
-------------------------------------------------------------------------- 		
# lsof				<= 'li'st 'o'pen 'f'iles using by PID or Program
-------------------------------------------------------------------------- 	
	http://www.thegeekstuff.com/2012/08/lsof-command-examples
	
	$ lsof 				<= list all opened files start 'Init 1'

	$ lsof -p $$		<= $$ Processor ID
	
	$ lsof -u apark		<= List of all opened files by user

	# Which processes have this file open
	$ lsof  /var/log/nginx/access.log
		COMMAND   PID  USER    FD   TYPE DEVICE SIZE/OFF      NODE NAME
		nginx   13786  root    5w   REG  253,0        0 135700508 /var/log/nginx/access.log
	
	# list opened files under a directory
	$ lsof +D  /var/log  ==   $ lsof /var/log/
	
	# List opened files based on process names starting with
	$ lsof -c ssh
	
	# Which PID have opened files?
	$ ps aux | grep nginx |
	$ lsof -p nginx_PID							<= Process ID (PID)

	$ lsof -p 1									<= all files are opened with Init or SystemD
	$ lsof -p `pgrep systemd`
		
	# Where is the Binary for this process?
	$ lsof -p `pgrep ngnix` | grep bin
	
	# Which shared Libraries is this program using? (manually upgrading software, i.e. openssl)
	$ lsof -p PID | grep .so
	
	# Where is this thing logging to?
	$ lsof -p ABC | grep log
	
	# Which processes still have this old library open?
	$ lsof | grep libname.so
	
	# Which files does user_id have open?
	$ lsof -u user_id
	$ lsof -u user_id -i 					<= network only
	
	** # Kill all process that belongs to a particular user
	$ kill -9 `lsof -t -u user_name`
	
	# Network connection - Which process is listening on Port/Protocol
	$ lsof -i :80
	$ lsof -i tcp
	$ lsof -i tcp; lsof -i udp;
	
	
	# Finding which Process uses port 80
	$ lsof -i :80  
	COMMAND PID     USER            FD   TYPE DEVICE SIZE/OFF NODE NAME
	java    477   """logstash"""   79u  IPv6  22585      0t0  TCP *:lxi-evntsvc (LISTEN)

	$ lsof -Pni :3306
	
	/dev/mapper/U14--vg-root on / type ext4 (rw,errors=remount-ro)
	
	$ lsof -i:3306
		kill the running process with
		kill -9 PID

--------------------------------------------------------------------------	
# Kernel supports filesystem
--------------------------------------------------------------------------
	$ cat /proc/filesystems
		nodev   devpts
        ext3
        ext2
        ext4
		
	$ cat $(which sys-unconfig) 	<=newer / older=>  $ cat `which sys-unconfig`
	
	### sys-unconfig  <= shutdown the <SYSTEM>!!!
	
	
	$ df -h    <= report file system disk space usage (Check Disk Free)
	$ df -T   <= file system type
	$ df -i   <= inodes
		Filesystem                Inodes IUsed    IFree IUse% Mounted on
		/dev/mapper/centos-root 18358272 26054 18332218    1% /

	df -h | xargs | awk '{ print $11 " / " $9 " are free" }'

	df -h | grep /dev/mapper | awk '{print $4 "/" $2 "GB is free"}'

	df -h | awk '{print $5}' | grep % | grep -v Use |  sort -n | tail -1 | cut -d "%" -f1 -

 
  169  cat /proc/mounts
  
  170  ls /mnt
  
  171  mount -l
  
  172  history | grep mount
  
  173  ps aux | grep jarvis
  
  174  ls -la /
  
  175  ls -la /mnt/
  
  176  history
		$ 
		$ !ls
  
  
  
  
-------------------------------------------------------------------------- 
# pkexec - Execute a command as another user
# runuser - run a command with substitute user and group ID
--------------------------------------------------------------------------
 $ runuser -l node -c 'cat index.js'
			-l <-login user name,  -c <-command 

use it in script
#!/sbin/runuser username


				
Run as root		
$ su - -c "command arg1"	
$ su - root -c "command"
$ su - root -c "ls -l /root			

$ su - apark -c 'ulimit -aHS'

  
--------------------------------------------------------------------------  
# chmod 
--------------------------------------------------------------------------

https://en.wikipedia.org/wiki/Chmod
In "chmod aw" the "a" means "All" (everyone) and the "w" means "write" (edit) the file.

chmod + x
chmod + xu
chmod 755
	 -7 	owner
	  -5 	group
	   -5 	other

u = user tha t owns the file
g = group that owns the file
o = other (everyone else)
a = all (everybody)

r = 4   <= read aces to the file
w = 2	<= write access
x = 1	<= execute (run) access
  
### Numerical permissions
#	Permission	rwx
7	rwx		read, 	write, 	execute	
6	rw- 	read, 	write	
5	r-x		read, 	execute	
4	r--		read 		
3	-wx		write,	execute	 
2	-w-		write 		
1	--x 	execute 
0	---		none	
  
$ chmod a-x 	text.txt			<= -x 'REMOVES' Execute permission for all classes
$ chmod a+rx 	test.sh				<= adds read and execute permissions for all classes
$ chmod -R u+w,go-w  text.txt		<= adds write permission to the directory docs and all its contents 
						(i.e. Recursively) for owner, and removes write permission for group and others
$ chmod ug=rw groupAgreements.txt	<= sets read and write permissions for owner and group  
 -------------------------------------------------------------------------- 
  
  
  177  lsblk
  
  178  sudo mount /dev/xvdf /mnt
  179  ls -la /mnt/
  180  sudo vim /etc/fstab
  181  history | grep start
  182  sudo start jarvis-01 && sudo start jarvis-02 && sudo start jarvis-03 && sudo start jarvis-04
  183  less /mnt/log/jarvis-01/jarvis.log
  184  sudo service mongos start
  185  sudo stop jarvis-01 && sudo stop jarvis-02 && sudo stop jarvis-03 && sudo stop jarvis-04 && \ 
	   sudo start jarvis-01 && sudo start jarvis-02 && sudo start jarvis-03 && sudo start jarvis-04
  186  less /mnt/log/jarvis-01/jarvis.log
  187  ps aux | grep nginx
  188  curl 'http://localhost:53213/tcg/api/1/status'
  189  less /mnt/log/jarvis-01/jarvis.log
  190  exit
	
	


### Count and Check Active connections
	# ss			<= another utility to investigate sockets

	$ ss  		<= is  used to dump "Socket Statistics"(replacement of NETSTAT.)
				http://www.binarytides.com/linux-ss-command/
	$ ss -t 	<= tcp("established = connected)  
	$ ss -tna 	<= n <- not to resolve hostname, -a <- all
	$ ss -u 	<= udp
	$ ss -ltn
	
				
	$ netstat -na | grep ESTA | wc -l				<== n no reverse lookup(quick), t tcp, a all, p program
	$ netstat -na | grep ESTABLISHED.*sshd		<= sshd connection list
	$ ps auxwww | grep sshd:						<= sshd connection list
	$ netstat -plunt								<= which ports are open and which program(application) is listening
	$ netstat -punt								<= program, udp, no lookup, tcp
	$ netstat -ntl								<= no lookuop, tcp, LISTEN port

	$ watch -d -n1 "netstat -ntap | grep ESTA"  		<= Print active connections n1 <= every 1sec
	
	$ watch -n5 'netstat -ntap | grep ESTA | wc -l'	<= Print active connections n1 <= every 1sec
	
	$ watch -n 0.1 "dmesg | tail -n $((LINES-6))"
	
	$ while true; do dmesg -c ; sleep 1 ; done
--------------------------------------------------------------------------
# Infinite loop - while loop
--------------------------------------------------------------------------
https://unix.stackexchange.com/questions/42287/terminating-an-infinite-loop/121391#121391

>>>	$ while true; do ping google.com; done  	
	
	
	$ Ctrl +c 			<= hold down UNTIL '^C'
	$ Ctrl +z 			<= stop the job
	$ jobs
		[1]+  Stopped                 ping google.com
	$ kill %!
		[1]+  Terminated              ping google.com
	
	$ while [ 1 ]; do COMMAND || break; done;
	
	
--------------------------------------------------------------------------
# vmstat - report virtual memory statistics
--------------------------------------------------------------------------
	- vmstat reports information about processes, memory, paging, block IO, 
	  traps, disks and cpu activity.
	vmstat is a tool that collects and reports data about your system’s memory, 
	swap, and processor resource utilization in real time. It can be used to 
	determine the root cause of performance and issues related to memory use.
	$ vmstat [interval] [count]
	$ vmstat 1 20 	<= 1sec  20 times
	$ vmstat -S k 1 10
		In the default operation, vmstat displays memory statistics in kilobytes. 
		vmstat considers a single kilobyte equal to 1024 bytes. To generate vmstat 
		reports where 1 kilobyte is equal to 1000 bytes, use the following form:
	
	

	
--------------------------------------------------------------------------	
# Alias
--------------------------------------------------------------------------
	#only used in interactive shells and not in scripts!!
	https://mywiki.wooledge.org/BashGuide/CommandsAndArguments
	
	$ nmap -Pn -A --osscan-limit 192.168.0.1  	<= from shell
	
	$ alias nmapp="nmap -Pn -A --osscan-limit"	<= alias
	$ nmapp 192.168.0.1							<= usage
	
	$ cd; vi .bash_profile
	$ alias server1='ssh root@ip_address -p 3404'   <= add your alias in .bash_profile
	$ alias lp='ls -al ../'							<= ls -al parent directory  

--------------------------------------------------------------------------
# NTP
--------------------------------------------------------------------------

	alias today='date +"%A, %B %-d, %Y"'

	date  => Thu Dec  4 10:11:00 EST 2014
	today => Thursday, December 4, 2014
	date +"%m-%d-%Y" => 12-04-2014

	### CentOS7 ###
	$ timedatectl - Control the system time and date
 	   --adjust-system-clock
           If set-local-rtc is invoked and this option is passed, the system
           clock is synchronized from the RTC again, taking the new setting
           into account. Otherwise, the RTC is synchronized from the system
           clock.

	To correct zone and time
	$ systemctl restart  ntpd.service
	
	# TimeZone changes to PST 
	https://www.cyberciti.biz/faq/centos-linux-6-7-changing-timezone-command-line/
	$ date
	$ ls -l /etc/localtime
	$ timedatectl
	$ timedatectl list-timezones
	$ timedatectl list-timezones | grep -i los
		America/Los_Angeles
	$ timedatectl set-timezone America/Los_Angeles

	
	
--------------------------------------------------------------------------	
# cat 
# zcat 
--------------------------------------------------------------------------
	$ cat /usr/bin/bash XXXX
    
	$ zcat file.archived.tar.gz			<= reading gzip (view zip) file without extract first 
	
	$ strings /usr/bin/bash  <= human readable content buried inside the program.
 

--------------------------------------------------------------------------   
# uptime		<= uptime, number of users,        CPU load 1,   5,  15 mins
--------------------------------------------------------------------------
	09:35:45 up 10 days, 21:18,  2 users,  load average: 0.03, 0.03, 0.05
	
	### How many days it has been running
	$ uptime | awk '{print $3}'    | cut -f1 -d,
		10  <= 10days
		
	$ uptime | awk '{print $3,$4}' | cut -f1 -d,
		10 days	
   
### Script 	<= to TEXT file		
	# Record and Replay Linux Terminal Sessions
	https://www.tecmint.com/record-and-replay-linux-terminal-session-commands-using-script/ 
	$ script userInfo.log
		$ w
		$ whoami
		$ exit		<= when the recording is done
	
	$ cat userInfo.log				<= view
	$ vi  userInfo.log				<= edit
	
	# Append(adding more session)
	$ script -a userInfo.log		<= -a append
	
	### Scriptreplay  <= Movie file ###
	$ script 	   --timing=timeline.txt movieRecording.log
	$ scriptreplay --timing=timeline.txt movieRecording.log
--------------------------------------------------------------------------	
# Services Listing 
--------------------------------------------------------------------------
	$ /usr/sbin/service --status-all
	$ /sbin/service --status-all
	$ service --status-all

--------------------------------------------------------------------------	
# mkdir
--------------------------------------------------------------------------
	$ mkdir -p production/{modules,manifests}      <= creating multi SUB folders
	
	mkdir and cd into directory
	$ mkdir ~/docker-registry && cd $_	

	$_	
	echo "$_"
	echo "test"
	echo "$_"
	
------------------------------------------------------------------------------------
# Echo 
------------------------------------------------------------------------------------
	# echo -n opntion
	$ echo -n Hello World\$			<= -n no return line
		Hello World$apark@i7~$		<= \$ ignore special char   
	
	$ echo -n "Hello World\n\n"
		Hello World\n\napark@i7~$   <= -n NO return line
									<= \n\n prints as string
	# echo -e option	
	$ echo -e "Hello World\n\n"		<= -e enable interpretation of backslash escapes
		Hello World
		empty line					<= \n prints a new empty line 
		empty line					<= \n prints a new empty line 

	$ echo ~						<= prints user home directory
	$ echo ~root					<= prints root home directory
	$ echo ~+  == pwd
	$ echo ~-  == cd -
	$ echo s{pe,pi}ll				<= don't use quote ""
		spell spill
		
	$ echo 1/3 | bc -l                <= .3333333
	
	
	# Long string variable
		
	$echo "test"\
	> "test"\			<= NO Space in front
	> "test"\			<= NO Space in front
	>
	=> testtesttest		<= NO Space between test

$ echo 'export FORMAT="\nID\t{{.ID}}\nIMAGE\t{{.Image}}\nCOMMAND\
\t{{.Command}}\nCREATED\t{{.RunningFor}}\								<= NO Space in front
\nSTATUS\t{{.Status}}\PORTS\t{{.Ports}}\								<= NO Space in front
\nNAME\t{{.Names}}\n"' >> ~/.bashrc && source ~/.bashrc


------------------------------------------------------------------------------------
# Echo server using '|' pipe
------------------------------------------------------------------------------------
# echo forwarding server
$ nc -lp 8888| nc -lp 9999

# 2nd Terminal
$ nc localhost 8888
	This is 8888 Echo msg		<= echo this msg to echo server

# 3rd Terminal 
$ nc localhost 9999
	This is 9999 Echo msg		<= Echo forward this 3rd terminal 



	
------------------------------------------------------------------------------------	
# double & single quotes	
------------------------------------------------------------------------------------
    $ echo 'Single quotes "protects" the double quotes.'
        Single quotes "protect" double quotes.
		  
	$ echo "My name is $USER"			<= apark  variable
	$ echo 'My name is $USER'			<= $USER  same
	
	$ pdir="/tmp/files/today"
	$ fname="report"
	$ mkdir -p $pdir/$fname
	$ ls $pdir
		report
	$ touch $pdir/$fname_jan		<= Not working
	$ touch $pdir/${fname}_jan		<= user {} to protect the variable $fname
	$ ls $pdir/$	/tmp/files/today/report_jan

	$ echo  My name is $(whoami)	
	$ echo "My name is $(whoami)"
	$ echo  My name is `whoami`
	$ echo "My name is `whoami`"
	$ chown -R $(whoami) /home/$(whoami)/test

--------------------------------------------------------------------------	
# Nested command substitution
--------------------------------------------------------------------------
	$ echo " Permissions for find are $(ls -l $(which find))"
		4th			3rd					2nd			1st(/bin/find)
		Permissions for find are -rwxr-xr-x. 1 root root 199200 Nov 20  2015 /usr/bin/find
	
	$ HOSTNAME=hostname							<=variable HOSTNAME into command hostname
	$ echo "PC name is $HOSTNAME." 				<= PC name is PC.
	$ echo "PC name is $(hostname)."			<= PC name is PC.
	$ echo "PC name is $(HOSTNAME)."			<= PC name is PC.
	
	---------------------------
	$ HOSTNAME=$($(which hostname))
	$ which hostname
	  /bin/hostname
	$ echo $HOSTNAME
	  mc1
	---------------------------
	
--------------------------------------------------------------------------
# \ ingnore special character
--------------------------------------------------------------------------
    $ echo "PC name is \"($HOSTNAME)\"."		<= \ ignores " as a string 
    


    $ echo "Well, isn’t that \"special\"?"
    $ Well, isn’t that "special"?

    $ echo "You have `ls | wc -l` files in `pwd`"
		You have 43 files in /home/bob
 
    $ x=100
	$ echo "The value of \$x is $x"
        The value of $x is 100

###
	# echo -e    						 <= Enables interpretation of backslash escapes
	$ echo -e "Inserting blank\n\n\n" 					<= \n  adding blank newline lines to text
		
	$ echo -e "worlds\tseperate\tby\ttabs."				<= \t  inserting tabs
	
	$ echo -e "\aMy computer \\went \"beep\"." 			<= \a  alert	makes your terminal beep										
														<= \\	backslash	insert a backslash
	
	
	
	
	
	
### Path substitution ### 
	
### Pushd						<= add directory
	Add directories to stack.
    Adds a directory to the top of the directory stack, or rotates
    the stack, making the new top of the stack the current working
    directory.  With no arguments, exchanges the top two directories.
	
### dirs						<= list directories
	Dispay directory stack
	Display the list of currently remembered directories.  Directories
    find their way onto the list with the `pushd' command; you can get
    back up through the list with the `popd' command.

### popd						<= remove a directory
	are shell builtins which allow you manipulate the directory stack. 
	This can be used to change directories but return to the directory from which you came.
	https://unix.stackexchange.com/questions/77077/how-do-i-use-pushd-and-popd-commands
	
	$ pushd /var/log/httpd
	$ pushd /var/www/html
	$ dirs
	  /var/log/httpd  /var/www/html
	$ echo ~0
	  /var/log/httpd
	$ echo ~1
	  /var/www/html
	$ popd +1
	$ dirs
	  /var/log/httpd
	
	
	
--------------------------------------------------------------------------	
# brctl <= Network Bridge						
--------------------------------------------------------------------------	
# Check & Install  Bridge
https://www.itzgeek.com/how-tos/mini-howtos/create-a-network-bridge-on-centos-7-rhel-7.html
$ modinfo bridge
# Enable 
$ modprobe --first-time bridge
 
$ yum install bridge-utils -y
$ brctl show	
	
	
-------------------------------------------------------------------------
# $ last						<= show listing of last logged in users
--------------------------------------------------------------------------

# Check Last time reboot
	$ last reboot
	$ who -b

# $ last reboot
	reboot system boot 3.2.13-grsec-xxx Tue Apr 3 07:34 - 09:17 (9     + 01 : 42) 
																 ^9days 1hr 42mins after restart
																 
																 $ last reboot
	reboot   system boot  3.10.0-862.11.6. Tue Sep 18 12:24 - 16:00 (363+03:36)	
							     ^ Kerenl's version has been updated
	reboot   system boot  3.10.0-862.2.3.e Tue Sep 18 12:19 - 16:00 (363+03:40)
									^ Old Kerenl version

	$ last						<= last, lastb - show listing of last logged in users
	$ last -x
	$ last -x reboot
	$ last -x shutdown
	$ lastlog					<= reports the most recent login of all users 
    
	$ who -b					<=  time of last system boot
	$ who
	$ who -r 					<= runlevel
	$ who -b
	
--------------------------------------------------------------------------
# shutdown 
--------------------------------------------------------------------------
https://www.computerhope.com/unix/ushutdow.htm
	halt 
	poweroff 
	reboot 
	shutdown
	
	$ shutdown -h 10 wall "Shutdown in 10 mins"				<= Shutdown immediately
	$ shutdown -h 0				<= Shutdown immediately
	$ shutdown now				<= Shutdown immediately
	$ shutdown -P now			<= Shutdown immediately
	$ shutdown -r now			<= Reboot immediately.
	
	$ reboot					<= Reboot immediately.
	
	$ echo "reboot" | at 0000 jun 27 | wall "Rebooting"
	
	$ shutdown -r 10 wall "Reudo sboot in 10 mins"			<= 1 for 60mins
		Shutdown scheduled for Tue 2017-12-05 11:08:28 PST, use 'shutdown -c' to cancel.
		Broadcast message from root@i7 (Tue 2017-12-05 10:58:28 PST):
				wall Reboot in 10 mins
		The system is going down for reboot at Tue 2017-12-05 11:08:28 PST!

	$ poweroff	

--------------------------------------------------------------------------		
# Reboot Check
--------------------------------------------------------------------------
	$ last reboot			<= check last reboot time
	### CANCEL SHUTDOWN SCHEDULE ###
	$ shutdown -c
	### Time count issue for Shutdown command
	# 1 <= 1 hour, 30 
	$ sleep 30s; shutdown -r now			<= reboot after 30sec
	$ shutdown -P +60						<= shutdown after 60 mins
	$ shutdown -P 1:00						<= shutdown at 1AM
	$ 
	
	
---------------------------------------------------------------------------------------	
# .bash_profile is a file???? 
--------------------------------------------------------------------------
	if [ -f .bash_profile ] ; then 					<= -f file is exist & regular file
		echo "you have the file in your home dir"		
    	else 
		echo "You do not have it"
	fi
---------------------------------------------------------------------------------------
# id -u        (-u <= --user for script)   
--------------------------------------------------------------------------
	print real and effective user and group ID
	uid=0(root) gid=0(root) groups=0(root) 				context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

---------------------------------------------------------------------------------------
### rm -rf ./tmp/test/delme/*    <=delete folder contents
    
	# deleting . file(hidden file)
	rm -rf .*
	rm -rfi .*
	
	## search all files bigger than 100MB
	# use ncdu utility
	
	$ find /  -type f -size +100M   					<= k-kilo, M-mega, G-giga

	$ find / -name "*.txt" | xargs rm -rf				<= search all and delete them
	
	$ find / -name “*.txt” | xargs grep “Tecmint”		<= search and search string "Techmint"
	
    ###!!! Danger !!!###
    rm -fr ./tmp/test/delme/ *                <=delete whole root folder(./tmp/test/delme)   

	$ find . -type f -name "*.bak" -exec rm -i {} \;
	
	$ find . -type f -name "FILE-TO-FIND" -exec rm -f {} \;
	
		-name "FILE-TO-FIND" : File pattern.
		-exec rm -rf {} \; : Delete all files matched by file pattern.
		-type f : Only match files and do not include directory names.
		
---------------------------------------------------------------------------------------	
# Delete local backup folder older than 2 days	
---------------------------------------------------------------------------------------	
	$ sudo find /backup/ -mindepth 1 -mtime +2 -delete

	$ sudo find /data/backups/ -type f -mtime +0 -name 'rrs_us_db.sql.*' -delete   	
				<= mtime (days), mmin (mins)
	
	-mtime   
		Day 0~1, 2, 3, 4, 5
		+2 <= 3,4,5 older than 2days, -2 <= 1,2 less than 2days
		
	ctime
		ctime is the inode or file change time. The ctime gets updated when the file attributes are changed, 
		like changing the owner, changing the permission or moving the file to an other filesystem but will 
		also be updated when you modify a file.

	mtime
		mtime is the file modify time. The mtime gets updated when you modify a file. Whenever you update 
		content of a file or save a file the mtime gets updated. 
		
		Most of the times ctime and mtime will be the same, unless only the file attributes are updated. 
		In that case only the ctime gets updated.

	atime
		atime is the file access time. The atime gets updated when you open a file but also when a file is 
		used for other operations like grep, sort, cat, head, tail and so on.

 
---------------------------------------------------------------------------------------	
# touch 
---------------------------------------------------------------------------------------	
	touch file_{1..100}    <= creating 100 files e.g. 1,2..100
    touch file_{01..100}   <= to resolve file format problem above logic e.g. 001, 002..100
    touch {1,2,3,4,5}	   <= creating 1,2,3,4,5 files
    touch ./ls/{1..100}    <= creating files under ./ls/ folder
    touch ./tmp/test/delme/file_{01..100}
  
    echo {1..10}            	 <= print out 1,2,..10
    echo {1..10..2}	   			 <= Prints out 1 3 5 7 9
    echo {1..10..3}        		 <= Prints out 1 4 7 10
    echo {A..Z}              
    echo {A..z}		   			 <= Capital first    
    echo {w..d..2}	    		 <= Reverse order for every 2nd alphabet

    touch {apple,banana,cherry,durian}_{01..100}{1..10} 
    ls -1 | wc -l    	   		 <= 4000 result

    Prints special character without errors	
    echo '"! # $ % & '\'' ( ) * + , - . / : ; & < = > ? @ [ \ ] ^ _ { | } ~"'
    echo "! # $ % & '\'' ( ) * + , - . / : ; & < = > ? @ [ \ ] ^ _ { | } ~"
	
---------------------------------------------------------------------------------------
# bash version
---------------------------------------------------------------------------------------
	$ bash -version
	$ echo $BASH_VERSION
	
	$ echo $MACHTYPE
	$ echo $SECONDS 					<= Count script started 
	
	$ echo $SHELL						<= /bin/bash
	$ env | grep SHELL					<= SHELL=/bin/bash
	
	$ ps					 
	  1453 pts/1    00:00:00 bash  					<= BASH
	$ ps | grep `echo $$` | awk '{ print $4 }'		<= BASH

	
---------------------------------------------------------------------------------------	
#	IP address 						<= finding IP from ifconfig using grep re  gex
---------------------------------------------------------------------------------------	
	$ ifconfig | grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b"
	 192.168.232.132

	$ ip neighbour show			<= how your subnet's computers

	$ ip addr show eth0 | grep inet | awk '{ print $2; }' | sed 's/\/.*$//'
	$ ip addr show eth0 | grep inet | awk '{ print $2; }' | sed 's/\/.*$//' | head -n 1
	
---------------------------------------------------------------------------------------		
# tail
---------------------------------------------------------------------------------------	
  tail -f *Slave.log
  tail -f *vSlave.log | grep "Info: Slave - Sending status"	



25. a="hello"
	echo ${#a}   <= 5 number for character
	
---------------------------------------------------------------------------------------		
# Symbolic vs Hard link (softlink vs hardlink)
https://blog.usejournal.com/what-is-the-difference-between-a-hard-link-and-a-symbolic-link-8c0493041b62
https://medium.com/@307/hard-links-and-symbolic-links-a-comparison-7f2b56864cdd
---------------------------------------------------------------------------------------	



---------------------------------------------------------------------------------------	
# Printf vs echo (http://wiki.bash-hackers.org/commands/builtin/printf)
---------------------------------------------------------------------------------------	
	$ printf Hello World			Hello					<= Prints only first argument
	$ printf "Hello World"			Hello World				<= Prints both, but No New Line
		hello worldapark@i7~/								<= NO NEW LINE
	$ printf '%s\n' "Hello World"							<= Prints both, '%s\n' put a New Line
		Hello World											
	$ eho "Hellow World"
	
	$ printf '%s\n' "$var"				<= %s	Interprets the associated argument literally as String
										<= %n	a new line
	-------------------------------------------------------------------------												
	$ echo "\n\t"
		-n     do not output the trailing newline
		-t     do not output the trailing horizontal
	
	$ echo -e "Hello\nworld"			<= -e  enable interpretation of backslash escapes
	$ echo -e 'Hello\nworld'			<= \n  a new line
	$ echo Hello$'\n'world
	  hello
	  World

---------------------------------------------------------------------------------------		
# %s 	<= a format specifier for printf command.
---------------------------------------------------------------------------------------	

	https://www.quora.com/What-does-s-mean-in-Bash
	%s <= In BASH: Using the format string %s causes the arguments to be concatenated without 
		intervening spaces. It interprets the associated argument literally as s string.

	$ printf "Name:\t%s\nID:\t%04d\n" "Albert" "12" 	<= \t tab, %s string,\n new line,%04d 4digit
	  Name:   Albert
	  ID:     0012
	-----------------------------------------------
	#!/bin/bash
	today=$(date +%F)
	time=$(date +%T)
	printf -v d "\tCurrent User:\t%s\n\tDate:\t\t%s at(@) %s\n"  $USER $today $time
	echo "$d"
-----------------------------------------------
	Current User:   apark
	Date:           2017-01-18 at(@) 17:02:28

	$ printf "My name is \"%s\".\nIt's a pleasure to meet you."  "Albert"
	My name is "Albert".
	It's a pleasure to meet you.
	
	$ echo $USER
	$ echo $LOGNAME	




---------------------------------------------------------------------------------------	
# Empty logs or file contents  
---------------------------------------------------------------------------------------	
						Bourne POSIX  zsh    csh/tcsh  rc/es  fish
	> file                Y      Y      N(1)   N(1)      N      N
	: > file              N/Y(2) Y(3)   Y      Y(4)      N(5)   N(5)
	true > file           Y(5)   Y      Y      Y(5)      Y(5)   Y(5)
	cat /dev/null > file  Y(5)   Y      Y(5)   Y(5)      Y(5)   Y(5)
	cp /dev/null file (7) Y(5)   Y      Y(5)   Y(5)      Y(5)   Y(5)
	printf '' > file      Y(5)   Y      Y      Y(5)      Y(5)   Y
	eval > file           Y(3,8) Y(3)   Y      Y(6)      Y      Y
	
---------------------------------------------------------------------------------------	
#   Eval  
---------------------------------------------------------------------------------------	
	is part of POSIX. Its an interface which can be a shell built-in.

	https://unix.stackexchange.com/questions/23111/what-is-the-eval-command-in-bash
	Man Page: The args are read and concatenated together into a single command.   This  command  is
    then  read  and executed by the shell, and its exit status is returned as the value of
    eval.  If there are no args, or only null arguments, eval returns 0.
	
	Example-1
	1) foo=10 x=foo		<= define $foo with the value '10' and $x with the value 'foo'.
	2) y='$'$x			<= define $y, which consists of the string '$foo' / ecapted with '$'
	3) echo $y
	4) 	$foo			<= check $y value
	5) eval y='$'$x		<= evaluate $x to the string 'foo'. y=$foo which will get evaluated to y=10
	6) echo $y
	7) 10				<= The result of echo $y is now the value '10'
	
	Example-2
	$ A="ls | less"		<= two cmds to concatenated	
	$ $A 			
	  -bash: ls|less: command not found
	$ eval $A			<= use eval to execute the variable $a

------------------------------------------------------------	
### let  <= Simple arithmatic evaluation    
------------------------------------------------------------	
	https://askubuntu.com/questions/939294/difference-between-let-expr-and
	
	$ let 			<= Arithmatic Expansiion
	$ expr 			<= Evaluate Expression
	$ ((1+1))			<= Arithmatic Expansiion
	$ bc			<- An arbitrary precision calculator language
		$ [1+1]		<= Using [ ] is *Deprecated* Arithmatic Expansiion
	
	
	$ let arg [arg ...]  same as (( )) <- ARITHMETIC EVALUATION
        Each  arg  is an arithmetic expression let == (( )) to be evaluated.
        If the last arg evaluates to 0, let returns 1; 0 is returned otherwise.
	
	e.g.  using 'let'
	$ numA=4 numB=5
	$ let result=numA+numB
	$ echo $result
	  9
	  
	$ let r=1024*8
	$ echo $r
	  8192
 
	 
	$ let numA++ 
	$ let numB-- 
	
	$ result=numA+numB
	$ result=$[ numA + numB ]
	$ result=(( numA + numB ))
	$ echo $result
	  9
	  
	$ result=`expr 3 + 4`				<= If no spze 3+4 out put will be 3+4, not 7
	$ result=$(expr numA + 4)			<= If no spze no1 +4 out put will Syntax error!
	

	$ no1=4
	
	# BC   - An arbitrary precision calculator language
		$ yum install bc -y
	
	$ echo "4 * 0.56" | bc
	$ result=`echo "$no1 * 1.5" | bc`	<= BC for decimal
	
	# Bash only works with Integers, not decimals
	$a=$(echo 1/3 |bc -l)						<= user BC An arbitrary precision calculator language
												-l, --mathlib
												Define the standard math library.
	$ echo $a
		.33333333
	
	
	$ echo $result
	  6.0

	Decimal => binary => Octal  
	
	$ result2=`expr $no1 \* 2`			<= EXPR Only for Integer and use '\' for '*' multiply
	
	
	
	
#####  Number Operations 산수 계산 #####
# Arithmetic (Numbers) Operations	
	value=$(( $a expression $b ))
-----------------------------------------
	Operation				Operator
-----------------------------------------
	Exponentiation			(( $a ** $b ))
	Multiplication			(( $a * $b ))
	Division				(( $a / $b ))
	Modulo					(( $a % $b ))
	Addition				(( $a + $b ))
	Subtraction				(( $a - $b ))
-----------------------------------------
	$ echo $(( 10 * 10 ))	<= 100



##### Comparing String Values #####  	
# Test Statement [ ... ] , [[ ... ]]
# Comparison Operations (STRING)
	
-----------------------------------------------
	Operation						Operator
-----------------------------------------------
	Less than					[[ $a < $b ]]
	Greater than				[[ $a > $b ]]
	Less than or equal to 		[[ $a <= $b ]]
	Greater than or equal to	[[ $a >= $b ]]
	Equal(single or double)		[[ $a == $b ]] or [[ $a = $b ]]
	Not Equal 					[[ $a != $b ]]
-----------------------------------------------	
	[[  expressions  ]]  	0: True    1: False

# Equal
$ [[ cat == cat ]]  or	
  [[ cat = cat ]]  or 
   [ cat = cat ] or 
   [ cat=cat ]
$ [[ 'cat' == 'cat']]  or	
  [[ 'cat' = 'cat' ]] or 
   [ 'cat' = 'cat' ] or 
  [[ 'cat'='cat' ]] 
$ [[ "cat" == "cat" ]]
	^space		   ^space !MUST!

	$ [[ 20 > 100 ]]
	$ echo $? 
		0 <= true because it compares STRINGS, not the number! 
			20 is higher than 100 lexcially.

##### Comparing Numbers using [[ ... ]] Test Statement 번호 비교 #####
##### Comparison Operations (INTEGER) #####
-----------------------------------------------
	Operation						Operator
-----------------------------------------------
	Less than					[[ $a -lt $b ]]
	Greater than				[[ $a -gt $b ]]
	Less than or equal to 		[[ $a -le $b ]]
	Greater than or equal to	[[ $a -ge $b ]]
	Equal(single or double)		[[ $a -eq $b ]] 
	Not Equal 					[[ $a -nq $b ]]
-----------------------------------------------
$ if [[ 1 -lt 2 ]]; then echo "1 is less than 2"; else echo "1 is NOT less than 2"; fi


# Logic Operations
-----------------------------------------------
	Operation						Operator
-----------------------------------------------
	AND							[[ $a && $b ]]
	OR							[[ $a || $b ]]
	NOT 						[[ ! $a ]]
-----------------------------------------------


# String NULL(EMPTY) value Check
-----------------------------------------------
	Operation						Operator
-----------------------------------------------
	Is Null(Empty)?					[[ -z  $a ]]
	Is NOT Null(Empty)?				[[ !-z $a ]]

	Is NOT Null(Empty)?				[[ -n  $a ]]
	Is Null(Empty)?					[[ !-n $a ]]
-----------------------------------------------

	e.g.
	$ a=""
	$ b="cat"

	$ if [[ -z $a ]]; then echo 'Var $a is empty'; else echo 'Var $a is not empty';  fi
	$ if [[ -n $b ]]; then echo 'Var $b is NOT empty'; else echo 'Var $b is empty';  fi
	
	# Is $a Empty?
	$ [[ -z $a ]]		<= Is $a Empty?
	$ echo $?      		<= 0 true(Yes Empty)
	
	# Is $b Empty?
	$ [[ -n $b ]]		<= Is $b Not Empty?
	$ echo $?  	 		<= 0 true(Yes, Not empty) 


	# [ ]  single test statement "test - check file types and compare values"
	# [[ ]] 		<= bash 2.2 extended test


# Working with string & Reverse
	$ a="hello"
	$ b="world"
	$ c=$a$b
	$ d="$b $a"				 <= Reverse order
	$ echo $c $d			 <=	helloworld world hello

# how long is the string for variable $a and $c
	$ echo ${#a}
	  5
	$ echo ${#c}
	  10

# sub string or piece
d=${c:3}			<= starting at the 3rd character
loworld				<= $c=helloworld   h=0, e=1, l=2, l=3
d=${c:3:4}			<= starting at the 3rd character to next 4 character
echo $d 
lowo
d=${c: -4}			<= count from end of string
orld
d=${c: -4:3}		<= 3 of last 4
ord






	
	
	
# df .
  df / 

	df -h / | grep -E "\/$" | awk '{print $4}'
	df -h / | xargs | awk '{print $11}'      	
	
	fdisk -l				
	
	sfdisk -l -uM				<=partition table manipulator for Linux
	
	cfdisk /dev/sda1  			<= a linux partition editor with an interactive user interface based on ncurses
	
	parted -l  <= list out partitions 
	
	df -h | gep ^/dev
	
	$ lsblk  				<= Lists out all the storage blocks
		NAME  MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
		xvda1 202:1    0  12G  0 disk /

	$ blkid 			 <= prints the block device(partitions and storage media) 
							attributes like uuid and file system type.
	




36. 







40. Clean Up Logs
	
    $ cat > /var/log/log_file
    $ cat /dev/null > /var/log/messages
	$ cat /dev/null > /var/log/wtmp			<= utmp, wtmp - login records
  
	echo "Log files cleaned up."


# Login and Logout Info
	# Last 
	http://www.softpanorama.org/Utilities/last.shtml	
	=> searches 'Last Login List' through the file /var/log/wtmp 
				(or the file designated by the -f(file) flag)
	
	$ last <=same as => last -f /var/run/wtmp 
					<= default location without using -f(file) /var/log/wtmp 			
	$ last -a     	<= Display  the  hostname  in  the last column	
	
	$ last -F     	<= Print -F(full) login and logout times and dates.
	
	$ last -f /var/run/wtmp     <= /var/run/utmp for different location using '-f' file flag
	$ last -f /var/log/btmp		<= Failed Logins
 
	
	# wtmp <= historical data of utmp.	
	# btmp <= records only failed login attempts.	

	## utmp <= complete picture of users logins at which terminals, logouts, system events 
				and current status of the system, system boot time (used by uptime) etc.

# Find out failed login
	$ grep "Failed password" /var/log/auth.log
	$ egrep "Failed|Failure" /var/log/auth.log
	$ egrep "Failed|Failure" /var/log/secure
	$ cat /var/log/auth.log | grep "Failed password"

	$ journalctl _SYSTEMD_UNIT=ssh.service  | egrep "Failed|Failure"
	$ journalctl _SYSTEMD_UNIT=sshd.service | egrep "Failed|Failure"  #In RHEL, CentOS 
	$ journalctl _SYSTEMD_UNIT=sshd.service | grep  "failure"
	$ journalctl _SYSTEMD_UNIT=sshd.service | grep  "Failed"	

41.	Watch command to real time monitoring

	$ watch lsof -i			<= Watching who are connecting to a system and disconnecting, every 2sec
  

  
	$ watch -n 1 "ps aux | grep app_name"
<<<<<<< Updated upstream

=======
>>>>>>> Stashed changes

### encryption and decryption
	$ openssl passwd -1 your_password  
	  $1$nd1lHN8J$ehlK62ZgzrZDBq6ZHPT/   <= hashed value
	
	
	$ mkpasswd -l 15 -d 3 -C 5

	$ pwgen -s -1



	
42. strace - trace(monitor) system calls and signals of a program
	$ strace foobar.sh  	<= debugging a command/script   
	$ strace -p PID
	
	1. Trace the Execution of an Executable
		$ strace ls
		execve("/bin/ls", ["ls"], [/* 21 vars */]) = 0
		................
	2. Trace a Specific System Calls in an Executable Using Option -e
		$strace -e open ls
		open("/etc/ld.so.cache", O_RDONLY)      = 3
		.............
	#only the open system call of the ls command
		$ strace -e trace=open,read ls /home
	3. Save the Trace Execution to a File Using Option -o
		$ strace -o output.txt ls
	4. Execute Strace on a Running Linux Process Using Option -p
		$ strace -p PID
		
	5. Print Timestamp for Each Trace Output Line Using Option -t
		$ strace -t -e open ls /home
		11:59:25 open("/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
		11:59:25 open("/lib64/libselinux.so.1", O_RDONLY|O_CLOEXEC) = 3

	6. Print Relative Time for System Calls Using Option -r
		$ strace -r ls 
	7. Generate Statistics Report of System Calls Using Option -c
		$ strace -c ls /home
		noza  nozatech
		% time     seconds  usecs/call     calls    errors syscall
		------ ----------- ----------- --------- --------- ----------------
		25.48    0.000859          29        30           mmap
		14.12    0.000476          24        20           mprotect
	8. 

# Network Troubleshooting
	# mtr(Mutli function TRracert) 
	
	https://www.linode.com/docs/networking/diagnostics/diagnosing-network-issues-with-mtr
	combines  the  functionality  of the traceroute and ping in a single network diagnostic tool.
	
	# Generating report
	$ mtr google.com 				<= realtime
	$ mtr -rw google.com			<= report <- default 10 report
    $ mtr -rw -c 5 google.com       <= -r report, -w wide report, -c count 5 instead of 10 by default
	
	Start: Thu Dec  1 13:55:32 2016
	HOST: cos7    Loss%   Snt   Last   Avg  Best  Wrst StDev  <= standard deviation of the latencies to each host
	1.|-- gateway  0.0%     5    0.1   0.1   0.1   0.2   0.0
	2.|-- i7       0.0%     5    0.3   0.3   0.3   0.4   0.0

	$ mtr --no-dns -rw -c 5 google.com

	# traceroute
	$ traceroute -n -w 3 -q 1 -N 32 -m 16 google.com
		-n         : Disable DNS lookup to speed up queries.
		-w seconds : Set the time (in seconds) to wait for a response to a probe (default 5.0 sec).
		-q  NUMBER : Sets the number of probe packets per hop. The default is 3.
		-m  max hop:
	
       -o fields order
       --order fields order
              Use this option to specify the fields and their order when loading mtr.
                                                     ┌──┬─────────────────────┐
                                                     │L │ Loss ratio          │
                                                     ├──┼─────────────────────┤
                                                     │D │ Dropped packets     │
                                                     ├──┼─────────────────────┤
                                                     │R │ Received packets    │
                                                     ├──┼─────────────────────┤
                                                     │S │ Sent Packets        │
                                                     ├──┼─────────────────────┤
                                                     │N │ Newest RTT(ms)      │
                                                     ├──┼─────────────────────┤
                                                     │B │ Min/Best RTT(ms)    │
                                                     ├──┼─────────────────────┤
                                                     │A │ Average RTT(ms)     │
                                                     ├──┼─────────────────────┤
                                                     │W │ Max/Worst RTT(ms)   │
                                                     ├──┼─────────────────────┤
                                                     │V │ Standard Deviation  │
                                                     ├──┼─────────────────────┤
                                                     │G │ Geometric Mean      │
                                                     ├──┼─────────────────────┤
                                                     │J │ Current Jitter      │
                                                     ├──┼─────────────────────┤
                                                     │M │ Jitter Mean/Avg.    │
                                                     ├──┼─────────────────────┤
                                                     │X │ Worst Jitter        │
                                                     ├──┼─────────────────────┤
                                                     │I │ Interarrival Jitter │
                                                     └──┴─────────────────────┘
    $ mtr -o "LSD NBAW" 	google.com

	
	* * *
	The firewall to block ICMP packets. As such, you will sometimes see a series of asterisks indicating 
	that trace route was not able to get information from a particular host.
	
	
	# Ping
	Windows Ping TTL is 128
	RedHat	Ping TTL is 64
	Other linux < 128
	
	$ ping -i 4   127.0.0.1    	<= send out ping packets every 4 sec
	$ ping -i 0.1 127.0.0.1    	<= send out ping packets every 0.1 sec
	
	# ping localhost
	$ ping localhost
	$ ping 127.0.0.1
	$ ping 127.0.0.2~254
	$ ping 0.0.0.0
	$ ping 0					<= 
	
	# flood localhost network
	$ ping -f 0
	
	# change the packet size which is by default 64 bytes
	$ ping -s 200   0			<= 0 localhost
	
	# how to specify time out
	
	$ ping -W 10     0			<= 0 localhost 	
	
	# ping every 10sec, ping localhost
	$ ping -i 10 0			<= ping 0(localhost) every 10sec 
	
	
	# IP Address, Gateway, Route
	$ ip address 		<= ip a, ip add, ip addr 	<- all woorks
	
	# IP Gateway
	$ ip route			<= ip r, ip rou, ip rout	<- all works
	
	
	
	
<<<<<<< Updated upstream
43. stty 		   <= change and print terminal line settings
	$ stty -echo   <= hide(turn off) terminal typing(Can't see typing)
    $ stty echo    <= show(restore) terminal typing(able to see typing)
    $ TTY  		   <= TeleTYpe


	
### strace - trace system calls and signals	
	$ strace foobar.sh  	<= debugging a command/script   
	




>>>>>>> Stashed changes

### stat 				<= detailed and statistic on a file (ls -l foo.bar)
	
	$ stat foo.bar 
		File: ‘foo.bar’
		Size: 354             Blocks: 8          IO Block: 4096   regular file
		Device: fc00h/64512d    Inode: 1046543     Links: 1
		Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
		Access: 2016-04-08 21:16:25.603244615 -0700
		Modify: 2016-04-08 21:16:45.355245237 -0700
		Change: 2016-04-08 21:16:45.355245237 -0700
		Birth: -


### lsmod = Show the status of modules in the Linux Kernel
			same as  "cat /proc/modules"

	
	
##	mrmod					 <= rmmod - Simple program to remove a module from the Linux Kernel
	Error msg  "blk_update_request: I/O error, dev fd0, sector 0 errors"
	http://unix.stackexchange.com/questions/282845/blk-update-request-i-o-error-dev-fd0-sector-0
	$ rmmod floppy		
	$ echo "blacklist floppy" | sudo tee /etc/modprobe.d/blacklist-floppy.conf
	
	
	more info:
	fstab(5), 
	findfs(8), 
	mount(8) 
	blkid(8) 

	
	


45. Disabling User account
	#1. Change login passwd in /etc/passwd
	apark:x:503:504::/home/apark:/bin/bash
	apark:* or !:503:504::/home/apark:/bin/bash   <= X to '*' or '!'  
		note: will require you to assign a new password to the user if you re-enable the account,
	#2.
	apark:x:503:504::/home/apark:/bin/bash/nologin    <= add /nologin at the end
	apark:x:503:504::/home/apark:/bin/nologin
	apark:x:503:504::/home/apark:/usr/sbin/nologin
	
	e.g. $ ssh test@192.168.232.138
			test@192.168.232.138's password:
			Permission denied, please try again.
	
	#3. using usermod
	$ usermod --lock --expiredate 1970-01-01 <username>
	$ usermod --lock --shell /bin/nologin username
	$ usermod -L     -s      /bin/nologin username
	
	#4. Change Age to '0'
	$ chage -E 0 user_ID                                       <= $chage -E -1 user_ID   
	$ ssh test1@192.168.232.138
	test1@192.168.232.138's password:
	Your account has expired; please contact your system administrator
	Connection closed by 192.168.232.138
	
	#5. Lock the password
	http://unix.stackexchange.com/questions/7690/how-do-i-completely-disable-an-account
	$ passwd -l user_ID			<= -l lock
	
	Doesn't satisfy constraints because public key authentication bypasses PAM and still allows direct login.
	$ usermod -s /sbin/nologin <user>
	
	#####
	Doesn't satisfy constraints because breaks the enterprise scheduler
	$ usermod --lock --expiredate 1970-01-01 <user>  
	$ usermod -l 	 -e 		  1970-01-01 user_ID
	<<< This is our winner. Remote login disabled, yet root can still su <user>, as can 
		other users via sudo so the scheduler functions properly and authorized end users 
		can become the target service account as needed. >>>


45. messaging and anouncement
	#1. Wall 	<= messaging to everybody
	$echo testing > wall.txt
	$wall wall.txt					
		Broadcast Message from noza
			(/dev/pts/1) at 19:46 ...
		testing
	
	$ echo "Please log out" | wall 
		Broadcast Message from noza@fm
			(/dev/pts/1) at 19:47 ...
		Please log out
	#2. write — send a message to another user
		$ write <user> 
		  Some text goes here
		  Ctrl+D (EOF)
		$ echo "Some text goes here" | write <user>
	#3. mesg - Control if (non-root) users can send messages to your terminal.
	#4. talk - Talk with other logged in users.
	
	
### REDHAT ENT Linux Init Systems ###
Date		| 	Init Systems
---------------------------
2002-2010 	|	SysV Init
2010-2013	|	Upstart		
2013-current|	Systemd

		
### SysV Init ### 
	- Introduced runlevel or states
	- Default runlevel determined by /etc/inittab
	- Initialization scripts stored in /etc/init.d
	- Symbolic links to init scripts stored in /etc/rd.d/rc[0-6]
		e.g. rc 3  	<= server
			 rc 5   <= GUI
		- One directory per runlevel
	- Start/Stop and order determined by link name
		
		SysV init service link names
		
### Upstart Features ###
	- Asynchronous service start-up
	- Automatic restart of crashed services
	- event-based starting of services

### Systemd ### 
	- Parallel processing of start-up scripts
	- Service dependencies
	- On-demand service activation using sockets and D-Bus
	- System state snapshots
	- Process tracking using control groups
	- Parallel mounting and checking of file systems
	- Binary optimization of start-up programs
	

A. CentOS 6.5(sysV)
	
/sbin/chkconfig --list

	a.List of all services for system
	  chkconfig --list  | more
	b.Runlevel
	  cat /etc/inittab
	c.check runlevel
	  runlevel
	d.Service scripts
	  ls /etc/init.d/
 	e.symbolic link to init.d directory
	  ls /etc/rc3.d
	  

B. CentOS 7(systemd)
	a.List of all services for system
	  systemctl list-units --type service --all
	  systemctl list-units --type service --all | grep running
	  systemctl list-units 

	b.
	  /etc/systemd.system

	c. same as /etc/init.d
	  /usr/lib/systemd/system

	d.
  
   
  
  
### List processes 

	## chkconfig ##					## systemd ##
	
	chkconfig --list				systemctl list-unit-files
	
	chkconfig nginx on				systemctl enable  nginx
	chkconfig nginx off				systemctl disable nginx
	service   nginx start			systemctl start   nginx
	service   nginx stop			systemctl stop 	  nginx
	service   nginx status			systemctl status  nginx
	
	$ systemctl list-unit-files | grep 'influx\|grafana'


CentOS 6.5(sysV)     		   vs     	 CentOS 7 (systemd)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
service httpd status				    systemctl -l(full) status httpd
chkconfig --level 35 httpd  on			systemctl enable/disable httpd.service
chkconfig --list | grep httpd			systemctl start httpd
service httpd start|stop		     	systemctl stop httpd
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
target							systemctl list-unit-files
cat /etc/inittab				systemctl list-units --type target  (--all for not active)					                        
								systemctl list-units --type services
								systemctl get-default
								change to multi-user target
								systemctl set-default multi-uiser.target
								systemctl reboot
								startx 							<= Windows environment 
								systemctl set-default graphical.target
								systemctl reboot
								systemctl start/stop sshd.service
								systemctl enable/disalbe sshd.service  (constant run)		
								systemctl is-enabled sshd 		<= check for sshd
								sytemctl daemon-reload          <= Revert httpd.conf orginal file reload 
								systemctl reload httpd			<= httpd service reload

# journalctl - Query the systemd journal

	journalctl may be used to query the contents of the systemd(1)
    journal as written by systF$(whoemd-journald.service(8).

	$ journalctl | grep fail
	$ journalctl -b          <= current boot
	$ journalctl -b -1       <= previous boot
	$ journalctl -f          <= follow
	$ journalctl -xe		 <= -x  Augment log lines with explanation texts from the message catalog.
							    -e  jump to the end of the journal 
	
	# Check specifig program log
	$ journalctl -u nginx 		 	<= -u unit
	$ journalctl -u -f sshd    		<= -f follow

	$ journalctl --since "10 min ago"



	

----------------------------------------------------------------
Distribution			|	SystemD	  |	System V  |	 Upstart
----------------------------------------------------------------
Amazon Linux											V
CentOS 6												V
CentOS 7						V
Debian 7 ("Wheezy")							V
Debian 8 ("Jessie")				V 
RHEL 6													V
RHEL 7							V
Ubuntu 14.04 or lower									V
Ubuntu 16.04 or higher			V	
----------------------------------------------------------------		
	

From 10.04 up to 17.04:

   Ubuntu      |       Debian  
18.04  bionic     buster  / sid   - 10
17.10  artful     stretch / sid   - 9
17.04  zesty      stretch / sid
16.10  yakkety    stretch / sid
16.04  xenial     stretch / sid

15.10  wily       jessie  / sid   - 8
15.04  vivid      jessie  / sid
14.10  utopic     jessie  / sid
14.04  trusty     jessie  / sid

13.10  saucy      wheezy  / sid   - 7
13.04  raring     wheezy  / sid
12.10  quantal    wheezy  / sid
12.04  precise    wheezy  / sid
11.10  oneiric    wheezy  / sid

11.04  natty      squeeze / sid   - 6
10.10  maverick   squeeze / sid
10.04  lucid      squeeze / sid


	
46. List of package installed
	$ rpm -qa | grep ^httpd
	
# CentOS 'YUM' package list
	$ yum list *httpd  											<= online
    $ yum list installed (same as 'rpm -qa') | grep httpd		<= on system 
    $ yum info httpd  											<= version and detail pkg info


47. EPEL (Extra Packages for Enterprise Linux) 
	CentOS 6 x64 EPEL 
	$ rpm -ivh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm

	CentOS7 
	$ sudo yum install epel-release


	
49. echo command
	#!/bin/bash
	now=$(date)			              <= $(date) will print date command
	echo $now			              <= invoke variable now
	echo "Today's date is $(date)."   <= $(date) will print date command
	cal 	 		                  <= display calendar


50.
	
	

--------------------------------------------------------------------------------------------------
# Diff Tools
https://www.tecmint.com/best-linux-file-diff-tools-comparison/

# diff  	<= compare files line by line
# sdiff 	<= side-by-side merge of file differences
# comm 		<= compare two sorted files line by line
# diff3		<= compare 3 files line by line 
# cmp		<= compare two files byte by byte
# diffstat
# colordiff	<= https://www.colordiff.org/
  vim -d   <= https://www.tutorialspoint.com/vim/vim_diff.htm
 
--------------------------------------------------------------------------------------------------
	https://www.lifewire.com/compare-two-text-files-linux-3861434
     
	$ colordiff file1 file2
    $ diff -u file1 file2 | colordiff

	https://stackoverflow.com/questions/16787916/difference-between-two-directories-in-linux
	
	$ diff -r dir1 dir2 | grep dir1 | awk '{print $4}' > differences.txt	
	
	$ diff -rq dir1 dir2		
			-r - Recursively compare any subdirectories found.
			-q - Output only whether files differ.
			
	https://askubuntu.com/questions/421712/comparing-the-contents-of-two-directories
	
	---------------------------------------------------------------------------
	# Diff between two remote folders through SSH
	---------------------------------------------------------------------------
	Server 1
	IP: images.server1.com
	Images Path: /home/www/images/test_images
	
	Server 2
	IP: images.server2.com
	Images Path: /var/www/site/images/test_images

-------------------------------------------------------------------------------------------
	$ diff -B <\
		(sshpass -p 'pa$$word1' ssh ubuntu1@images.server1.com \
		"find /home/www/images/test_images -type f\ 
		| sed 's/\/home\/www\/images\/test_images\///g'" | sort -n) < \
		
		(sshpass -p 'pa$$word2' ssh ubuntu2@images.server2.com \ 
		"find /var/www/site/images/test_images -type f |  \ 
		sed 's/\/var\/www\/site\/images\/test_images\///g'" | sort -n)\ 
		| grep ">" | awk '{print $2}'
-------------------------------------------------------------------------------------------
		Explanation:

		You can use diff -B <() <() for taking the diff between two streams. The command first uses 
		sshpass to ssh into the two servers without having to enter your passwords interactively.

		Each parameter for diff -B uses find command to recursively list all your images in the 
		specified directory and uses sed to remove the root path of the files (because they are 
		different for two servers - and to make it work for the diff command); and the sort command to sort them.

		Since the output of the diff command returns either > or <, grep is used to filter out 
		only the diffs from your Server 2. Last, awk prints out only the second column 
		(removes the > column from the output).

		NOTE: You need to install sshpass first. 
		sudo apt-get install sshpass


# csplit   Split a file into context-determined pieces

# cut      Divide a file into several parts


# Comparing two directories
https://stackoverflow.com/questions/16787916/difference-between-two-directories-in-linux/16788549	   

$ diff -r dir1 dir2 | grep dir1 | awk '{print $4}' > difference1.txt

$ diff -rq dir1 dir2 				<= -r - Recursively compare any subdirectories found.
									<= -q - Output different files.
  

# comm		<=  files those are in dir1 and not in dir2
	$ comm -23 <(ls dir1 |sort) <(ls dir2|sort)		<= <( ) sign, you can google it as 'process substitution'.


  
52. # chkconfig --add <script_name>
	# chkconfig <script_name> on


chkconfig  provides  a  simple  command-line  tool to maintaining the
       /etc/rc[0-6].d directory hierarchy by relieving  system  administrators
       of  the  task  of  directly manipulating the numerous symbolic links in
       those directories.

       This implementation of chkconfig was inspired by the chkconfig  command
       present  in the IRIX operating system. Rather than maintaining configu‐
       ration information outside of the  /etc/rc[0-6].d  hierarchy,  however,
       this  version  directly  manages  the  symlinks in /etc/rc[0-6].d. This
       leaves all of the configuration  information  regarding  what  services
       init starts in a single location.

       chkconfig  has five distinct functions: adding new services for manage‐
       ment, removing services from management, listing  the  current  startup
       information  for  services,  changing  the startup information for ser‐
       vices, and checking the startup state of a particular service.

       When chkconfig is run with only a service name, it checks to see if the
       service  is configured to be started in the current runlevel. If it is,
       chkconfig returns true; otherwise it returns false. The --level  option
       may be used to have chkconfig query an alternative runlevel rather than
       the current one.

       When chkconfig is run with the --list argument, or no arguments at all,
       a listing is displayed of all services and their current configuration.

       If  one  of  on,  off, reset, or reset priorities is specified after the
       service name, chkconfig changes the start-up information for the  specified  
	   service.  The on and off flags cause the service to be started or
       stopped, respectively, in the runlevels being changed. The  reset  flag
       resets  the  on/off state for all runlevels for the service to whatever
       is specified in the init script in question, while the  reset priorities
       flag  resets  the  start/stop priorities for the service to whatever is
       specified in the init script.

       By default, the on and off options affect only runlevels 2, 3,  4,  and
       5,  while  reset and reset priorities affects all of the runlevels.  The
       --level option may be used to specify which runlevels are affected.

       Note that for every service, each runlevel has either a start script or
       a  stop  script.   When  switching runlevels, init will not re-start an
       already-started service, and will not re-stop a  service  that  is  not
       running.

       chkconfig also can manage xinetd scripts via the means of xinetd.d con‐
       figuration files. Note that only the on, off, and --list  commands  are
       supported for xinetd.d services.

       chkconfig  supports  a  --type argument to limit actions to only a spe‐
       cific type of services, in the case where services of either  type  may
       share a name. Possible values for type are sysv and xinetd.



54. List of groups
# getent <- 'get ent'ries from Name Service Switch libraries
# stat   <- display file or file system status

  # Check who owns folder?
	$ stat -c %U /var/www/html			<= %U User
		www-data
		
	$ stat -c %G /var/www/html			<= %G Group
		www-data	
	
	$ cat /etc/group  <= Group List
 	$ getent group | cut -d: -f1
	
  # groups
	$ getent - get entries from Name Service Switch libraries


	
  # Who(users list) is in the group
	$ getent group www-data
		www-data:x:33:
	
  # Add an User to Group
	$ sudo usermod -G www-data -a apark
	
	$ getent group www-data
		www-data:x:33:apark  <= apark added
	
  # Delete group
	$ groupdel Group_Name
	
	$sudo groupdel $(getent group | cut -d: -f1 | grep dock)

	
  ### gpasswd
	
	$  adduser apark
    $  gpasswd -a apark wheel
    $  gpasswd -a apark root
    $  vi /etc/sudoers
	
	The term "WHELL" is derived from the slang phrase big wheel, 
	referring to a person with great power or influence.
	
	
55. SElinux <= add port to security-enhanced linux 
https://wiki.centos.org/TipsAndTricks/SelinuxBooleans

	$ cat /etc/sysconfig/selinux

	semange port -a -t http_port_t -p tcp 10051  

	### set SELINUX Permissive mode
	$ getenforce  <= CHECK
    $ setenforce permissive
	$ sed -i 's\=enforcing\=permissive\g' /etc/sysconfig/selinux
	# To Check
	$ getenforce
	Permissive
  
	#Enable nis_enable
	  $ sudo setsebool -P nis_enabled on				<= -P  Permanent
	
	#Check
	  $ getsebool nis_enabled

  
  
  
56. Region and Time NTP

#### Region - TIME ZONE ####

  Ubuntu
  sudo dpkg-reconfigure tzdata
  sudo vi /etc/timezone	<= change to **America/Los_Angeles or America/New_York


#### NTP - TIME ZONE ####

	# CenOS 7.x #

	$ yum install ntp
	$ systemctl start ntpd
	$ systemctl enable ntpd

	$ ls -l /etc/localtime 
	$ timedatectl list-timezones
	$ timedatectl set-timezone America/Los_Angeles


	# CentOS 6.x # 
	America/Tijuana

	$ ntpdate pool.ntp.org  		<=0~3.us.pool.ntp.org 
	$ chkconfig ntpd on				<= put in into startup
	$ /etc/init.d/ntpd start

	# Ubuntu 14.04(Trusty) #
	$ apt-get install ntp
	$ ntpq -p  							<=-p peer list of ntp servers  (ntpq=std NTP query program)
	$ vi /etc/ntp.conf					<= replace with **US** 0~3.us.pool.ntp.org 
		########################
		server 0.us.pool.ntp.org
		server 1.us.pool.ntp.org
		server 2.us.pool.ntp.org
		server 3.us.pool.ntp.org
		########################
	$ sudo service ntp restart 

	# To stop ntpd #
	$ sudo /etc/init.d/ntp stop
	$ sudo service ntp stop

	# To prevent it from starting at boot:
	$ sudo update-rc.d -f ntp remove

# Update NTP time Ubuntu
	$ sudo ntpdate pool.ntp.org
	4 May 17:34:07 ntpdate[38018]: step time server 204.2.134.163 offset 53.202434 sec

	$ sudo ntpdate pool.ntp.org  <=  **** no servers can be used, exiting ***
		31 Aug 19:05:55 ntpdate[8911]: the NTP socket is in use, exiting
	$ sudo service ntp stop
		[ ok ] Stopping NTP server: ntpd.
	$ sudo ntpdate pool.ntp.org
		31 Aug 19:07:11 ntpdate[10355]: adjust time server 46.29.176.115 offset -0.002893 sec
	$ sudo service ntp start

	
	
57. CentOS6.x MySQL auto start and start MySQL services
	$ chkconfig mysqld on
	$ /etc/init.d/mysqld start


	
	
### Ubuntu 16.04 Iptables setup ###
	http://linux-sys-adm.com/ubuntu-16.04-lts-how-to-configure-firewall-iptables-fail2ban/
	https://oitibs.com/easy-ubuntu-16-server-firewall/
	http://dev-notes.eu/2016/08/persistent-iptables-rules-in-ubuntu-16-04-xenial-xerus/
	
	# Install IPTables Persistent Package 
	$ apt-get install -y iptables-persistent

	# Add netfilter-persistent Start-up
	$ invoke-rc.d netfilter-persistent save		<= enable for start up
	
	$ service netfilter-persistent stop

	# Start netfilter-persistent Service		<=	### After Edit IPtables ##
	
	$ service netfilter-persistent     start | stop | status
	$ /etc/init.d/netfilter-persistent start | stop | status
	
	$ iptables -nL			<= List  all rules in the selected chain. '-n' NO DNS lookup (Faster)
	$ iptables -S			<= Print all rules in the selected chain. 	
	
	### Restore
	sudo iptables-restore < ~/serenity-iptables-rules/ruleset-v4
	sudo ip6tables-restore < ~/serenity-iptables-rules/ruleset-v6

	### Preroute rediect
	$ sudo iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3000	
	

	
	### CentOS7 ### 

### Iptables opens all	###
  $ iptables -I INPUT -j ACCEPT
	
	
	$ sudo iptables -L			<=  -L, --list 		 [List  all  rules  in the selected chain.]
	$ sudo iptables -S 			<=  -S, --list-rules [chain - Print all rules in the selected chain. ]
	$ sudo iptables -nL			<= List  all rules in the selected chain. '-n' NO DNS lookup (Faster)

# IPtables Block all but 80(http),443(https)
iptables -A INPUT -p tcp -m tcp -m multiport ! --dports 80,443 -j DROP	

# IPtables Block all but 22
iptables -A INPUT -p tcp -m tcp -m multiport ! --dports 22 -j DROP	

	
# NAT
# Reroute

	# Check NAT List
	https://unix.stackexchange.com/questions/205867/viewing-all-iptables-rules
	
	$ sudo iptables -t nat -L  (iptables 티넷 리스트)
	
	Chain PREROUTING (policy ACCEPT)
	target     prot opt source               destination
	REDIRECT   tcp  --  anywhere             anywhere             tcp dpt:http redir ports 3000
	REDIRECT   tcp  --  anywhere             anywhere             tcp dpt:http redir ports 3000	
	
# Install IPtabls on CentOS 7	
	$ sudo yum -y install iptables-services
	$ sudo systemctl enable iptables 		
	$ sudo systemctl mask firewalld			
	$ sudo systemctl stop firewalld

	$ vi /etc/sysconfig/iptables
	$ sudo systemctl start iptables
	
	Writing a Simple Rule Set( https://wiki.centos.org/HowTos/Network/IPTables)

IMPORTANT: At this point we are going to clear the default rule set. If you are connecting 
remotely to a server via SSH for this tutorial then there is a very real possibility that 
you could lock yourself out of your machine. You must set the default input policy to 
accept before flushing the current rules, and then add a rule at the start to explicitly 
allow yourself access to prevent against locking yourself out.
We will use an example based approach to examine the various iptables commands. In this 
first example, we will create a very simple set of rules to set up a Stateful Packet 
Inspection (SPI) firewall that will 'allow all outgoing connections' but 'block all 
unwanted incoming connections':


# iptables -P INPUT ACCEPT
# iptables -F
# iptables -A INPUT -i lo -j ACCEPT									#Localhost Interface 
# iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
# iptables -A INPUT -p tcp --dport 22 -j ACCEPT
# iptables -P INPUT DROP
# iptables -P FORWARD DROP
# iptables -P OUTPUT ACCEPT
# iptables -L -v

which should give the following output:


Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 ACCEPT     all  --  lo     any     anywhere             anywhere
    0     0 ACCEPT     all  --  any    any     anywhere             anywhere            state RELATED,ESTABLISHED
    0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:ssh
Chain FORWARD (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination


Now lets look at each of the 8 commands above in turn and understand exactly what we've just done:

iptables -P INPUT ACCEPT 
	If connecting remotely we must first temporarily set the default policy on the INPUT chain to ACCEPT 
	otherwise once we flush the current rules we will be 'locked out' of our server.
iptables -F 
	We used the '-F' switch to 'flush' all existing rules so we start with a clean state from which to add new rules.
	
iptables -A INPUT -i lo -j ACCEPT 
	Now it's time to start adding some rules. We use the -A switch to 'append' (or add) a rule to a specific chain, 
	the INPUT chain in this instance. Then we use the '-i' switch (for interface) to specify packets matching or 
	destined for the lo (localhost, 127.0.0.1) interface and finally -j (jump) to the target action for packets 
	matching the rule - in this case ACCEPT. So this rule will allow all incoming packets destined for the 
	'localhost interface' to be accepted. This is generally required as many software applications expect to be 
	able to communicate with the localhost adaptor.
	
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT 
	This is the rule that does most of the work, and again we are adding (-A) it to the INPUT chain. Here we're 
	using the -m switch to load a module (state). The state module is able to examine the state of a packet 
	and determine if it is NEW, ESTABLISHED or RELATED. NEW refers to incoming packets that are new incoming 
	connections that weren't initiated by the host system. ESTABLISHED and RELATED refers to incoming packets 
	that are part of an already established connection or related to and already established connection.

iptables -A INPUT -p tcp --dport 22 -j ACCEPT 
	Here we add a rule allowing SSH connections over tcp port 22. This is to prevent accidental lockouts when 
	working on remote systems over an SSH connection. We will explain this rule in more detail later.

iptables -P INPUT DROP 
	The -P switch sets the default policy on the specified chain. So now we can set the default policy on the 
	INPUT chain to DROP. This means that if an incoming packet does not match one of the following rules it 
	will be dropped. If we were connecting remotely via SSH and had not added the rule above, we would have 
	just locked ourself out of the system at this point.

iptables -P FORWARD DROP 
	Similarly, here we've set the default policy on the FORWARD chain to DROP as we're not using our computer 
	as a router so there should not be any packets passing through our computer.

iptables -P OUTPUT ACCEPT 
	and finally, we've set the default policy on the OUTPUT chain to ACCEPT as we want to allow all outgoing 
	traffic (as we trust our users).

$ iptables -L -v 	
	Finally, we can list (-L) the rules we've just added to check they've been loaded correctly.


### -j REJECT --reject-with icmp-host-prohibited
	https://unix.stackexchange.com/questions/124624/what-a-input-j-reject-reject-with-icmp-host-prohibited-iptables-line-does-ex
	

-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited




REJECT
This is used to send back an error packet in response to the matched packet: otherwise it is equivalent to DROP so it is a 
terminating TARGET, ending rule traversal. This target is only valid in the INPUT, FORWARD and OUTPUT chains, and 
user-defined chains which are only called from those chains. The following option controls the nature of the error 
packet returned:

--reject-with type
The type given can be:

icmp-net-unreachable
icmp-host-unreachable
icmp-port-unreachable
icmp-proto-unreachable
icmp-net-prohibited
icmp-host-prohibited or
icmp-admin-prohibited (*)

which return the appropriate ICMP error message (port-unreachable is the default). The option tcp-reset can be used 
on rules which only match the TCP protocol: this causes a TCP RST packet to be sent back. This is mainly useful for 
blocking ident (113/tcp) probes which frequently occur when sending mail to broken mail hosts (which won't accept 
your mail otherwise).

(*) Using icmp-admin-prohibited with kernels that do not support it will result in a plain DROP instead of REJECT




	
### Changing data directory on Centos 7 + MySQL 5.6	
	$ wget http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm
	$ yum localinstall mysql57-community-release-el7-7.noarch.rpm
	
	
	$ systemctl mysqld stop
	$ cp -rap /var/lib/mysql/ /target_dir/mysql/
	$ chown mysql:mysql /target_dir/mysql
	
	# edit /etc/my.conf to redirect datadir & socket to new path
	$ vi /etc/my.conf 
	# change to new data, socket & client connection location
	datadir=/target_dir/mysql
	socket=/target_dir/mysql/mysql.sock
	[client]
	socket=/target_dir/mysql/mysql.sock
	
	
	# manage SELinux => install semanage (In Centos 7 minimal is not installed)
	$ getenforce
	$ yum provides /usr/sbin/semanage
	$ yum -y install policycoreutils-python


	$ getenforce
	$ semanage fcontext -a -t mysqld_db_t "/new_dir/mysql(/.*)?"
	$ restorecon -Rv /new_dir/mysql
	$ systemctl start mysqld
	
	# Check SELinux reports any error on MySQL
	$ grep mysqld /var/log/audit/audit.log

	

	
	
	
58. # CentOS 6 Turn off firewall and SELINUX

	chkconfig iptables off && chkconfig ip6tables off && 
	service iptables stop &&  service ip6tables stop && setenforce 0
	$ vi /etc/sysconfig/iptables
	$ iptables -L		<= List
	$ iptables -L -n   <= NO DNS lookup (Faster)
	$ iptables -nL
	$ iptables -S
  
  
    
  ### Ubuntu 14.04 IPtables setup ###
	$ sudo apt-get update
	$ sudo apt-get install iptables-persistent
	$ sudo vi /etc/iptables/rules.v4 
  	
	$ /etc/init.d/iptables-persistent {start|restart|reload|force-reload| save|flush}
	$ service iptables-persistent start | stop | restart | reload
  
  ### Check error restore test ###
	$ iptables-restore -vv < /etc/iptables/rules.v4

	
	
  ### Ubuntu 16.04 Iptables setup ###
	http://linux-sys-adm.com/ubuntu-16.04-lts-how-to-configure-firewall-iptables-fail2ban/
	https://oitibs.com/easy-ubuntu-16-server-firewall/
	
	# Install IPTables Persistent Package 
	$ apt-get install -y iptables-persistent

	# Add netfilter-persistent Startup
	$ invoke-rc.d netfilter-persistent save
	
	$ service netfilter-persistent stop | start
	
	### Edit IPtables ##
	
	
	$ service netfilter-persistent enable
	$ service netfilter-persistent start
	$ service netfilter-persistent status
	
	$ iptables -nL







# Security Selinux
vi /etc/selinux/config #replace SELINUX=enforcing with SELINUX=disabled

58. Bash History with time stamp
	###										###	
	#  $ export HISTTIMEFORMAT='%F '          #		<= %F Equivalent to %Y - %m - %d
	#  $ export HISTTIMEFORMAT='%F %T  ' 	  #		<= %F Equivalent to %Y - %m - %d,  %T Replaced by the time ( %H : %M : %S )
	** Check the export list	
	$ export -p | grep HIST*
	$ env | sort | grep HIST


59. Prompt Color red

	Permanently add Green color scheme to .bashrc file
	######															  ####
	# echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'" >> ~/.bashrc #   <= GREEN Color
	######															  ####

	### Temp current shell
	export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]' 		 >> ~/.bashrc		<= Green 
	export PS1='\[\e[1;31m\][\u@\h \W]\$\[\e[0m\]' 		 >> ~/.bashrc		<= Red  
    export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"  	 >> ~/.bashrc		<= Green|Red 
    export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[1;32m\w$"  	 >> ~/.bashrc		<= Green|Red 
    
	### Permanent
	echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'" 	 >> ~/.bashrc		<= Green 
    echo 'export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"'   >> ~/.bashrc		<= Green|Red 

								
	


	
59. network monitoring and troubleshooting
	
	# iftop
	$ iftop -n -i eth1        <= -n no name lookup -i interface eth1
		### batch mode ###
		$ iftop -t 					<= t text mode 
		$ iftop -t > iftop.log		<= save to iftop.log file 
		$ iftop -t -s 60 > log.txt	  <= 60 sec  
		$ iftop -t -s 60 > log.txt &  <= running in background
	
	# vnstat	 					
	# a console-based network traffic monitor
	https://www.howtoforge.com/tutorial/vnstat-network-monitoring-ubuntu/
	
		$ vnstat -l					<= real time network statistic
		$ vnstat --testkernel		<= check the kernel is providing all the information
		$ vnstat -d					<= -h hour, -d day, -w week, -w month,
		$ vnstat --dumpdb			<= export to Excel or 
	
	# traceroute TTL(time-to-live) value, also known as hop limit, option
	$ traceroute -w 3 -q 1 -m 16 example.com

	# MTR
		MTR - MTR represents an evolution of the traceroute command by providing a greater data sample, as if augmenting 
		traceroute with ping output. This document provides an in depth overview of MTR, the data it generates, and how 
		to properly interpret and draw conclusions based on the data provided by it.
	
	### GUI tool
	$ iptraf
	
	
	$ iperf 	<= is a tool for active measurements of the maximum achievable bandwidth on IP networks.
	https://iperf.fr/
	
	$ vnstat
	http://www.happyapps.io/blog/2015-08-15-track-and-log-a-linux-server-s-bandwidth-use

	$ nload
	
	$ nethogs
	
	
	
	
	
	
### netsniff-ng 								<- the packet sniffing beast
	Its gain of performance is reached by zero-copy mechanisms, so that on packet reception and transmission 
	the kernel does not need to copy packets from kernel space to user space and vice versa.
	toolkit can be used for network development and analysis, debugging, auditing or network reconnaissance.

	$ apt-get|yum install netsniff-ng 					
	http://netsniff-ng.org/
		
	For geographical AS TCP SYN probe trace route to a website:
	$ astraceroute -d eth0 -N -S -H netsniff-ng.org			<- autonomous system trace route utility

	For kernel networking statistics within promiscuous mode:
	$ ifpps -d eth0 -p						<-fetch and format kernel network statistics

	For high-speed network packet traffic generation, trafgen.txf is the packet configuration:
	$ trafgen -d eth0 -c trafgen.txf

	For compiling a Berkeley Packet Filter fubar.bpf:
	$ bpfc fubar.bpf

	For live-tracking of current TCP connections (including protocol, application name, city and country of source and destination):
	$ flowtop

	For efficiently dumping network traffic in a pcap file:
	$ netsniff-ng -i eth0 -o dump.pcap -s -b 0	
	
	
###	tcpdump
	http://www.rationallyparanoid.com/articles/tcpdump.html
	
	$ tcpdump -i eth1 -n tcp port 10050
	####################################
	# tcpdump -i any -n tcp port 10050 #    <= zabbix agent port
	####################################
	-i = listening interface
	-n = not to convert addresses

	### Capture Ping - icmp message ##
	$ tcpdump -n icmp
	$ tcpdump -v -n icmp
	$ tcpdump -n icmp and 'icmp[0] != 8 and icmp[0] != 0'
	$ tcpdump -n icmp and icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply

	### Capture any packets where the source host is 192.168.1.1. Display IP addresses and port numbers:
	http://www.rationallyparanoid.com/articles/tcpdump.html
	
	$ tcpdump 		  -n src host 192.168.1.1
	$ tcpdump -i eth1 -n src host 192.168.1.1
	
	### Exclude port 22 https://danielmiessler.com/study/tcpdump/
	
	$ tcpdump -i any -n tcp port not 22 and host 1.2.3.4
		
  https://www.cyberciti.biz/faq/tcpdump-capture-record-protocols-port/
	
   $ /usr/sbin/tcpdump -n -c 30000 -w /root/port80debug.txt
	 Capturing traffic information using cronjobs
	tcpdump can be used to find out about attacks and other problems. Let us say your webserver facing problem everday at midnight. Enter following command into cron. It will schedule capturing of 30,000 packets and writing raw data to a file called port.80.debug.txt:
	$/usr/sbin/tcpdump -n -c 30000 -w /root/port.80.debug.txt

	Next day you can log into your box and read the /root/port.80.debug.txt file:
	tcpdump -X -vv -r /root/port.80.debug.txt
  
###	NetCat (nc)

								
	 <<< Client Sending	>>>	 ====>	  <<< Server listening >>>
	 
	$ nc -v 192.168.110.220  22 	  $ nc -l 22 	  
	$ nc -v google.com       80 	  $ nc -l 22 	  
	
		 -v verbose sending  		       -l <= listening mode
	
 
  
  
  https://www.digitalocean.com/community/tutorials/how-to-use-netcat-to-establish-and-test-tcp-and-udp-connections-on-a-vps
  
	Ping and capture IP only
	$ ping -c 1 www.google.com | gawk -F'[()]' '/PING/{print $2}'
  


# DNS # Domain Lookup
http://www.beetlebrow.co.uk/what-do-you-need/help-and-documentation/unix-tricks-and-information/dns-checking-with-whois-nslookup-and-dig
https://securityonline.info/tutorial-nslookuphostdigwhois-dns-information-gathering/

	# Simple to get theIP
	
	$ ping 		domainName.com		<= get IP
	
	$ host 		domainName.com			
	
	# DNS Detail info
	$ nslookup
      return IP addresses and domain names for a network server.
	
	
	$ dig 
	
	
	$ whois  	domainName.com
	
    $ yum install bind-utils
    
	$ host $HOSTNAME | xargs | cut -f4 -d' '
	
	$ dig +short +trace www.name.com
	


# dig
$ type -a dig
	dig is /usr/bin/dig
$ which dig
	/usr/bin/dig


cento7 initial setup
$ yum install bind-utils -y






	
# CentOS7 DNS IP Change
https://www.techrepublic.com/article/how-to-configure-a-static-ip-address-in-centos-7/
/etc/sysconfig/network-scripts/ifcfg-eth0


IPADDR=192.168.1.200
NETMASK=255.255.255.0
GATEWAY=192.168.1.1
DNS1=8.8.8.8
DNS2=10.128.1.44
DNS3=10.128.18.63

 sudo systemctl restart network

  
  
61. telnet     ip_address    port_number  <= check port is open for connection
	$ telnet ip_address 22
	
	to exit 
	
	'ctrl + ]'  or '^]' 
	telnet> "close"			<= Exit



	
62. Ubuntu/Debian Package Clean ### Package Clean ###
https://www.cyberciti.biz/tips/linux-debian-package-management-cheat-sheet.html
Both Debian and Ubuntu Linux provides a number of package management tools.

	apt-get : APT(Advanced Package Tool). It supports installing packages over 
			  internet using ftp or http protocols. 
	dpkg 	: Debian packaging tool which can be used to install, query, uninstall packages.
	apt 	: Interactive and recommended command for all users.
	aptitude: It is a text-based interface to the Debian GNU/Linux package system.
	synaptic: GUI front end for APT
	
		rpm <- Red hat Linux package names 
		deb <- Debian package names
	
	ex: apache_1.3.31-6_i386.deb
			apache : Package name
			1.3.31-6 : Version number
			i386 : Hardware Platform on which this package will run (i386 == intel x86 based system)
			.deb : Extension that suggest it is a Debian package
	
# Ubuntu/Debian package Search
  apt-cache - query the APT cache

  syntax: apt-cache search {package-name} 
		$ apt-cache search mysql
		$ apt-cache search vim
	
  Syntax:  apt-get 		<- APT package handling utility -- command-line interface
	
		$ apt-get install -y mysql
		$ apt-get install -y vim

# Package Remove
	# removes package but keeps the configuration files
		Syntax: apt-get remove  				
	
	# removes package & configuration files	
		$ sudo apt-get --purge remove {package-name}

# Package Remove + Dependencies
	# apt-get autoremove
	# apt-get --purge autoremove

# Update my system
	# apt-get update
	# apt-get upgrade

# Upgrade individual package 	
	$ apt-get install bash
	$ apt-get install vim
	
	$ yum update bash
	

# Backing ups:
  sudo cp /etc/apt/sources.list /etc/apt/sources.list.original
  sudo cp /var/lib/dpkg/status /var/lib/dpkg/status.original
  sudo cp -r --parents home/aws/* /home/apark/awn/   #--recursive

  sudo apt-get clean 
  sudo apt-get autoclean 


  # Fixing dependencies using apt's fix-broken mode
	sudo apt-get -f install
	
 E:Internal Error when using apt-get	
	sudo apt-get update
	sudo apt-get clean
	sudo apt-get cehck                       <= 
	sudo apt-get install -fy
	sudo dpkg -i /var/cache/apt/archives/*.deb
	sudo dpkg --configure -a
	sudo apt-get install -fy
	sudo apt-get dist-upgrade
	
####	Broken package fix ### 
sudo dpkg --configure -a
sudo apt-get install -f

$ sudo vi /var/lib/dpkg/status 			<= remove whole trouble block
$ cat /etc/apt/sources.list

#####
sudo sh -c "apt-get update;apt-get dist-upgrade;apt-get autoremove;apt-get autoclean"  
####
sudo sh -c "apt-get update;apt-get autoclean;apt-get clean;apt-get autoremove"
####
sudo dpkg --remove -force --force-remove-reinstreq package_name
####



### Clean APT Key

$ apt-key list
	pub   4096R/563278F6 2016-04-08 [expires: 2018-04-08]
	uid                  Foreman Automatic Signing Key (2016) <packages@theforeman.org>

$ apt-key del 563278F6  <= key_name

# how to remove -file
To remove a file whose name starts with a '-', for example '-foo',

$ rm --help
use one of these commands:
	rm -- -foo
	rm ./-foo


  

63. How do I fix a “Problem with MergeList” or “status file could not be parsed” error when trying to do an update?


sudo rm /var/lib/apt/lists/* -vf
sudo apt-get update



64.  AWS AMI MYSQL setup root pw issue 

1. apt-get/yum install mysql-server
2. /usr/bin/mysql_secure_installation
3. service mysqld stop 
4. rm -rf /var/lib/mysql/*
5. service mysqld start  <= recreate tables 


6. /usr/bin/mysqladmin -u root password 'Your-New-Passwd'  <= Your mysql root's pw

	mysql remote host login
	$ mysql -u webadmin –h 65.55.55.2 –p
	$ mysql -ID_admin -p'password'  -hlocalhost

	### MySQL remote login with different port ###
	$ mysql -uroot -p`cat ~/.mysql_shadow` -h localhost -P 7506

	
	# To install only mysql (client) you should execute
		$ yum install mysql
		$ yum install mariadb
	
	# To install mysql client and mysql server:
		$ yum install mysql mysql-server
		$ yum install mysql mariadb-server
	
	grant all privileges on *.* to apark@localhost identified by 'passwd' with grant option;
	GRANT ALL ON apark.* TO 'apark'@'74.217.69.194' IDENTIFIED BY 'apark';
	
	MariaDB [(none)]> select user, host from mysql.user
    -> ;
+----------------------+---------------+
| user                 | host          |
+----------------------+---------------+
| root                 | 127.0.0.1     |
| apark                | 74.217.69.194 |		<= access from remote ip
| root                 | ::1           |
| apark                | localhost     |		<= access from local
| root                 | localhost     |
+----------------------+---------------+

	
	
	
	
	
	
65. Swappiness

	Swappiness is a Linux kernel parameter that controls the relative weight given to 
	swapping out runtime memory, as opposed to dropping pages from the system page cache. 
	Swappiness can be set to values between 0 and 100 inclusive. A low value causes the 
	kernel to avoid swapping, a higher value causes the kernel to try to use swap space. 
	The default value is 60, and for most desktop systems, setting it to 100 may affect 
	the overall performance, whereas setting it lower (even 0) may decrease response latency.

Value	Strategy
vm.swappiness = 0	The kernel will swap only to avoid an out of memory condition.
vm.swappiness = 60	The default value.
vm.swappiness = 100	The kernel will swap aggressively.
To temporarily set the swappiness in Linux, write the desired value (e.g. 10) to /proc/sys/vm/swappiness 
using the following command, running as root user:


	# Out of meory
	$ grep oom /var/log/*
	$ grep total_vm /var/log/*

	http://unix.stackexchange.com/questions/128642/debug-out-of-memory-with-var-log-messages
	http://unix.stackexchange.com/questions/92525/can-linux-run-out-of-ram/92544#92544
	
	$ grep oom /var/log/*
	/var/log/messages:Jan  5 18:41:12 app-03 kernel: httpd invoked oom-killer: gfp_mask=0x200da, order=0, oom_score_adj=0
	/var/log/messages:Jan  5 18:41:14 app-03 kernel: [<ffffffff8116d24e>] oom_kill_process+0x24e/0x3b0
	/var/log/messages:Jan  5 18:41:17 app-03 kernel: [ pid ]   uid  tgid total_vm  rss nr_ptes swapents oom_score_adj name
	
	pid 			<= The process ID.
	uid 			<= User ID.
	tgid 			<= Thread group ID.
	total_vm 		<= Virtual memory use (in 4 kB pages)
	rss 			<= Resident memory use (in 4 kB pages)
	nr_ptes			<= Page table entries
	swapents 		<= Swap entries
	oom_score_adj 	<= Usually 0; a lower number indicates the process will be less likely to die when the OOM killer is invoked.
	
	
	
# Set the swappiness value as root
  $ echo 10 > /proc/sys/vm/swappiness
 
# Alternatively, run this 
  $ sysctl -w vm.swappiness=10   # sysctl - configure kernel parameters at runtime
 
# Verify the change
  $ cat /proc/sys/vm/swappiness
  10

 
# Alternatively, verify the change
  sysctl vm.swappiness
  vm.swappiness = 10
  Permanent changes are made in /etc/sysctl.conf via the following configuration line     
  inserted, if not present):
  vm.swappiness = 10

  
  
  
66. rpm package 
	### install 
    $ rpm -ivh foo-xxx.rpm <= install
    $ rpm -Uvh foo-xxx.rpm <= upgrade

    ### Remove
	$ rpm -qa | grep -i webmin
	$ rpm -e <package name>
	# Combo
	$ sudo rpm -e $(sudo rpm -qa docker*)


	### Search installed sw
	$ rpm -qa | grep mysql
	$ /sbin/ldconfig -v | grep mysql

  
67. nmap  <= Short for Network Mapper 
    It is an open source security tool for network exploration, security scanning and     
	auditing. However, nmap command comes with lots of options that can make the utility     
	more robust and difficult to follow for new users.
	
	$ nmap -sT ip_address  ( s-scan, T-TCP)
	$
	nmap -v -A scanme.nmap.org

	$ nmap -T4 -A -v localhost
		-T<0-5>: Set timing template (higher is faster)
		-A: OS & version detection, script scanning, and traceroute
		-v: Increase verbosity level (use -vvvv)

	$ nmap -v -sn 192.168.0.0/16 10.0.0.0/8
		-sn: Ping Scan - disable port scan
	
	$ nmap -v -iR 10000 -Pn -p 80
		-iR <num hosts>: Choose random targets
		-Pn: Treat all hosts as online -- skip host discovery
		-p <port ranges>: Only scan specified ports

	
	
	
68. pgrep, pkill - look up or signal processes based on name and other attributes


69. rthunter(Root Kit Hunter)


70. 
	
		
# hanging terminal during the SSH connection.	
http://askubuntu.com/questions/344863/ssh-new-connection-begins-to-hang-not-reject-or-terminate-after-a-day-or-so-on	
A problem can arise when you are trying to connect from behind a NAT router using OpenSSH. 
During session setup, after the password has been given, OpenSSH sets the TOS (type of service) 
field in the IP datagram. Some routers are known to choke on this. The effect is that your 
session hangs indefinitely after you gave your password. Here is the example output from such 
an ssh session:		
$ ssh -vvv id@x.com	
$ ssh -o 		"ProxyCommand nc %h %p" id@x.com
	  ^option	
		




ifconfig eth1 down | up


		
71.  Transfer a file using SCP + pem key

	$ scp -i /path/to/key.pem     /path/to/file.jpg    user_id@ip:/home/id/folder
	
	## -i Identity_file such as pem key
	$ scp -i /AWS-OC-Key/oc-prod.pem /AWS-OC-Key/oc-sDATEe.pem ec2-user@ip_address:/home/ec2-user/.cert
	------------------------
	### Local ===> Remote 
	------------------------
	$ scp -pv file.zip noza@192.168.221.128:/home/noza/download   	<- -p      Preserves modification times, access times, and modes from the original file.
																	<- -v verbose	
	$ scp -r /home/id/.ssh  id@ip:/home/id/			<= -r recursive for sub directories
	$ scp -i .ssh/oc.pem  ami.zip ubuntu@10.10.1.89:/home/ubuntu/

	------------------------
	### Remote ===> Local 
	------------------------
	$ scp -r id@ip:/home/id/.ssh    /home/id/.ssh			<- -r      Recursively copy entire directories.
	

	### install unzip
	$ unzip file.zip -d /tmp/name			<= -d destin directory
    
	### Make a directory remotely
	$ ssh apark@remote_ip "mkdir /path/to/dir"

	
72. Combine Xargs with Find command 
	xargs -build and 'execute command lines' from standard input

	$ ls
	one.c  one.h  two.c  two.h

	$ find . -name "*.c" | xargs rm -rf

	$ ls
	one.h  two.h

	Use fgrep in order to match for plain ., or escape the dots.

	$ find . -type f | xargs fgrep '...'   <= or if you still want to use grep :
	$ find . -type f | xargs grep '\.\.\.' <= And if you only want the current directory 	and not its subdirs :
	$ find . -maxdepth 1 -type f | xargs fgrep '...'
	
	Finding directory files with word
	$ grep -Ril "finding_word" /target/dir  <= R-recursive, i-ignore case, l-show file name
	
	

	

73. 
	iptables -L
	iptables -L -n
	iptables -nvL

	Ubuntu	iptables-restore < /etc/iptables/rules.v4
	
	CentOS	iptables-restore < /etc/sysconfig/iptables

	iptables-restore and ip6tables-restore are used to restore
	IP and IPv6 Tables from data specified on STDIN.  Use  I/O
        redirection(> <) provided by your shell to read from a file
	(< /etc/iptables/rules.v4)

	



75. Stdin, Stdout(1), Stderr(2) 
	1>&2    <= STD output should go to the same place as STD error is going.

	
76. Backtick
	Command within backticks is evaluated (executed) by the shell before the main command (like chown in your examples), 
	and the output of that execution is used by that command, just as if you'd type that output at that place in the command line.

	$ sudo chown `id -u` /somedir
	$ sudo chown 1000 /somedir



77. Hostname 
	$ hostname
	$ hostname -f			<= full
	$ hostname -s 			<= short
	
	
	Hostname Change
	# CentOS 6
	https://support.rackspace.com/how-to/centos-hostname-change/
	
	$ vi /etc/sysconfig/network
	$ vi /etc/hosts
	$ vi /etc/hostname
	$ /etc/init.d/netwrok restart
	# or REBOOT
	
	
78. DNS lookup utility
https://nilminus.wordpress.com/penetration-testing/enumeration/dns/using-nslookup-dig-and-host/
https://www.quora.com/Bash-shell-What-are-the-differences-between-host-dig-and-nslookup-and-when-should-I-use-each

	$ nslookup	<= was the first tool for querying the DNS

	$ dig		<= probing the DNS
	
	$ host 		<= Best for BASH scripting
					output has one value per line and is easily scriptable using awk to extract IP
	$ host google.com  
	
	google.com has address 216.58.194.174
	google.com has IPv6 address 2607:f8b0:4005:804::200e
	google.com mail is handled by 40 alt3.aspmx.l.google.com.
	...


Ubuntu
/etc/hosts
/etc/hostname


CentOS6.x file location =>  
	$ cat /etc/hostname 
	$ hostname 
	$ echo new_name > /etc/hostname 
	
	$ systemctl restart systemd-hostnamed
	
	$ cat /etc/sysconfig/network
	
	$ hostnamectl status
	
	
###########################################
#	$ hostnamectl set-hostname new_name   # 
###########################################
	
	
	$ timedatectl status 
	$ timedatectl list-timezones
	$ timedatectl set-timezone UTC
	$ timedatectl set-timezone America/New_York
	$ timedatectl set-timezone America/Los_Angeles
	
	
	Ubuntu timezone change
	$ sudo dpkg-reconfigure tzdata	
	
	
	
	
78. CenOS7 eno****** to eth0

	Step1#
	$sudo vi etc/sysconfig/grub	<= add "net.ifnames=0 biosdevname=0"
GRUB_CMDLINE_LINUX="rd.lvm.lv=centos/swap vconsole.font=latarcyrheb-sun16 rd.lvm.lv=centos/root \
					crashkernel=auto  vconsole.keymap=us rhgb quiet net.ifnames=0 biosdevname=0"

	Step2#
	$sudo grub2-mkconfig  -o /boot/grub2/grub.cfg
	<= Using “grub2-mkconfig” command to re-generate a new grub configuration file

	Step3#
	$sudo mv /etc/sysconfig/network-scripts/ifcfg-eno16777736  /etc/sysconfig/network-scripts/ifcfg-eth0
		<= Rename “Eno” network file using”mv”command


	Step4#$
	su vi /etc/sysconfig/network-scripts/ifcfg-eth0 <= configuration file and set 
							the value of “Name” field to “eth0".
	<strong>NAME=eth0</strong>

	Step5# 
	reboot system, after rebooting system, using “ifconfig” command check 
	network interface information again.
	
	


80.  Ubuntu apt-get update fail some of packages.
	W:Failed to fetch bzip2:
     	E:Some index files failed to download. 

	sudo rm -rf /var/lib/apt/lists/*
	sudo apt-get update



81. Check Kernel Version

	$ ls -al /etc/ld.so.conf.d

	kernel-2.6.32-431.11.2.el6.x86_64.conf
	
	$ uname -a				<= all info 
	
	$ uname -r 
	2.6.32-431.1.2.0.1.el6.x86_64
	
	
	# Results are all same	
	$ cat /boot/config-$(uname -r)  
	$ cat /boot/config-`uname -r`
	$ cat /boot/config-4.2.0-27-generic 
	

	# Ubuntu
    $ sudo dpkg -l | grep linux-headers | grep ii
    $ sudo dpkg -l | grep linux-headers | grep ii | awk '{print $3}'
	


# scp
---------------------------------------------------------------------------------------------
#!/bin/bash
set -e

tar -zcvf jenkins-proxy.tar.gz jenkins-proxy/
scp -v jenkins-proxy.tar.gz jproxy@10.10.100.130:/var/app/
ssh -v -t jproxy@10.10.100.130 'cd /var/app && tar -zxvf jenkins-proxy.tar.gz && \
			rm jenkins-proxy.tar.gz && chmod -R a+w jenkins-proxy/ && sudo service jenkins_proxy restart'
---------------------------------------------------------------------------------------------

82. rsync
	https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories-on-a-vps
	-v  --verbose
	-q  --quiet
	-a  --archive
	-p	--perm			<= preserver permissions
	-n  --dry-run
	-z  --compress
	
	--size-only
              This  modifies  rsync’s "quick check"
	
### LOCAL >>>> REMOTE ###  **PUSH to Remote**
	###			LOCAL file location  >>>>   REMOTE 'creating' remote folder
	rsync -avn  /home/apark/testFile  		apark@45.55.5.69:/home/apark/      
			<= -a, -v verbose -n dry-run
	rsync -av   ~/rtest          	 		apark@45.55.5.69:/home/apark/   
		
	###			Local files	  		 >>>>	Remote 'Copying' FILES in the remote folder 
	rsync -av   /home/apark/rtest/ 	 		apark@45.55.5.69:/home/apark/         <= copy to apark    
	rsync -av   ~/rtest/          	 		apark@45.55.5.69:/home/apark/rtest/   <= copy to rtest

	
### REMOTE >>>> LOCAL ###   **PULL to Local**
	###			REMOTE file & folder location	>>>>    LOCAL location		
	rsync -av   apark@45.55.5.69:/home/apark/			/home/apark/ 		   <= copy to apark    
	rsync -av   apark@45.55.5.69:/home/apark/rtest/ 	~/rtest/       	       <= copy to rtest
	rsync -av   apark@45.55.5.69:/home/apark/dir1    	~					   <= creating 'dir1' home folder
	rsync -av   apark@45.55.5.69:/home/apark/dir1    	~/					   <= home folder
	receiving incremental file list
	



### REMOTE  ===>>>>  LOCAL			  <Remote location>						<Local location>
	$ rsync -azP  --exlude=dir* 	     apark@45.55.5.69:/home/apark/     	 	~   	
	$ rsync -azP  --exclude={dir1,dir2}  apark@45.55.5.69:/home/apark/       	~/		
	
	
	$ rsync -avzp ssh  --progress jenkins@10.100.200.41:/Users/Shared/Jenkins/Home/jobs/Chroma* 	/tmp
	$ rsync -avzp --progress      jenkins@10.100.200.41:/Users/Shared/Jenkins/Home/jobs/Chroma* 	/tmp
	
	
	
	Will it work????
	$ rsync -avzp --progress --exclude={Perforce*,Library*,2015*,Mono*,Out*,Back*,Vill*,Strange*} jenkins@10.100.200.41:/Users/Shared/Jenkins/* 	/Users/Shared/Jenkins/
	
rsync -avzp --progress --exclude={Perforce*,Library*,2015*,Mono*,Out*,Back*,Vill*,Strange*,\
Install*,*.dmg,backup_log,tmp*,test*} jenkins@10.100.200.41:/Users/Shared/Jenkins/*  /Users/Shared/Jenkins/



	
	
### CAN NOT do REMOTE >>>> 2 REMOTES ###
		Server A  <-Rsync-> XX Server B XX <-Rsync-> Server C
		http://unix.stackexchange.com/questions/183504/how-to-rsync-files-between-two-remotes
		Cannot use rsync with a remote source and a remote destination. 
		Assuming the two servers can't talk directly to each other, 
			it is possible to use "ssh to tunnel" via your local machine.

	$ ssh -R localhost:50000:host2:22   host1 'rsync -e "ssh -p 50000" -var /var/www localhost:/var/www'
	
	
	# Options	
	rsync -avz	source 		destination		<= -z Compression
	rsync -avzP	source 		destination	 	<= -P Progress

	Copy a File from a Remote Server to a Local Server with SSH
	To specify a protocol with rsync you need to give “-e” option with protocol name you want to use. 
	Here in this example, We will be using “ssh” with “-e” option and perform data transfer.
	$ rsync -avzhe ssh root@192.168.0.100:/root/install.log     /tmp/	
	
	
### Delete Destination folder has different than Source folder
	$ rsync -az -e --delete ssh /cdn/data/  apark@10.128.34.80:/cdn/data/
		
		# -e option is used to run a different remote shell
		http://unix.stackexchange.com/questions/232041/rsync-permission-denied-13-while-executing-rsync-as-a-root
	
		--delete
		https://unix.stackexchange.com/questions/203846/how-to-sync-two-folders-with-command-line-tools
	


	$ rsync -a --exclude=pattern 					source 		destination
	$ rsync -a --exclude=pattern --include=pattern 	source 		destination
	$ rsync -az -e(--ignore-existing) 				source		destination 
	
	
	# LOCAL >>> REMOTE
	$ rsync -azP  /home/apark/rtest/  	apark@45.55.5.69:/home/apark/rtest2 
	
	
	
	
	---
    rsync files in rtest folder
	rsync -av      /home/apark/rtest/   apark@45.55.5.69:/home/apark/rtest

	rsync -av iptables.vm5-sgas           remote_host:/etc/sysconfig
	rsync -av rog sgas sg-rog:/home

    rsync -av /etc/hosts                  remote_host:/etc
    rsync -av /etc/sysconfig/iptables     remote_host:/etc/sysconfig
    
	rsync -av   lobby1:/root/       /root --exclude ".ssh"

	ssh sg-eu "service iptables restart"



### Rsync issues ### 
### rsync failed due to remote host doesn't have the program installed.
	$rsync -av /RRServerLinux/ apark@10.128.8.174:/home/apark/RRServerSlave/
		apark@10.128.8.174's password:
		bash: rsync: command not found
		rsync: connection unexpectedly closed (0 bytes received so far) [sender]
		rsync error: remote command not found (code 127) at io.c(600) [sender=3.0.6]
		### FIX, Install rsync on REMOTE ###

	### S3CMD  
	# https://tecadmin.net/s3cmd-file-sync-with-s3bucket/
	
	$ s3cmd sync --dry-run --delete-removed --skip-existing s3://mobile-xpromo/  /cdn/data/
	
	# If file has been modified but suing same file name, remove the '--skip-existing' option(using date, size, md5sum)
	$ s3cmd sync --dry-run --delete-removed s3://mobile-xpromo/  /cdn/data/
	
	
	# Directory(include files) upload
		$ s3cmd sync -recursive local_dir1   s3://remote_dir1/
		local_dir1/file{1..10}  =>  s3://remote_dir1/local_dir1/file{1..10}
	
	# Files upload
		$ s3cmd put -r local_dir1/   s3://remote_dir1/
		local_dir1/file{1..10}  =>  s3://remote_dir1/file{1..10}
	
	
	
	
	
	
### IFS (Internal Field Separator)###
http://unix.stackexchange.com/questions/16192/what-is-ifs-in-context-of-for-looping
IFS isn't directly related to looping, it's related to word splitting. IFS indirectly determines how 
the output from the command is broken up into pieces that the loop iterates over.
When you have an unprotected variable substitution $foo or command substitution $(foo), there are 
two cases:
•	If the context expects a single word, e.g. when the substitution is between double quotes "$foo", 
	or in a variable assignment x=$foo, then the string resulting from the substitution is used as-is.
•	If the context expects multiple words, which is the case most of the times, then two further .
	expansions are performed on the resulting string:
•	The string is split into words. Any character that appears in $IFS is considered a word separator. 
	For example IFS=":"; foo="12:34::78"; echo $foo prints 12 34  78 (with two spaces between 34 and 
	78, since there's an empty word).
•	Each word is treated as a glob pattern and expanded into a list of file names. For example, 
	foo="*"; echo $foo prints the list of files in the current directory.

	For loops, like many other contexts, expect a list of words. So
	for x in $(foo); do …
	breaks $(foo) into words, and treats each word as a glob pattern. The default value of IFS isspace,
	tab and newline, so if foo prints out two lines hello world and howdy then the loop body is 
	executed with x=hello, then x=world and x=howdy. If IFS is explicitly changed to contain a newline 
	only, then the loop is executed for hello world and howdy. If IFS is changed to be o, then the 
	loop is executed for hell,  w, rld␤h (where ␤ is a newline character) and wdy.

	





### SSH TUNNEL
# origin
	$ ssh -N -R 8822:localhost:22 remote.host.com
	The optional -N says "don't execute a command" (helpful to prevent accidents caused by leaving 
	remote shells laying around.)

	Now from remote, you can SSH to host1 like this: (The remote port 8822 forwards to host1, but 
	only on the loopback interface.)

# remote
	$ ssh -p 8822 localhost
	For extra credit, you can export the forwarding to the whole world, allowing anyone get to host1 
	by hitting remote's port 8822. (Note the extra initial colon)

# origin
	$ ssh -N -R :8822:localhost:22 remote.host.com



	
	
############### 
# System Info #
###############

1. $ ssh -i .ssh/pem_key 		user_id@IP_Address			# -i uses file

   $ ssh -p <port>       		user_id@IP_Address			# -p specify port

   $ ssh-copy-id 	       		user_id@IP_Address			# copy .ssh/id_rsa.pub to remote
   
-------------------------  
# ssh-agent vs ssh-add  #
-------------------------
https://www.ssh.com/ssh/agent
	
  # SSH-AGENT 	<= SINGLE SIGN-ON USING SSH
	ssh-agent is a program to hold private keys used for public key authentication(RSA, DSA, ECDSA, Ed25519).  
	ssh-agent is usually started in the beginning of an X-session or a login session, and all other windows 
		         or programs are started as clients to the ssh-agent program. 
	
  #	ssh-add
	 ssh-add — adds private key identities to the authentication agent

  
   $ ssh-agent bash
   $ ssh-add
   $ ssh-add --help
   
   https://stackoverflow.com/questions/22272299/what-is-the-difference-between-ssh-add-and-ssh-agent
   So you can give your passphrase to 'ssh-agent' once and it will use it whenever required. 
   You use 'ssh-add' to give your keys to 'ssh-agent'. You can always check what all keys your 
   'ssh-agent' is managing by issuing '$ ssh-add -l '.
   
   https://superuser.com/questions/284374/ssh-keys-ssh-agent-bash-and-ssh-add
   
	$ ps							<= Currnet user process
	  PID TTY          TIME CMD
	16818 pts/1    00:00:00 bash
	16841 pts/1    00:00:00 ps
	
	$ ssh-agent bash && ssh-add ~/.ssh/id_rsa
	
    $ ps
		PID TTY          TIME CMD
	16818 pts/1    00:00:00 bash
	16849 pts/1    00:00:00 bash		<= A New BASH Shell created
	16867 pts/1    00:00:00 ps
	
	###the agent would remain running, but inaccessible###

	
	$ eval 	<= The args are read and concatenated together into  a  single  command.   This
               command  is  then  read  and  executed  by the shell, and its exit status is
               returned as the value of eval.  
			   If there are no args, or  only  null  arguments, eval returns 0.
			$ eval `something`
			$ echo $?
				0   <= null arguments
				
	# If ssh-agent is not automatically started at login, it can be started manually						  
    $ eval `ssh-agent` 	<= this runs the agent in background, and sets the apropriate 
						   environment variables for the current shell instance.
	  Agent pid 17599	<= return arguments
						   
								   
	
	###	automatically kills ssh-agent when you close the terminal window###
	$ exec ssh-agent bash 		<= starts a new instance of the 'bash shell', replacing the 
									current one.	
		$ ps
			PID TTY          TIME CMD
		16818 pts/1    00:00:00 bash
		17198 pts/1    00:00:00 ps
	$ exec ssh-agent bash
		$ps
			PID TTY          TIME CMD
		16818 pts/1    00:00:00 bash			<=SAME BASH SHELL PROCESS ID will be killed when close
		17222 pts/1    00:00:00 ps
	------------------------------------------------------------
	$ exec ssh-agent bash && ssh-add ~/.ssh/id_rsa		<= use this 
	------------------------------------------------------------
		$ ps
			PID TTY          TIME CMD
		16818 pts/1    00:00:00 bash			<= NO New PID
		17553 pts/1    00:00:00 ps

		$ ssh-add -l							<= check the list

	### Edit your ~/.bashrc file, and add ###
	-----------------------------------------------------------------------------------------
		ssh-add &>/dev/null || eval `ssh-agent` &>/dev/null # start ssh-agent if not present
		[ $? -eq 0 ] && {                                   # ssh-agent has started
		ssh-add ~/.ssh/your_private.key1 &>/dev/null        # Load key 1
		ssh-add ~/.ssh/your_private.key2 &>/dev/null        # Load key 2
		}
		-----------------------------------------------------------------------------------------

   # CentOS sudo without passwd
   
	$ sudo visudo   (old way   vi /etc/sudoers)
	  nozatech ALL=(ALL:ALL) NOPASSWD:ALL
	  %sudo    ALL=(ALL:ALL) NOPASSWD:ALL
	
	# Run specific 'script' without password prompt! 
	$ username  ALL=(ALL) NOPASSWD: /home/username/pydatertc.sh	
	
	# Ubuntu sudo without passwd
	$ sudo visudo
 		# User privilege specification
		root    ALL=(ALL:ALL) ALL
		apark   ALL=(ALL:ALL) NOPASSWD:ALL									<= Add NOPASSWD
		# Members of the admin group may gain root privileges
		%admin ALL=(ALL) NOPASSWD:ALL										<= Add NOPASSWD
		# Allow members of group sudo to execute any command
		%sudo   ALL=(ALL:ALL) NOPASSWD:ALL									<= Add NOPASSWD

	$ visudo -cf %s  	<= validate 

 # User has to re-login after add to a new Group 
	https://superuser.com/questions/122744/is-there-a-way-in-linux-to-update-the-user-group-properties-without-having-to-lo
	 A user does not exist as an object at the kernel level; only processes (and files) do. 
	 A process has a uid (effective and whatnot) and a list of group ids.

	 When you are adding a user to a group, the kernel has no idea what it means. 
	 It only knows, indirectly, that the next time /bin/login or /usr/bin/newgrp 
	 is run a process with that user id will have a new group id in its list.
	
	
	# Allowing/Denying User-Level Cron
	$ /etc/cron.allow

2 Processor
	killall <process name>
    kill -9 
	kill 15
	pkill <pid>  		<		= Kill all child processes, too.
	ps -u <user_id>				<= u (user)
		
		
# OS TYPE
Linux CentOS or Ubuntu Version check	
	
    $ cat /etc/*rel*
    $ uname -a				<= all include kernel, processor(32/64bit), hostname, 
    $ uname -r 				<= kernel version
    $ cat /proc/version
	$ echo $OSTYPE
	$ lsb_release -d | awk -F"\t" '{print $2}'
	$ cat /etc/os-release
	$ cat /etc/centos-release
	$ cat /etc/lsb-release
	
	$ cat /etc/*-release | grep -w NAME | cut -d= -f2 | tr -d '"'
	
    ---------------------------------------------------------------------
	#!/bin/bash
	DISTRO=$(cat /etc/*-release | grep -w NAME | cut -d= -f2 | tr -d '"')
	echo $DISTRO
	---------------------------------------------------------------------
	
	###Install lsb_release on CentOS7
	$ yum provides */lsb_release			
	$ yum install redhat-lsb 
	$ yum install redhat-lsb-core
	$ lsb_release -a
   

    $ yum provides
	When you install sw and got a message saying that "xxx requires xxx library and xxx are missing."
	Then run 'yum provides "xxx"' to check dependencies.
	
	$ lsb_release -a						<= lsb <-Linux Standard Release
		No LSB modules are available.
		Distributor ID: Ubuntu
		Description:    Ubuntu 16.04.4 LTS
		Release:        16.04
		Codename:       xenial
	
	How to detect the OS from a Bash script?
    https://stackoverflow.com/questions/394230/how-to-detect-the-os-from-a-bash-script
   
	#!/bin/bash
	
	if [[ "$OSTYPE" == "linux-gnu" ]]; then
        # ...
	elif [[ "$OSTYPE" == "darwin"* ]]; then
        # Mac OSX
	elif [[ "$OSTYPE" == "cygwin" ]]; then
        # POSIX compatibility layer and Linux environment emulation for Windows
	elif [[ "$OSTYPE" == "msys" ]]; then
        # Lightweight shell and GNU utilities compiled for Windows (part of MinGW)
	elif [[ "$OSTYPE" == "win32" ]]; then
        # I'm not sure this can happen.
	elif [[ "$OSTYPE" == "freebsd"* ]]; then
        # ...
	else
        # Unknown.
	fi
   
    #!/bin/bash
    if [ -f /etc/redhat-release ]; then
		echo "CentOS"
	fi
	if [ -f /etc/lsb-release ]; then
		echo "Ubuntu"
	fi
   
   
   
   
4. ulimit 		
	system wide, user resource limits
	 Provides control over the resources available to the shell and to processes
     started by it, on systems that allow such control.  The -H and  -S  options
     specify  that the hard or soft limit is set for the given resource. 
			  
	OS needs memory to manage each open file
	$ ulimit -a							<= All current limits are reported
	$ ulimit -n 						<= *** The maximum number of open file *****
	$ lsof | wc -l						<= Count all opened files by all process
	$ cat /proc/sys/fs/file-max			<= Get maximum open files count allows
	$ cat /proc/sys/fs/file-nr			<= to get the current number of open files from the Linux kernel's point of view
	
	Example: This server has 40096 out of max 65536 open files, although lsof reports a much larger number:
	$ cat /proc/sys/fs/file-max
		65536
	$ cat /proc/sys/fs/file-nr 
		40096   0       65536
	$ lsof | wc -l
		521504
	
	### System-wide File Descriptors (FD) Limits  ###
	
	The Number Of Maximum Files Was Reached, How Do I Fix This Problem?????
	$ cat /proc/sys/fs/file-max
	  65536
	
	$ sudo sysctl -w fs.file-max=800000             <= change from 65536 to 800000
	# (or) $ echo 800000 > /proc/sys/fs/file-max
	
	$ cat /proc/sys/fs/file-max
	  800000
	
	$ vi /etc/sysctl.conf 
		fs.file-max=800000			<= append so that after reboot, still load 800000
	#(or) $ echo 'fs.file-max=800000" >> /etc/sysctl.conf
	#exit & relogin
	
	$ sudo sysctl -p
	
	$ cat /proc/sys/fs/file-max
		800000
	# (or)$ sysctl fs.file-max
		fs.file-max = 800000
	
	### User Level FD Limits
	$ cat /etc/security/limits.conf
	# for user 
	apark  -  nofile    64000
	
	# for all users
	*    soft    nofile 8192
	*    hard    nofile 8192
	
#---------------------------------------------------------------------------------------------------------	
	### Ulimit ### There are two limits in play: 
#-----------------------------------------------------------------------------------------------------
	1) the maximum number of open files the OS kernel allows (fs.file-max) and
	2) the per-user limit (ulimit -n). The former(Kernel) must be higher than the latter(User).
	
	
	If you are getting error “Too many open files (24)” then your application/command/script is hitting max 
	open file limit allowed by linux. You need to increase open file limit as below:
	
	### Check
	$ ulimit -a			<= All current limits
	$ ulimit -n			<= maximum number of open file
	$ ulimit -Sn		<= soft limit number
	$ ulimit -Hn		<= hard limit number
	
	### Current shell
	$ ulimit -n 1000		<= change
	$ ulimit -n 			<= check
	
	
	$ cat /proc/sys/fs/file-max
	 784694	
	
	### setting a new value in kernel variable /proc/sys/fs/file-max
	$ sysctl -w fs.file-max=100000   					 <= -w write
	
	$ vi /etc/sysctl.conf		
		fs.file-max = 100000			<= add
		## $ echo "fs.file-max = 100000"  >>  /etc/sysctl.conf
	
	$ sysctl -p		( or --load)						 <= load 
	  fs.file-max = 100000
	
	$ cat /proc/sys/fs/file-max 						 (or  $ sysctl fs.file-max )
	

	### Specific user limits ### 
	$ vi /etc/security/limits.conf		<= add this lines
	#<domain>   <type>  <item>         <value>
		*       soft     nproc          65535		<= * all users but no root
		*       hard     nproc          65535
		*       soft     nofile         65535
		*       hard     nofile         65535
		apark    -       nofile         65535		<= apark specific
		root     -       nofile         65535		<= root specific
		
		*** Relog-in to apply changes ***
	
		
	### Per-User Limit
	
	$ cat /etc/security/limits.conf
	#<domain>      <type>  <item>         <value>
	#root            hard    core            100000		<=100k files to open
	#@student        hard    nproc           20			<= only 20 files to open
	
	### pam-limits							<= session-related modules common to all services
	$ cat /etc/pam.d/common-session 		
	
	### System-Wide Limit
	$ echo 'fs.file-max = 2097152' >> /etc/sysctl.conf 			<= configure kernel parameters at runtime
	
	# configure kernel parameters at runtime
	$ sysctl -p 						sys						<= -p Load in sysctl settings from the file 
																	specified or /etc/sysctl.conf 
											
	### Verify New Limits
	$ cat /proc/sys/fs/file-max 			<= max limit of file descriptors:
	2097152
		
	### Check limit for other user
	$ su - www-data -c 'ulimit -aHS' -s '/bin/bash'
	$ su - user_ID  -c 'ulimit -aHS' -s '/bin/bash'
		core file size          (blocks, -c) 0
		data seg size           (kbytes, -d) unlimited
	
	
	### Check limits of a running process:
	$ cat /proc/Process_ID/limits
		Limit                     Soft Limit           Hard Limit           Units
		Max cpu time              unlimited            unlimited            seconds
	
	
	####
	$ echo 'fs.inotify.max_user_watches=100000' | sudo tee -a /etc/sysctl.conf; 			<=tee -a(append)
	$ sudo sysctl -p
	####06
	
	https://help.ubuntu.com/community/RootSudo
	$ ls | sudo tee -a /root/somefile 
	
	# pass the whole command to a shell process run under sudo to have the file written to with root permissions
	$ sudo sh -c "ls > /root/somefile"
	#-----------------------------------------------------------------------------------------------------

	### is my ulimit exceeded?
	Under high load it happens from time to time that network connections fail although everything they work 
	perfectly under low load. The reason be the ulimit set on a process. A ulimit restricts the process from 
	opening up more files (and network connections) than a certain number.
	http://www.linuxintro.org/wiki/Is_my_ulimit_exceeded
	$ ulimit -a
	
	$ You can permanently set the limits in /etc/security/limits.conf
	# to check
	$ ulimit -n 
		1024  			<= default max file open value
	$ ps -A (or -e)  select All processes
	  PID TTY          TIME CMD
		1 ?        00:00:10 systemd
		2 ?        00:00:00 kthreadd

	
###  Nagios
	systemctl start nagios.service
	systemctl start httpd.service
	systemctl enable httpd.service		<= Apache in startup
	ln -s '/usr/lib/systemd/system/httpd.service'  '/etc/systemd/system/multi-user.target.wants/httpd.service'

###  NignX Status page 
	# nginx -V 2>&1| grep -o http_stub_status_module
	
	
	
### AWS Access Key/ Secret key
	
	/home/ubuntu/jarvis/ami/tcg/production/
	
    $ sudo sh -c "export OC_AWS_ACCESS_KEY=AKIAI6FBDACL5LFOEACA; \
				  export OC_AWS_SECRET_KEY=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; \
				  fab tcg.credit_gems:host=10.10.1.50,player_id="544053a5c66354d43f5f07d3",amount=50"
		
	To make a usage listing of the directories in the /home partition.  Note that this runs the commands in a
    sub-shell to make the cd and file redirection work.

    $ sudo sh -c "cd /home ; du -s * | sort -rn > diskUsage"   <= diskUsage file located in /home
	
	
#### Ubuntu  ###	
http://stackoverflow.com/questions/21515463/how-to-increase-maximum-file-open-limit-ulimit-in-ubuntu

echo "* soft nofile 102400" > /etc/security/limits.d/*_limits.conf && \
echo "* hard nofile 102400" >> /etc/security/limits.d/*_limits.conf 
####	
	
	$ man bash  					
	/ulimit  				<= ulimit is built in shell
	-S   Change and report the soft limit associated with a resource. 
    -H   Change and report the hard limit associated with a resource. 
    -a   All current limits are reported. 
    -c   The maximum size of core files created. 
    -d   The maximum size of a process's data segment. 
    -f   The maximum size of files created by the shell(default option) 
    -l   The maximum size that can be locked into memory. 
    -m   The maximum resident set size. 
    -n   The maximum number of open file descriptors. 
    -p   The pipe buffer size. 
    -s   The maximum stack size. 
    -t   The maximum amount of cpu time in seconds. 
    -u   The maximum number of processes available to a single user. 
    -v   The maximum amount of virtual memory available to the process. 
	###
	
#-------------------------------------------------------------------------------------------------------------
	
	
5. sysctl  			<= Configure kernel parameters at runtime
	The ulimit and sysctl programs allow to limit system-wide resource use. 
	This can help a lot in system administration, e.g. when a user starts too 
	many processes and therefore makes the system unresponsive for other users.

	
	
	
	
6. Swap space swapon
	swapon -s 		<= Summary
		   -a 		<= All
	
	sudo dd if=/dev/zero of=/var/swapfile bs=1M count=2048
	sudo chmod 600 /var/swapfile
	sudo mkswap /var/swapfile
	echo /var/swapfile none swap defaults 0 0 | sudo tee -a /etc/fstab
	sudo swapon -a

  
	sudo dd if=/dev/zero of=/mnt/{filename}.swap bs=1M count={swap_size}
	sudo mkswap /mnt/{filename}.swap
	sudo swapon /mnt/{filename}.swap
	sudo vi /etc/fstab
	Add the following text at the end of the file, 
	/mnt/{filename}.swap  none  swap  sw  0 0
  

  ### Create a swap space script ### 
	#!/bin/sh
	# make swap space s
	dd if=/dev/zero of=/swapfile bs=1024 count=8388608
	mkswap /swapfile
	echo '/swapfile         swap            swap    defaults 0 0' >> /etc/fstab
	swapon -a
	swapon -s
	###
  
  ### creating random data for 10MB # dd <= covert and copy a file
    dd if=/dev/urandom of=foo bs=1000 count=10000     
		^ if=FILEread from FILE instead of stdin
		                ^ of=FILE, write to FILE instead of stdout
							  ^ bs=bytes, read and write up to BYTES bytes at a time



  
7. Start Up service for Ubuntu
  
	sudo mv /filename /etc/init.d/
	sudo chmod +x /etc/init.d/filename 
	sudo update-rc.d filename defaults 
	
	
# UID
	# Finding user UID
	$ id				<=  print real and effective user and group IDs
	$ id -u apark		<= User ID
		1000
	     -g, --group    <= print only the effective group ID
		 -G, --groups	<= print all group IDs
		 -u, --user		<= Effective user ID

8. UUID  <= identifier for block devices. 
	UUIDs are 128 bit long numbers represented by 32 hexadecimal digits and which are used in 
	software development to uniquely identify information with no further context. 
	
	#Linux implementation and generation
	 In Linux UUIDs are generated in /drivers/char/random.c?id=refs/DATEs/v3.8, and you can generate new ones via proc:
	# Usage in fstab
	As mentioned UUIDs are most often used in Linux to identify block devices. Imagine, you have a couple of hard disks 
	attached via USBs, than there is no persistent, reliable naming of the devices: sometimes the first USB hard disk is 
	named “sda”, sometimes it is named “sdb”. So to uniquely address the right disk for example in your /etc/fstab, 
	you have to add an entry like:
	
	UUID=9043278a-1817-4ff5-8145-c79d8e24ea79 /boot ext3 defaults 0 2

	For the block device itself, the uuid is stored in the superblock.
	Beware however that UUIDs should not be used in fstab when you work with LVM snapshots(no 2 device using same uuid). 
	
	
	
	$ cat /proc/sys/kernel/random/uuid
		eaf3a162-d770-4ec9-a819-ec96d429ea9f
	
	# There is also the library libuuid which is used by uuidgen and especially 
		by the ext2/3/4 tools E2fsprogs to generate UUIDs:
	$ uuidgen 
	f81cc383-aa75-4714-aa8a-3ce39e8ad33c
	
	# bash style
	$ls -l /dev/disk/by-uuid
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 116a4716-c5ec-4ce5-aea4-9fea29d78f76 -> ../../dm-1
	lrwxrwxrwx. 1 root root  9 Apr 13 03:17 2015-12-09-23-03-16-00 -> ../../sr0
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 3479cc28-9e7d-4798-abef-a50bafef8761 -> ../../dm-0
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 62f1fbbf-e39b-4140-90ed-3d88a7988fa5 -> ../../sda1

	
	$sudo blkid /dev/sda1
	/dev/sda1: UUID="62f1fbbf-e39b-4140-90ed-3d88a7988fa5" TYPE="xfs"
	
	$ udevadm info -q all -n /dev/sda1|grep uuid
	S: disk/by-uuid/62f1fbbf-e39b-4140-90ed-3d88a7988fa5
	E: DEVLINKS=/dev/disk/by-path/pci-0000:00:10.0-scsi-0:0:0:0-part1 /dev/disk/by-uuid/62f1fbbf-e39b-4140-90ed-3d88a7988fa5
	
	### When not to use UUID ###
	Since it is not possible to mount two file systems with the same UUID, extra care need to be taken 
	when LVM snapshots (or cloned disks) are used in an environment: mounting might fail due to duplicate UUIDs.
	
	XFS: Filesystem dm-2 has duplicate UUID – can’t mount
	
	One way to deal with this is by the way to change the UUID during creation or afterwards, 
	another way is to mount with the nouuid option.

### Package List
	# Ubuntu
	$ dpkg -l | grep mysql
	
	# CentOS
	$ rpm -qa | grep mysql
	
	


#########################
# 5. PROCESS HANDLING.  #
#########################

To suspend a job, type CTRL+Z while it is running. You can also suspend a job with CTRL+Y. 
This is slightly different from CTRL+Z in that the process is only stopped when it attempts to 
read input from terminal. Of course, to interupt a job, type CTRL+C.


Linux_CMD &  # runs job in the background using '&' at the end and prompts back the shell

jobs         # lists all jobs (use with -l to see associated PID)

fg           # brings a background job into the foreground
fg %+        # brings most recently invoked background job
fg %-        # brings second most recently invoked background job
fg %N        # brings job number N
fg %string   # brings job whose command begins with string
fg %?string  # brings job whose command contains string

kill -l      # Options, returns a list of all signals on the system, by name and number
kill PID     # terminates process with specified PID
killall
pkill

$ pgrep nginx		# pgrep, pkill - look up or signal processes based on name and other attributes
$ pkill nginx		# kill all child process that spawned from nginx process id

$ pgrep -u apark	# process grep all apark's process
$ pkill -u apark	# kill all apark's process


kill 	PID			# same as 'kill 15'
kill 15 PID			# gracefully shutdown (terminate)
kill 9	PID			# force to kill
killall name 		#




### kill all processes relate to same process name
	$ pkill -f s3cmd
	$ ps -ef | grep myProcessName | grep -v grep | awk '{print $2}' | xargs kill -9


	ps           # prints a line of information about the current running login shell and any processes running under it
	ps -a        # selects all processes with a tty except session leaders

### ps -ef	vs ps aux
		Unix, BSD and Debian
		ps aux 				<= BSD system		(Ubuntu, Debian) 	
		ps -ef 				<= System V system	(Redhat, CentOS) <= Unix family
EXAMPLES
       To see every process on the system using standard syntax: <= Unix family
          ps -e
          ps -ef
          ps -eF
          ps -ely

       To see every process on the system using BSD syntax:
          ps ax
          ps axu

       To print a process tree:
          ps -ejH
          ps axjf

       To get info about threads:
          ps -eLf
          ps axms

       To get security info:
          ps -eo euser,ruser,suser,fuser,f,comm,label
          ps axZ
          ps -eM

       To see every process running as root (real & effective ID) in user format:
          ps -U root -u root u

       To see every process with a user-defined format:
          ps -eo pid,tid,class,rtprio,ni,pri,psr,pcpu,stat,wchan:14,comm
          ps axo stat,euid,ruid,tty,tpgid,sess,pgrp,ppid,pid,pcpu,comm
          ps -Ao pid,tt,user,fname,tmout,f,wchan

       Print only the process IDs of syslogd:
          ps -C syslogd -o pid=

       Print only the name of PID 42:
          ps -q 42 -o comm=

		

	$ ps aux | grep RRServer | grep -v grep | awk '{print $2}'
	

	$ ps  **options
	********* simple selection *********  ********* selection by list *********
	-A all processes                      -C by command name
	-N negate selection                   -G by real group ID (supports names)
	-a all w/ tty except session leaders  -U by real user ID (supports names)
	-d all except session leaders         -g by session OR by effective group name
	-e all processes                      -p by process ID
	T  all processes on this terminal     -s processes in the sessions given
	a  all w/ tty, including other users  -t by tty
	g  OBSOLETE -- DO NOT USE             -u by effective user ID (supports names)
	r  only running processes             U  processes for specified users
	x  processes w/o controlling ttys     t  by tty
	*********** output format **********  *********** long options ***********
	-o,o user-defined  -f full            --Group --User --pid --cols --ppid
	-j,j job control   s  signal          --group --user --sid --rows --info
	-O,O preloaded -o  v  virtual memory  --cumulative --format --deselect
	-l,l long          u  user-oriented   --sort --tty --forest --version
	-F   extra full    X  registers       --heading --no-heading --context
    ********* misc options *********
	-V,V  show version      L  list format codes  f  ASCII art forest
	-m,m,-L,-T,H  threads   S  children in sum    -y change -l format
	-M,Z  security data     c  true command name  -c scheduling class
	-w,w  wide output       n  numeric WCHAN,UID  -H process hierarchy

	
## kill pts (pseudo terminal device  e.g. xterm, screen, ssh)
	A tty is a Regular Terminal Device (the console on your server).
	A pts is a Psuedo  Terminal Slave  (ssh, xterm, screen connection).
	
	$ who

		apark    pts/1        2017-08-01 18:32 (65.87.26.124)
		apark    pts/0        2017-08-01 16:25 (65.87.26.124)   <= need to kill!!!
	
	$ who -la
		           system boot  2019-11-19 21:12
           run-level 5  2019-11-19 21:12
			LOGIN      ttyS0        2019-11-19 21:12               853 id=tyS0
			LOGIN      tty1         2019-11-19 21:12               866 id=tty1
			apark    + pts/0        2019-11-19 21:13 01:55        1130 (74.217.69.194)
			apark    + pts/1        2019-11-19 21:17 01:44        1130 (107.0.238.61)
			apark    + pts/2        2019-11-19 21:37 01:23        1831 (107.0.238.61)
			apark    + pts/3        2019-11-19 21:49 01:17        1966 (74.217.69.194)
			apark    + pts/4        2019-11-19 22:42   .          2267 (74.217.69.194)
	
	# Kill TTY/PTS	
	  $ kill -9 1130 1130 ....
	
	$ ps -ft pts/0				<= -f <-full-format listing,  -t <- by tty
								
		UID        PID  PPID  C STIME TTY          TIME CMD
		apark    28110 28109  0 16:25 pts/0    00:00:00 -bash
		root     28129 28110  0 16:25 pts/0    00:00:00 sudo su
	
	$ kill -9  28110 28129 
	$ who
		apark    pts/1        2017-08-01 18:32 (65.87.26.124)

# kill or terminate unwanted tty/pts sessions
	https://www.crybit.com/kill-unwanted-tty-unix/
	PTS <- stands for pseudo terminal.
	
	# Who is doing what
	who <- show who is logged on
	w	<- Show who is logged on and what they are doing.
	who -la		<- l(login) -a(all)
	
	# your tty/pts session
	tty		<- print the file name of the terminal connected to standard input
	ps 		<- 
	finger	<- user information lookup program

		
###  How to kill zombie processTo suppress error message
		A zombie is already dead, so you cannot kill it. To clean up a zombie, it must be waited on by its parent, 
		so killing the parent should work to eliminate the zombie. (After the parent dies, the zombie will be 
		inherited by init or systemD (process ID 1), which will wait on it and clear its entry in the process table.) 
		If your daemon is spawning children that become zombies, you have a bug. Your daemon should notice 
		when its children die and wait on them to determine their exit status.

		Example command:
		kill $(ps -A -ostat,ppid | awk '/[zZ]/{print $2}')

			
###	
	Zombie processes (also show as <defunct>), aren't real processes at all. They are just entries in the kernel process table. 
	This is the only resource they consume. They do not consume any CPU or RAM. 
	*** The only danger of having zombies, is running out of space in process table!!!***
	
	$ cat /proc/sys/kernel/threads-max 
		30112
		
	They appear only when their parent process (i.e. process which fork()'ed them) is alive, but did not yet call wait() 
	system function. Once parent dies, the zombies are wait()'ed for by INIT(1st process) and disappear.
###


trap cmd sig1 sig2    # executes a command when a signal is received by the script
trap ""  sig1 sig2    # ignores that signals
trap -   sig1 sig2    # resets the action taken when the signal is received to the default

disown <PID|JID>      # removes the process from the list of jobs

wait                  # waits until all background jobs have finished



6. TIPS AND TRICKS.

# to quickly go to a specific directory
	$ cd; nano .bashrc
	> shopt -s cdable_vars
	> export websites="/Users/mac/Documents/websites"

	$ source .bashrc
	$ cd websites
	$ cd *				<= It works only if there is only one folder


############################################
###
############################################
1. Find and Sort Files Based on Modification Date and Time
	$ ls -lt		<= Modification Time
	$ ls -ltr		<= Modification Time but Reverse
	$ ls -ln		<= by name	
	$ ls -lnr		<= by name	Reverse
	
	B. List Files Based on Last Access Time
		


1. Grep 
	# grep  pattern  file_name  #search for pattern in files
	$ grep -i					# Case insensitive search
	$ grep -r 					# Recursive
	$ grep -r "port 80" /etc/httpd/*   <= search subdirectories with key words port 80.
	$ grep -v httpd				# Inverted search "everything but httpd"
	$ grep -o					# Show matched part of file only
	$ grep -v grep    			# don't match the grep word
    To remove the grep in search word, use "grep -v grep" 2nd grep is a word to -v <= don't match word .
	
	$ ps aux | grep some_PID => out puts grep word in list, and it counts the line as 1 + 1 actual process name some_PID
	$ pgrep PID				<= short



	
	$ grep A 1 B 1 -i "finding_word"  file.txt
		# B 1 < -before line 
		# A 1 <- after line 
	
	-B, --before-context=NUM  print NUM lines of leading context
	-A, --after-context=NUM   print NUM lines of trailing context
	-C, --context=NUM         print NUM lines of output context
	-NUM                      same as --context=NUM





#####################################################################
mkdir /var/www/html/status && chown apark: /var/www/html/status
#####################################################################
#!/bin/bash
# Check and put ok status if the RRS game service is running for Lobby 7504 & 7505 services

if (( $(ps aux | grep ./RRServer_0617_7504 | grep -v grep | wc -l) > 0  &&  \
$(ps aux | grep ./RRServer_0617_7505 | wc -l) > 0 ));
        then
        echo OK > /var/www/html/status/index.html;
else rm /var/www/html/status/index.html 2>&1 /dev/null;
fi
###############################################################



------------------------------------------------------------------------------------
# Find 
------------------------------------------------------------------------------------
	# Find biggest | largest files top 5 list Only



# NCurses Disk Usage	<= Utility
	$ ncdu /
	$ ncdu -x /    <= scan a full filesystem
		# Since scanning a large directory may take a while, you can scan a directory and 
			export the results for later viewing:
	
	$ ncdu -1xo- / | gzip > ncdu-list.gz
	 ...some time later...
	$ zcat ncdu-list.gz | ncdu -f-				<= read scanned list as a file
	
	
	$ find    / -type f -exec du -Sh {} + | sort -rh | head -n 10  				<= Top 10 list
	$ find /*/* -type f -exec du -Sh {} + | sort -rh | head -n 5

	### Top largest files (>100M) on your filesystem
	$ find / -xdev -type f -size +100M -exec du -sh {} ';' | sort -rh | head -n10 
			# -xdev  Don't descend directories on other filesystems.
	
	$ find / -xdev -type f -size +100M -exec ls -la {} \; | sort -nk 5
	
	$ find / -xdev -type f -size +100M
	
	
	$ du -ahx / | sort -rh | head -10		<= -ahx <-all, human, skip dirs on diff file systems
											<= -rh 	<- reverse, human
	
	
	$ find /dir/ -user 	user_ID			#Find files owned by name in dir
	$ find /dir/ -name 	file_name
	$ find /dir/ -iname fine_name
	


	$ find /etc | grep -e ulimit -e 4096 -e nofile   		<= e <-pattern (multi pattern search )


	$ find /lib64 | grep mysql
		/lib64/libmysqlcppconn.so
		/lib64/libmysqlcppconn.so.7.1.1.3
------------------------------------------------------------------------------------
# fin -  dictory is empty?
https://stackoverflow.com/questions/20456666/bash-checking-if-folder-has-contents
------------------------------------------------------------------------------------
	$ find . -maxdepth 0 -empty -exec echo {} is empty. \;







------------------------------------------------------------------------------------
# Tmux  Terminal Multiplexer
# screen
------------------------------------------------------------------------------------
ctrl-b <command>
ctrl-b c - new window


------------------------------------------------------------------------------------
# Package Installed - Ubuntu 
------------------------------------------------------------------------------------
	$ dpkg -l | grep mongo

	$ root@puppet:~# dpkg --get-selections
	$ root@puppet:~# dpkg --get-selections | grep puppet
		puppet-common                                   install
		puppetlabs-release                              install

------------------------------------------------------------------------------------
# sed (stream editor) 
------------------------------------------------------------------------------------
	https://linuxconfig.org/learning-linux-commands-sed
	http://www.grymoire.com/Unix/Sed.html

	$ sed -i 's/START=no/START=yes/g'    /etc/default/puppet		
			i 	<= in place(INSERT MODE for Same file)
			g 	<= global (replace all)
	
	$ sed -i 's/original/new/g' file.txt
			-i 		 <= in-place (Insert mode to original file)
			'		 <= gate open
			s 		 <= the substitute command
			original <= a regex describing the word to replace(or just the word itself)
			new 	 <= the text to be replaced with
			g 		 <= global (replace all)
			'		 <= gate close
			file.txt <= target file name

	$ sed -i 's/START=no/START=yes/'     /etc/default/puppet	<= 1st line
	$ sed -i 's/START=no/START=yes/4'    /etc/default/puppet	<= 4th line
	
	$ sed 's/yes/no/g' < yesNo.txt								<= feed file 
	
	$ sed '/Name/s/PAK/PARK/g' report.txt						<= only contain 'Name' lines
	
	# sed => a New file
	$ sed 's/PAK/PARK/g' reportNew.txt	 > new.txt
	
	# cat => sed => a New file
	$ cat report.txt | sed 's/PAK/PARK/g' > reportNew.txt

	
	#hostname change from Ubuntu14
	$ sed -i 's/u14/rieman/g' /etc/hosts


	
	# IP change (change ip) using sed
	$ sed -i -r  "s/10.1 28.232.181\b/10.130.244.212/g"     start-us-lobby-7505.sh
	$ sed -i 	 's/10.10.10.10/1.1.1.1/g'    ip.txt		<= works too
	$ sed 's/^/ /' ip.txt    > ip_new.txt					<= add a space in front
	$ sed 's/^/    /' ip.txt > ip_new.txt					<= add 4 space in front
		(space)1.1.1.1
	$ sed -n 1p ip.txt										<= display one 1st line
		1.1.1.1
	
	
	### CMD Input Mode
	$ sed 's/the/THE/'
		the
		THE
	$ Ctrl +D  <= exit
	$ nl file.txt | sed 's/the/THE'
	
	# from CMD using echo & sed
	$ echo day | sed 's/day/night/'			
		night
	

	$ cat textFile.txt | sed s/fromOrginal/changedTo/ 1> changed_textFile.txt
	$ cat textFile.txt | awk '/us/ {print $2}'		<= find starting with /us/ and Prints out 2nd field
		
		
		
		
		
------------------------------------------------------------------------------------
# Regular Expressions (regex)
------------------------------------------------------------------------------------

. 		<= Any single character except newline
*		<= zero or more occurances of any character
[...]	<= Any single character specified in the set
[^...]	<= Any single character not specified in the set
^		<= Anchor - beginning of the line
$		<= Anchor - end of line
\<		<= Anchor - begining of word
\>		<= Anchor - end of word
\(...\)	<= Grouping - usually used to group conditions
\n		<= Contents of nth grouping

[...]   <= Set Examples
[A-Z]	<= The SET from Capital A to Capital Z
[a-z]	<= The SET from lowercase a to lowercase z
[0-9]	<= The SET from 0 to 9 (All numerals)
[./=+]	<= The SET containing . (dot), / (slash), =, and +
[-A-F]	<= The SET from Capital A to Capital F and the dash (dashes must be specified first)
[0-9 A-Z]	<= The SET containing all capital letters and digits and a space
[A-Z][a-zA-Z]	<= In the first position, the SET from Capital A to Capital Z
In the second character position, the SET containing all letters

------------------------------------------------------------------------------------
# Regular Expression Examples
------------------------------------------------------------------------------------
/Hello/		<= Matches if the line contains the value Hello
/^TEST$/	<= Matches if the line contains TEST by itself
/^[a-zA-Z]/	<= Matches if the line starts with any letter
/^[a-z].*/	<= Matches if the first character of the line is a-z and there is at least one more of any character following it
/2134$/		<= Matches if line ends with 2134
/\(21|35\)/	<= Matches is the line contains 21 or 35
			   #Note: the use of ( ) with the pipe symbol to specify the 'or' condition
/[0-9]*/	<= Matches if there are zero or more numbers in the line
/^[^#]/		<= Matches if the first character is not a # in the line

------------------------------------------------------------------------------------
# Regex tutorial- A quick cheatsheet 
------------------------------------------------------------------------------------
https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285
Anchors ^ and $
^The        <= matches any string that starts with The
            https://regex101.com/r/cO8lqs/2
end$        <= matches a string that ends with end
^The end$   <= exact string match (starts and ends with The end)
roar        <= matches any string that has the text roar in it



------------------------------------------------------------------------------------
# IP Regular Expression 
http://www.analyticsmarket.com/freetools/ipregex
------------------------------------------------------------------------------------

Notes:
1. Regular expressions are case sensitive
2. Regular expressions are to be used where pattern is specified
------------------------------------------------------------------------------------------------------------

		
		  
### EOF (End Of File <= Add some rows and text)
https://stackoverflow.com/questions/2500436/how-does-cat-eof-work-in-bash

$ cat << EOF >> puppet.conf
	[agent]
	server = puppetmaster.domain.tld
	EOF
 

The 'cat << EOF' Bash syntax is very useful when one needs to work with multiline strings in Bash, 
	eg. when passing multiline string to a variable, file or a piped command.
1. Passing multiline string to a variable:
	$ sql=$(cat <<EOF
	  SELECT foo, bar FROM db
	  WHERE foo='baz'
	  EOF
	  )
------------------------------------------------------------------------------------------------------------
# cat  file.txt  VS.   cat < file.txt
	http://unix.stackexchange.com/questions/258931/difference-between-cat-and-cat
------------------------------------------------------------------------------------------------------------

cat file.txt  	<= reads the file myfile.txt then prints it to the standard output.

cat < file.txt 	<= here cat isn't given any file(s) to open, so -like many Unix commands do- reads 
					the data from the standard input, which is directed there from file.txt by the shell, 
					and prints to standard output.




	
noza@fm:~$ echo $sql
SELECT foo, bar FROM db WHERE foo='baz'

noza@fm:~$ echo -e $sql
SELECT foo, bar FROM db WHERE foo='baz'

noza@fm:~$ echo -e "$sql"
SELECT foo, bar FROM db
WHERE foo='baz'
The $sql variable now holds newlines as well, you can check it with echo -e "$sql" cmd.

------------------------------------------------------------------------------------------------------------
# EOF <=  Passing multiline string to a file:
------------------------------------------------------------------------------------------------------------
"EOF" is known as a "Here DATE". Basically << Here tells the shell that you are going to enter 
	a multiline string until the "DATE" Here. You can name this DATE as you want, it's often EOF or STOP.
	- The DATE can be any string, uppercase or lowercase, though most people use uppercase by convention.
	- The DATE will not be considered as a Here DATE if there are other words in that line. In this case, 
		it will merely be considered part of the string. The DATE should be by itself on a separate line, 
		to be considered a DATE.
	- The DATE should have no leading or trailing spaces in that line to be considered a DATE. Otherwise 
		it will be considered as part of the string.
	
	More: http://stackoverflow.com/questions/2500436/how-does-cat-eof-work-in-bash

  example:
  $ cat >> test <<HERE
	> Hello world HERE <--- Not the end of string
	> This is a test
	>  HERE <-- Leading space, so not end of string
	> and a new line
	> HERE <-- Now we have the end of the string

  $ cat << EOF > print.sh
	#!/bin/bash
	echo \$PWD
	echo $PWD
	EOF
	The print.sh file now contains:

	#!/bin/bash
	echo $PWD
	echo /home/user

------------------------------------------------------------------------------------------------------------
# EOF Passing multiline string to a command/pipe:
------------------------------------------------------------------------------------------------------------
  $ cat <<EOF | grep 'b' | tee b.txt | grep 'r'
	foo
	bar
	baz
	EOF
	This creates b.txt file with both 'bar' and 'baz' lines but prints only the "bar".

------------------------------------------------------------------------------------------------------------
# Reading and writing
------------------------------------------------------------------------------------------------------------
$ echo "something" | sudo tee --append /var/log.file    
				<= non suoder gets permission error so use " sudo tee --append" 

----------------------------------------------- 
# While Loop that inputs file.txt file
#!/bin/bash								
i=0
while read f; do
	echo "Line $i: $f"
	((i++))
done < file.txt
-----------------------------------------------
	$ echo -e "Test1\nTest2\nTest3\nTest4" > file.txt 		

	$./while.sh 
	Line 0: test1
	Line 1: test2
	Line 2: test3
	Line 3: test4


------------------------------------------------------------------------------------------------------------
Install Fabric, boto, yaml
------------------------------------------------------------------------------------------------------------
   1) Install fabric, boto and yaml

         wget https://bootstrap.pypa.io/get-pip.py
         sudo python get-pip.py
         sudo pip install fabric
         sudo pip install boto
         sudo pip install PyYaml

		 

###################################################################################################### 
###  Repair   ###
######################################################################################################
Recover from File System Corruption Using 'FSCK' and a Recovery ISO
https://www.digitalocean.com/community/tutorials/how-to-recover-from-file-system-corruption-using-fsck-and-a-recovery-iso

# fsck - check and repair a Linux filesystem
	$ fsck -yf /dev/sda			<= -y, always  attempt to fix
  after reboot 
	$ ls lost+found
# blkid - locate/print block device attributes



		 
######################################################################################################	 
### Security  ###
######################################################################################################

1.Verifying Which Ports Are Listening
CentOS
#which ports are listening for TCP connections from the network:
	nmap -sT -O localhost   # -sT <= scan TCP  -O <= OS

#To check if the port is associated with the official list of known services	
	cat /etc/services | grep unknown_port

	# check for information about the port	
	netstat -anp | grep unknown_port     # -anp  <= All, no dns lookup(faster), program
	
	lsof -i | grep port_number		<= internet connection open
	lsof -p                      	<= process_number
	
	https://www.cyberciti.biz/faq/what-process-has-open-linux-port/
	fuser -k 80/tcp  <- kill port?
	
	
2. Changing SSH port from default #22 to 2222 
	$ vi /etc/ssh/sshd_config
		#22 to 2222 
		
	$ /etc/init.d/sshd restart
	
# Update the IPTables
	
	$ iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
	
	Allow all from x.x.x.x IP 
	-A INPUT -m state -s x.x.x.x --state NEW -j ACCEPT

# Port 80  open
	
$ sudo iptables -A INPUT -i eth0 -p tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT	
	
	


######################################################################################################
# Iptables <= Allow everything from single IP using cmd	
######################################################################################################

###------------------------------------------------------------------
$ iptables -A INPUT -m state -s 65.87.26.0/24     --state NEW -j ACCEPT		<= Namcity  IP

$ iptables -A INPUT -m state -s 64.95.137.0/24    --state NEW -j ACCEPT		<= PacCity  IP

$ iptables -A INPUT -m state -s 73.231.235.144/32 --state NEW -j ACCEPT   	<= Home

$ iptables -A INPUT -m state -s 74.217.69.192/26 --state NEW -j ACCEPT   	<= New Corp IP 




###	-----------------------------------------------------------------

# iptables -t [filter|nat|mangle] [-A|-D|-L|-F] [INPUT|OUTPUT|FORWARD] -p [tcp|udp] -m [tcp|udp] -s [sourceip] -d [destip] --dport [port] -j [DROP|REJECT|ACCEPT] 

# iptables -t [filter] [-A] [ INPUT   ]   -p [tcp] -m [tcp] -s [source_ip] -d [dest_ip] --dport [port] -j [ DROP   ] 	
			   nat	    -D    OUTPUT          udp      udp												 REJECT
			   mangle	-L    FORWARD																     ACCEPT
						-F


# http://www.iana.org/assignments/icmp-parameters/icmp-parameters.xhtml#icmp-parameters-codes-8
-----------------------------------------------------------------
# Iptables <= allow ping from IPs
-----------------------------------------------------------------
$ iptables -A INPUT -s x.x.x.x -p ICMP --icmp-type 8 -j ACCEPT
-----------------------------------------------------------------
# Iptables <=  block all ping
-----------------------------------------------------------------
$ iptables -A INPUT -p ICMP --icmp-type 8 -j DROP

-----------------------------------------------------------------
# Iptables ## Ubuntu Port Forwarding using 
-----------------------------------------------------------------
	# Using Linux iptables for port 80 -> 8080
	
	This enables port forwarding of traffic between ports 80 and 8080. 
	You can keep Jenkins on the default port 8080.
	
	$ sudo vi /etc/rc.local
	Then add the following just before the exit 0
	
	#Requests from outside
	iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080
	
	#Requests from localhost
	iptables -t nat -I OUTPUT -p tcp -d 127.0.0.1 --dport 80 -j REDIRECT --to-ports 8080
	
	Now reboot or run sudo /etc/rc.local to enable port forwarding. 
	Additional info: https://gist.github.com/m5m1th/6870a54717c0387468c3

-----------------------------------------------------------------
Block out going port 80 
-----------------------------------------------------------------
From Command
$ iptables -A OUTPUT -p tcp --dport 80 -j DROP
$ iptables -A OUTPUT -p tcp -d 192.168.1.2 --dport 80 -j DROP  <= Specific IP

From /etc/sysconfig/iptables
A OUTPUT -p tcp --dport 80 -j DROP
$ Iptables -A OUTPUT -p tcp -d 192.168.1.2 --dport 80 -j DROP  <= Specific IP



######################################################################################################
# Firewalld  <= Allow everything from single IP using cmd	
Firewalld is a complete firewall solution available by default on CentOS 7 servers.
######################################################################################################
firewall-cmd administrative tool
https://www.linode.com/docs/security/firewalls/introduction-to-firewalld-on-centos/

	drop: 	The lowest level of trust. All incoming connections are dropped without reply and only 
			outgoing connections are possible.
	block:  Similar to the above, but instead of simply dropping connections, incoming requests are 
			rejected with an 'icmp-host-prohibited' or 'icmp6-adm-prohibited' message.

	public: Represents public, untrusted networks. You don't trust other computers but may allow selected 
			incoming connections on a case-by-case basis.
	external: External networks in the event that you are using the firewall as your gateway. It is configured 
			  for NAT masquerading so that your internal network remains private but reachable.
	internal: The other side of the external zone, used for the internal portion of a gateway. The computers 
			  are fairly trustworthy and some additional services are available.
	dmz: 	Used for computers located in a DMZ (isolated computers that will not have access to the rest of 
			your network). Only certain incoming connections are allowed.
	work: 	Used for work machines. Trust most of the computers in the network. A few more services might be allowed.
	home: 	A home environment. It generally implies that you trust most of the other computers and that a few 
			more services will be accepted.
	trusted: Trust all of the machines in the network. The most open of the available options and should be 
			used sparingly.
	
	To use the firewall, we can create rules and alter the properties of our zones and then assign our network 
	interfaces to whichever zones are most appropriate.
	
	Rule Permanence
		In firewalld, rules can be designated as either 'permanent' or 'immediate'. If a rule is added or modified, 
		by default, the behavior of the currently running firewall is modified. At the next boot, the old rules will
		be reverted.
		e.g firewall-cmd --permanant 				<- reloaded upon boot
	
# Install firewalld
https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-using-firewalld-on-centos-7
	
	# Installation
	$ sudo yum install firewalld -y	
	$ sudo systemctl start firewalld
	$ sudo systemctl status firewalld
	$ sudo systemctl enable firewalld
	# $ sudo reboot
	
	
		
	# Turning on the Firewall
		$sudo systemctl start firewalld.service
	
	# status of the service:
		$ firewall-cmd --state
			running  
		$ systemctl status firewalld
			Active: active (running) 

	# Defaults Zone Info
		$ firewall-cmd --get-default-zone
			public
	
		$ firewall-cmd --get-active-zones
	
		$ firewall-cmd --list-all				<= same as 'iptables -nL'
			public
			  target: default
			  icmp-block-inversion: no
			  interfaces:
			  sources:
			  services: ssh dhcpv6-client
			  ports:
			  protocols:
			  masquerade: no
			  forward-ports:
			  source-ports:
			  icmp-blocks:
			  rich rules:
		
		# information about other zones 
		$ firewall-cmd --get-zones		
			block dmz drop external home internal public trusted work
		
		# specific configuration associated with a zone
		$ sudo firewall-cmd --zone=home --list-all
			home
			  target: default
			  icmp-block-inversion: no
			  interfaces:
			  sources:
			  services: ssh mdns samba-client dhcpv6-client
			  ports:
			  protocols:
			  masquerade: no
			  forward-ports:
			  source-ports:
			  icmp-blocks:
			  rich rules:

		# Selecting Zones for your Interfaces
		  Changing the Zone of an Interface
			# transition our eth0 interface to the “home” zone
			$ sudo firewall-cmd --zone=public --change-interface=eth0
			$ sudo firewall-cmd --zone=home --change-interface=eth1
			$ sudo firewall-cmd --get-active-zones
				home
				  interfaces: eth1
				public
				  interfaces: eth0
			
		# Adjusting the Default Zone
			change any interface that had fallen back on the default to the new zone
			$ sudo firewall-cmd --set-default-zone=home
		
	
	# Setting Rules for your Applications
		$ firewall-cmd --get-services
			RH-Satellite-6 amanda-client amanda-k5-client bacula bacula-client bgp bitcoin....
			
		# .xml file within the '/usr/lib/firewalld/services' directory
		$ cat /usr/lib/firewalld/services/ssh.xml
		------------------------------------------------------------------------
			<?xml version="1.0" encoding="utf-8"?>
			<service>
			  <short>SSH</short>
			  <description>Secure Shell (SSH) is a protocol for logging into and executing commands on remote machines. It provides secure encrypted communications. If you plan on accessing your machine remotely via SSH over a firewalled interface, enable this option. You need the openssh-server package installed for this option to be useful.</description>
			  <port protocol="tcp" port="22"/>
			</service>
		------------------------------------------------------------------------

		HTTP traffic, we can allow this traffic for interfaces in our “public” zone
			$ sudo firewall-cmd --zone=public --add-service=http
				success
			$ sudo firewall-cmd --zone=public --list-services
				ssh dhcpv6-client http
			
			# Add to Permanent rule
			$ sudo firewall-cmd --zone=public --permanent --add-service=http
			
			$ sudo firewall-cmd --zone=public --permanent --list-services
	
			# “public” zone will now allow HTTP/HTTPS web traffic on port 80/443
				web server is configured to use SSL/TLS
			$ sudo firewall-cmd --zone=public --add-service=https
			$ sudo firewall-cmd --zone=public --permanent --add-service=https
	
		Opening a specific 5000 Port for Public Zones
			$ sudo firewall-cmd --zone=public --add-port=5000/tcp
			$ firewall-cmd --zone=public --list-port
				5000/tcp
			$ sudo firewall-cmd --zone=public --add-port=4990-4999/udp
			
			$ sudo firewall-cmd --zone=public --permanent --add-port=5000/tcp
			$ sudo firewall-cmd --zone=public --permanent --add-port=4990-4999/udp
			
			$ sudo firewall-cmd --zone=public --permanent --list-ports
	
	Defining a Service
		Services are simply collections of ports with an associated name and description. Using services is easier 
		to administer than ports, but requires a bit of upfront work. The easiest way to start is to copy an 
		existing script found in '/usr/lib/firewalld/services' to the '/etc/firewalld/services' directory where the 
		firewall looks for non-standard definitions.	
		
		$ sudo cp /usr/lib/firewalld/services/ssh.xml /etc/firewalld/services/example.xml
	
		$ cat /etc/firewalld/services/example.xml
		-------------------------------------------------------------
			<?xml version="1.0" encoding="utf-8"?>
			<service>
			  <short>SSH</short>
			  <description>Secure Shell (SSH) is a protocol for logging into and executing commands on remote machines. 
							It provides secure encrypted communications. If you plan on accessing your machine remotely 
							via SSH over a firewalled interface, enable this option. You need the openssh-server package 
							installed for this option to be useful.
			  </description>
			  <port protocol="tcp" port="22"/>
			</service>
		-------------------------------------------------------------

		# The majority of this definition is actually metadata.
		# Change to open up port 7777 for TCP and 8888 for UDP
		  
		  $ vi /etc/firewalld/services/example.xml
		-------------------------------------------------------------
			<?xml version="1.0" encoding="utf-8"?>
			<service>
			  <short>Example Service</short>
			  <description>This is just an example service.  It probably shouldn't be used on a real system.</description>
			  <port protocol="tcp" port="7777"/>
			  <port protocol="udp" port="8888"/>
			</service>
		-------------------------------------------------------------
		
		$ sudo firewall-cmd --reload	
	
		$ firewall-cmd --get-services
		
	
	Creating Your Own Zones
		$ sudo firewall-cmd --permanent --new-zone=publicweb
		$ sudo firewall-cmd --permanent --new-zone=privateDNS
		$ sudo firewall-cmd --permanent --get-zones
			output
			block dmz drop external home internal privateDNS public publicweb trusted work
												   ^^^^^^		     ^^^^^^^				
	
		$ firewall-cmd --get-zones
			block dmz drop external home internal public trusted work
		
		$ sudo firewall-cmd --reload		<= Need to RELOAD
		$ firewall-cmd --get-zones
	
		
		$ sudo firewall-cmd --zone=publicweb --add-service=ssh
		$ sudo firewall-cmd --zone=publicweb --add-service=http
		$ sudo firewall-cmd --zone=publicweb --add-service=https
		$ sudo firewall-cmd --zone=publicweb --list-all
			
		$ sudo firewall-cmd --zone=privateDNS --add-service=dns
		$ sudo firewall-cmd --zone=privateDNS --list-all
	
	
		# We could then change our interfaces over to these new zones to test them out:
		$ sudo firewall-cmd --zone=publicweb --change-interface=eth0
		$ sudo firewall-cmd --zone=privateDNS --change-interface=eth1
	
		sudo firewall-cmd --zone=publicweb --permanent --add-service=ssh
		sudo firewall-cmd --zone=publicweb --permanent --add-service=http
		sudo firewall-cmd --zone=publicweb --permanent --add-service=https
		sudo firewall-cmd --zone=privateDNS --permanent --add-service=dns
		
		# After permanently applying these your rules, you can restart your network and reload your firewall service:
			sudo systemctl restart network
			sudo systemctl reload firewalld
	
		# Validate that the correct zones were assigned:
		$ firewall-cmd --get-active-zones
	
		sudo firewall-cmd --zone=publicweb --list-services
	
		You have successfully set up your own zones! If you want to make one of these zones the default for other interfaces, 
		remember to configure that behavior with the --set-default-zone= parameter:
		
		sudo firewall-cmd --set-default-zone=publicweb
	
# firewalld	
	sudo firewall-cmd --list-all-zones
	
	# Add source IP, port, protocol
	sudo firewall-cmd --add-rich-rule='rule family="ipv4" source address="172.28.60.42/32" port port=21 protocol="tcp" accept'
	sudo firewall-cmd --add-rich-rule='rule family="ipv4" source address="172.28.60.45/32" port port=21 protocol="tcp" accept'
	# June
	sudo firewall-cmd --add-rich-rule='rule family="ipv4" source address="182.72.103.190" service name="ssh" accept'
	
	sudo firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="74.217.69.194"    service name="https" accept'
	sudo firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="74.217.69.192/26" service name="https" accept'
	sudo firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="172.28.60.45/32" port port=21 protocol="tcp" accept'
	sudo firewall-cmd --permanent --add-rich-rule='rule family="ipv4" source address="172.28.60.45/32" port port=22 protocol="tcp" accept'
	
	# Add full access from single ip
	https://serverfault.com/questions/890370/open-all-ports-to-specific-ip-with-firewalld
	
	sudo firewall-cmd --permanent --zone=public --add-source=74.217.69.194
	sudo firewall-cmd --permanent --zone=public --add-source=74.217.69.192/26
	
	sudo firewall-cmd --zone=trusted --add-source=64.39.96.0/20 <-???
	# June
	sudo firewall-cmd --zone=trusted --add-source=182.72.103.190
	
	# APARK
	sudo firewall-cmd --zone=trusted --add-source=73.231.230.24
	
	# temmp June's jumpbox
	sudo firewall-cmd --zone=trusted --add-source=167.172.202.23
	$ sudo firewall-cmd --permanent --zone=trusted --add-source=3.220.95.248
	
	
	
sudo firewall-cmd --permanent --zone=trusted --add-source=3.220.95.248	  <= AWS JUNE

sudo firewall-cmd --permanent --zone=trusted --add-source=3.127.96.152	


sudo firewall-cmd --permanent --zone=trusted --add-source=18.195.95.147	

sudo firewall-cmd --permanent --zone=trusted --add-source=18.194.224.122

sudo firewall-cmd --permanent --zone=trusted --add-source=10.138.24.132 

###################################################################
HOME
sudo firewall-cmd --permanent --zone=trusted --add-source=98.234.54.81
sudo firewall-cmd --permanent --zone=trusted --add-source=3.220.95.248
sudo firewall-cmd --permanent --zone=trusted --add-source=18.195.92.147
sudo firewall-cmd --permanent --zone=trusted --add-source=192.241.178.95
June-VPN
sudo firewall-cmd --permanent --zone=trusted --add-source=10.128.5.187
June-VPN-STG
sudo firewall-cmd --permanent --zone=trusted --add-source=10.135.35.99

#########################################################################

sudo firewall-cmd --permanent --zone=trusted --add-source=167.172.202.23   <= June DO jumpbox
sudo firewall-cmd --permanent --zone=trusted --add-source=10.138.24.132     <= June jumpbox
sudo firewall-cmd --permanent --zone=trusted --add-source=10.128.2.48
sudo systemctl restart firewalld


### apark home
sudo firewall-cmd --permanent --zone=trusted --add-source=98.234.54.81
sudo systemctl restart firewalld

##  Jpark
sudo firewall-cmd --permanent --zone=trusted --add-source=162.243.29.144
sudo systemctl restart firewalld









	# Home 3/13/2020
	$ sudo firewall-cmd --zone=trusted --add-source=98.234.54.81
		
	$ sudo firewall-cmd --permanent --zone=trusted --add-source=98.234.54.81

	
	# holds default configurations like default zones and common services. Avoid updating them because those files will be 
		overwritten by each firewalld package update.
	/usr/lib/FirewallD 
	
	# holds system configuration files.
	# Manually add IP address
	sudo vi /etc/firewalld/zones/public.xml 
	
	# BNEA
	sudo iptables -A INPUT -m state -s 74.217.69.194     --state NEW -j ACCEPT
	sudo iptables -A INPUT -m state -s 74.217.69.192     --state NEW -j ACCEPT
	sudo iptables -A INPUT -m state -s 98.234.54.81      --state NEW -j ACCEPT
	
	# JUNE
	sudo iptables -A INPUT -m state -s 182.72.103.190    --state NEW -j ACCEPT			
	


	
	https://game-api-stg.pacman.bner.ro
	206.81.17.204
	 
	https://game-api-live.pacman.namcowireless.com/login
	162.243.43.113
	
-----------------------------------------------------------------
# email setup and send out from shell ###
-----------------------------------------------------------------
	http://www.binarytides.com/linux-mailx-command/
	https://www.digitalocean.com/community/tutorials/how-to-send-e-mail-alerts-on-a-centos-vps-for-system-monitoring
	
	$ yum -y update
	$ yum install -y mailx
	$ ln -s     /bin/mailx     /bin/email
	
	# Sending an email to G-mail
		## Method 1
	$ mail -s "Test" albert@gmail.com < /dev/null
		## Method 2
	$ mail -s "Test" albert@gmail.com 
		This is the MSG Test
	  'CTRL+D'  to end the program.
	
	# $ vi /etc/mail.rc     <= ???
	
------------------------------------------------------------------------------------
# Email send if IP changes	
------------------------------------------------------------------------------------
#!/bin/bash
# If IP Address changes, it will send out an email for update.
#
currentIP=`ifconfig | grep 'inet'| awk '{print $2}' | head -1`;
time=`date`;
oldIP_file='/home/apark/ipAddress/oldIP.txt';
oldIP=`cat ${oldIP_file}`;

if [[ ${currentIP} != ${oldIP} ]]
then
    echo "New IP = ${currentIP}"
    echo "IP has been changed and need to send an email."
    echo -e "Hello\n\n\
            TimeOfChanges = ${time}\n \
			NEW_IP = ${currentIP}\n\nBye" | \
            /usr/bin/mail -s \
            "[INFO] IP has been changed to ${currentIP}" \
            albertpark5@gmail.com;
        echo ${currentIP} > ${oldIP_file};
else
        echo "no IP changes!"
fi
------------------------------------------------------------------------------------
$ crontab -e
*/30 * * * * /home/apark/ipAddress/ip_changes_alert.sh > /dev/null
------------------------------------------------------------------------------------
		
	
### email send out from SHELL
	echo "hello" | mailx -v -s 'test' apark@bnga.com		<= -v verbose
	echo "hello" | mail  -s    'test' apark@bnga.com
	echo "Test"  | mail  -s    "This is Subject" -r "Icinga2<apark@bnga.com>" apark@bnga.com
															^ -r <= from<email>?	
 
	### Check mail log
	
 
 ### Use external SMTP server
	echo "This is the message" | mailx -v \
		-r "apark@bnga.com" \
		-s "subject" \
		-S smtp="email-smtp.us-west-2.amazonaws.com:465" \
		-S smtp-use-starttls \
		-S smtp-auth=login \
		-S smtp-auth-user="AKIAIRJHSW462O4HVOVQ" \
		-S smtp-auth-password="ApWdT46E5NqybHu0l+Iw3iiUnHPf8I7LmmZUb1aCLXXz" \
		-S ssl-verify=true \
		apark@bnga.com
		
# email SMTP setup (email server setup)
	$vi	/etc/ssmtp/ssmtp.conf

	$cat /etc/ssmtp/ssmtp.conf.rpmsave
	-----------------------------------------------------------------------------------
	#mailhub=email-smtp.us-west-2.amazonaws.com:465
	#UseTLS=yes
	#FromLineOverride=yes
	#AuthUser=AKIAIRJHSW462O4HVOVQ
	#AuthPass=ApWdT46E5NqybHu0l+Iw3iiUnHPf8I7LmmZUb1aCLXXz

	# Config file for SMTP sendmail
	# The person who gets all mail for userids < 1000
	# Make this empty to disable rewriting.
	root=postmaster

	# The place where the mail goes. The actual machine name is required no
	# MX records are consulted. Commonly mailhosts are named mail.domain.com
	AuthUser=albertpark5@gmail.com
	AuthPass=
	mailhub=smtp.gmail.com:587
	UseSTARTTLS=YES

	# Where will the mail seem to come from?
	rewriteDomain=Icinga2.Host

	# The full hostname
	hostname=Icinga2

	# Are users allowed to set their own From: address?
	# YES - Allow the user to specify their own From: address
	# NO - Use the system generated From: address
	FromLineOverride=YES
	-----------------------------------------------------------------------------------
---------------------------------------------------------------
# RSA Key
# ECDSA Key

a RSA key will work everywhere. ECDSA support is newer
https://security.stackexchange.com/questions/23383/ssh-key-type-rsa-dsa-ecdsa-are-there-easy-answers-for-which-to-choose-when
---------------------------------------------------------------		
	RSA - A public-key encryption technology developed by RSA Data Security, Inc. 
		The acronym stands for Rivest, Shamir, and Adelman, the inventors of the technique. 
		
	ECDSA <- (Elliptical curve Digital Signature Algorithm) is an Elliptic Curve implementation 
			of DSA (Digital Signature Algorithm).
			
			
			
---------------------------------------------------------------
# SSL 
# TLS
---------------------------------------------------------------

DSA is faster in signing, but slower in verifying. A DSA key of the same strength as RSA (1024 bits) 
generates a smaller signature. An RSA 512 bit key has been cracked, but only a 280 DSA key. 
Also note that DSA can only be used for signing/verification, whereas RSA can be used for 
encryption/decrypt as well.


SSL has been around for long enough you'd think that there would be agreed upon container formats. And you're right, 
	there are. Too many standards as it happens. So this is what I know, and I'm sure others will chime in.

.csr This is a Certificate Signing Request. Some applications can generate these for submission to 
	certificate-authorities. The actual format is PKCS10 which is defined in RFC 2986. It includes 
	some/all of the key details of the requested certificate such as subject, organization, state, 
	whatnot, as well as the public key of the certificate to get signed. These get signed by the CA 
	and a certificate is returned. The returned certificate is the public certificate (which includes 
	the public key but not the private key), which itself can be in a couple of formats.
.pem Defined in RFC's 1421 through 1424, this is a container format that may include just the public 
	certificate (such as with Apache installs, and CA certificate files /etc/ssl/certs), or may 
	include an entire certificate chain including public key, private key, and root certificates. 
	Confusingly, it may also encode a CSR (e.g. as used here) as the PKCS10 format can be translated 
	into PEM. The name is from Privacy Enhanced Mail (PEM), a failed method for secure email but the 
	container format it used lives on, and is a base64 translation of the x509 ASN.1 keys.
.key This is a PEM formatted file containing just the private-key of a specific certificate and is 
	merely a conventional name and not a standardized one. In Apache installs, this frequently resides 
	in /etc/ssl/private. The rights on these files are very important, and some programs will refuse 
	to load these certificates if they are set wrong.
.pkcs12 .pfx .p12 Originally defined by RSA in the Public-Key Cryptography Standards, the "12" variant 
	was enhanced by Microsoft. This is a passworded container format that contains both public and private 
	certificate pairs. Unlike .pem files, this container is fully encrypted. Openssl can turn this into 
	a .pem file with both public and private keys: openssl pkcs12 -in file-to-convert.p12 -out converted-file.pem -nodes

P12 - File containing a digital certificate that uses PKCS#12 (Public Key Cryptography Standard #12) 
		encryption; used as a portable format for transferring personal private keys or other sensitive 
		information; used by various security and encryption programs.

A few other formats that show up from time to time:

.der A way to encode ASN.1 syntax in binary, a .pem file is just a Base64 encoded .der file. OpenSSL 
	can convert these to .pem (openssl x509 -inform der -in to-convert.der -out converted.pem). Windows 
	sees these as Certificate files. By default, Windows will export certificates as .DER formatted 
	files with a different extension. Like...
.cert .cer .crt A .pem (or rarely .der) formatted file with a different extension, one that is recognized 
	by Windows Explorer as a certificate, which .pem is not.
.p7b Defined in RFC 2315, this is a format used by windows for certificate interchange. Java understands 
	these natively. Unlike .pem style certificates, this format has a defined way to include certification-path certificates.
.crl A certificate revocation list. Certificate Authorities produce these as a way to de-authorize 
	certificates before expiration. You can sometimes download them from CA websites.
In summary, there are four different ways to present certificates and their components:

PEM Governed by RFCs, it's used preferentially by open-source software. It can have a variety of extensions 
	(.pem, .key, .cer, .cert, more)
PKCS7 An open standard used by Java and supported by Windows. Does not contain private key material.
PKCS12 A private standard that provides enhanced security versus the plain-text PEM format. This can 
	contain private key material. It's used preferentially by Windows systems, and can be freely converted 
	to PEM format through use of openssl.
DER The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. 
	Not routinely used by much outside of Windows.



------------------------------------------------------------------------------------
# boot   
# mistakenly deleted /boot folder and rebooted and  when Kernel is gone
------------------------------------------------------------------------------------
1. Bootup with Live CD.

2. Find the drive/partition where you have installed your root filesystem. 
	To do this you can open a terminal and run either 
	$ sudo parted -l     # parted  is  a  disk  partitioning  and  partition resizing program.o
	$ sudo fdisk -l 	 #  a menu-driven program for creation and manipulation of partition tables.

3. Assuming that your root partition that you found from the last step is /dev/sda1
	Since Linux 2.4.0 it is possible to remount part of the file hierarchy somewhere else. The call is 
	'mount --bind old_dir new_dir'

	$ mkdir mnt
	$ sudo mount /dev/sda1 mnt
	$ sudo mount --bind /dev /mnt/dev
	$ sudo mount --bind /proc /mnt/proc
	$ sudo mount --bind /sys /mnt/sys
	$ sudo chroot mnt						<= chroot - run command or interactive shell with special root directory

4. You will now be inside a chroot environment meaning that running commands here is equivalent to running 
	them on your installed system. 
	The first thing you want to do is reinstall GRUB2 to the device so that it copies the correct files into the /boot folder. 
	To do this run the following with the drive that your root partition is on (ie /dev/sda1 ):

	$ grub-install /dev/sda
	You now want to find out which packages you have installed that have files in the boot directory and reinstall them. 
	This will replace the kernel images that have been deleted among other things. The command to find the packages is:

5. $ dpkg -S /boot
		memtest86+, linux-image-3.13.0-32-generic, base-files: /boot
		 And to reinstall them:

6. $ apt-get --reinstall install linux-image-3.13.0-32-generic
	
	This step will probably require internet access (unless the packages are already in the cache), so make 
	sure you are connected if there is an problem.

	Since you will have deleted your kernels and reinstalled them, this should have triggered a GRUB2 update 
	automatically. But just in case they haven't, you can run:

7. $ update-grub
	Reboot and things should now be fixed. 
	One issue that I had the last time I did something similar was that Windows installs where not found by update-grub 
	One issue that I had the last time I did something similar was that Windows installs where not found by update-grub 
	when run in the chroot due to a bug in os-prober. If this is an issue, just run sudo update-grub 
	again in the repaired system.

------------------------------------------------------------------------------------	
# update vs upgrade	- CentOS package 
------------------------------------------------------------------------------------
'yum upgrade' and 'yum update' will perform the same function that update to the latest current version of package.

But the difference is 'Upgrade' will delete obsolete packages, while 'update' will preserve them.
$ yum upgrade kernel -y   	<= only upgrade the kernel 
$ yum update  kernel -y		<= 

------------------------------------------------------------------------------------
# CHROOT
------------------------------------------------------------------------------------
	Using chroot cmd to switch different file systems
	e.g. Host OS kernel, module, drivers on host file system1(Ubuntu /dev/sda1) can modify on file system2(Debina /dev/sda2) 

	Ubuntu 
	$nmap 			<- works
	$ cd /debian
	$ chroot . bash
	Debian 			<- Using host kernel but running on Debian file system to modify
	$ nmap 			<- need to install
	$ exit

------------------------------------------------------------------------------------
# Kernel Update 
------------------------------------------------------------------------------------
### Ubuntu
https://wiki.ubuntu.com/Kernel/LTSEnablementStack#Ubuntu_16.04_LTS_-_Xenial_Xerus
Ubuntu 16.04 LTS Server- Xenial Xerus

$ sudo apt-get install --install-recommends linux-generic-hwe-16.04 

$ sudo apt-get update && sudo apt-get dist-upgrade
	Update command is used to update cache, dist-upgrade will update your kernel to 
	latest available in repository.

	
### CentOS Kernel Update	
$ sudo yum update kernel -y	
	
	
------------------------------------------------------------------------------------	
#  multi screen utility 
# Byobu
# screen
# tmux
------------------------------------------------------------------------------------
# CentOS 7
$ sudo yum install --enablerepo=epel byobu

	$ byobu 

	F2 <= New terminal   
	Moving      F3 <=   => F$
	
	Shift+F2  <= Split 1/2 Vertically
	Ctrl +F2  <= Split 1/2 Horizontally

	# Make 4 Split screens with in 1 terminal
	Shift+F2 => Ctrl+F2 => Shift+4 => Ctrl+F2
	
	# Moving around
	Shift+F4
	
	# Too kill
	Ctrl+d    <=Exit
	
# Ubuntu 14
    $ byobu-enable/disable
	$ ctrl+F2
	
DOS - Remove all files including subdirectory
# Remove All directory with data and sub
rmdir "c:/temp/1" /s/q  














################################################################################################################
### Memcache  ##################################################################################################
################################################################################################################

$ watch -n1  'memcached-tool localhost stats'     <= number of time 1 sec
$ watch -n 1 'memcached-tool localhost stats'







------------------------------------------------------------------------------------
# Nginx  
------------------------------------------------------------------------------------

1. Nginx log rotate

$ mv /path/to/access.log /path/to/access.log.0
$ kill -USR1 `cat /var/run/nginx.pid`
$ sleep 1
[ post-rotation processing of old log file ]
	# rotates the logs is "kill -USR1 /var/run/nginx.pid". This does not kill the Nginx process, 
	# but instead sends it a signal causing it to reload its log files. This will cause new 
	# requests to be logged to the refreshed log file.


	
	
	
###
#  If the Nginx is running with PID, it copies to the /var/run/nginx.pid 	
$ ps aux | grep nginx
$ cat /var/run/nginx.pid














------------------------------------------------------------------------------------
# Log rotate
------------------------------------------------------------------------------------
to check logrotate, run cmd

$ logroate

### Configurations and default options
/etc/logrotate.conf

### Application-specific log file information (to override the defaults) 
$ ll /etc/logrotate.d/
-rw-r--r-- 1 root root 185 Feb  4  2016 httpd
-rw-r--r-- 1 root root 871 May 11 02:31 mysqld
-rw-r--r-- 1 root root 136 May  5 01:27 newrelic-sysmond


#### The logrotation for dpkg monitors the /var/log/dpkg.log file and does this on a 
/var/log/dpkg.log {
	monthly					# monthly basis - this is the rotation interval.
	size 100M				#### <===logs are rotated once the file size reaches 100M and this need not 
								wait for the monthly cycle.
	rotate 12				# 12 days worth of logs would be kept.
	compress				# logfiles can be compressed using the gzip format by specifying
	delaycompress			# will work only if 'compress' option is specified.
	missingok				# avoids halting on any error and carries on with the next log file.
	notifempty				# avoid log rotation if the log file is empty.
	create 644 root root	# create <mode> <owner> <group> creates a new empty file with the specified 
								properties after log-rotation.
}
####
	# Though missing in the above example, 'size' is also an important setting if you want to control the sizing of the logs growing in the system.

### Example ###	
$ cat /etc/logrotate.d/httpd
####
/var/log/httpd/*log {
    missingok
    notifempty
    sharedscripts
    delaycompress
    postrotate
        /sbin/service httpd reload > /dev/null 2>/dev/null || true
    endscript
}
####



### using Cron Job ###
	You can also set the logrotation as a cron so that the manual process can be avoided 
		and this is taken care of automatically. 
	By specifying an entry in /etc/cron.daily/logrotate , the rotation is triggered daily.

	$ /etc/cron.daily/logrotate
###
#!/bin/sh

/usr/sbin/logrotate /etc/logrotate.conf
EXITVALUE=$?
if [ $EXITVALUE != 0 ]; then
    /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"
fi
exit 0
###	
	
	

------------------------------------------------------------------------------------
# logrotate  - CUSTOM LOG FILE Rotation
------------------------------------------------------------------------------------

$ vi /etc/logrotate.d/rrserver

#####
/RRLinux/*log {
    weekly
    size 1G
    rotate 10
    missingok
    notifempty
    delaycompress
    postrotate
        /RRLinux/restart-eu-s1.sh > /dev/null		#switch to new log file
    endscript
}

#####
	

### Check log 	
$ cat /var/lib/logrotate.status

"/RRLinux/run.log" 2016-10-24     <=== New logrotate creation for /etc/logrotate.d/rrserver




### Testing logrotate.conf ###

$ logrotate -vdf /etc/logrotate.conf
		verbose flag, “-v”  
		debug flag,   “-d” 
		force flag,   "-f"

		
rotating pattern: /RRLinux/*log  forced from command line (10 rotations)
empty log files are not rotated, old logs are removed
considering log /RRLinux/error.log
  log does not need rotating

considering log /RRLinux/RidgerRacer_Server_vSlave.log
  log needs rotating

considering log /RRLinux/run.log
  log needs rotating
running postrotate script
running script with arg /RRLinux/error.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
rotating log /RRLinux/RidgerRacer_Server_vSlave.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
glob finding old rotated logs failed

renaming /RRLinux/RidgerRacer_Server_vSlave.log to /RRLinux/RidgerRacer_Server_vSlave.log-20161024
creating new /RRLinux/RidgerRacer_Server_vSlave.log mode = 0644 uid = 0 gid = 0

running postrotate script
running script with arg /RRLinux/RidgerRacer_Server_vSlave.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
rotating log /RRLinux/run.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
glob finding old rotated logs failed

renaming /RRLinux/run.log to /RRLinux/run.log-20161024
creating new /RRLinux/run.log mode = 0644 uid = 0 gid = 0
running postrotate script
running script with arg /RRLinux/run.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"

	

------------------------------------------------------------------------------------
# logrotate check - httpd ###
------------------------------------------------------------------------------------
$ logrotate -vdf /etc/logrotate.d/httpd

##########
reading config file /etc/logrotate.d/httpd
reading config info for /var/log/httpd/*log

Handling 1 logs

rotating pattern: /var/log/httpd/*log  forced from command line (no old logs will be kept)
empty log files are not rotated, old logs are removed
considering log /var/log/httpd/access_log
  log does not need rotating
considering log /var/log/httpd/error_log
  log does not need rotating
not running postrotate script, since no logs were rotated
########	
	
	
	
### specific logrotate run ###	

$ logrotate -vdf /etc/logrotate.d/rrserver


####	
reading config file /etc/logrotate.d/rrserver
reading config info for /RRLinux/*log
Handling 1 logs

#---------------------------------------------------------------------------------------
rotating pattern: /RRLinux/*log  forced from command line (10 rotations)
empty log files are not rotated, old logs are removed
considering log /RRLinux/error.log
  log does not need rotating
considering log /RRLinux/RidgerRacer_Server_vSlave.log
  log needs rotating
considering log /RRLinux/run.log
  log needs rotating
running postrotate script
running script with arg /RRLinux/error.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"

#---------------------------------------------------------------------------------------
rotating log /RRLinux/RidgerRacer_Server_vSlave.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
renaming /RRLinux/RidgerRacer_Server_vSlave.log.10 to /RRLinux/RidgerRacer_Server_vSlave.log.11 (rotatecount 10, logstart 1, i 1                                                              0),
.....                                                          
renaming /RRLinux/RidgerRacer_Server_vSlave.log.0 to /RRLinux/RidgerRacer_Server_vSlave.log.1 (rotatecount 10, logstart 1, i 0),                                                              
renaming /RRLinux/RidgerRacer_Server_vSlave.log to /RRLinux/RidgerRacer_Server_vSlave.log.1
running postrotate script
running script with arg /RRLinux/RidgerRacer_Server_vSlave.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
removing old log /RRLinux/RidgerRacer_Server_vSlave.log.11
error: error opening /RRLinux/RidgerRacer_Server_vSlave.log.11: No such file or directory


#---------------------------------------------------------------------------------------
rotating log /RRLinux/run.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
renaming /RRLinux/run.log.10 to /RRLinux/run.log.11 (rotatecount 10, logstart 1, i 10),
....
renaming /RRLinux/run.log.0 to /RRLinux/run.log.1 (rotatecount 10, logstart 1, i 0),
renaming /RRLinux/run.log to /RRLinux/run.log.1
running postrotate script
running script with arg /RRLinux/run.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
removing old log /RRLinux/run.log.11
error: error opening /RRLinux/run.log.11: No such file or directory
####	
	
	
73. Logrotate	
	Configurations and default options 
	$ /etc/logrotate.conf
	
	Application-specific log file information
	$ /etc/logrotate.d/
	
	 /etc/logrotate.d/dpkg. One of the entries in this file would be:

	/var/log/dpkg.log {
			monthly						<= this on a monthly basis - this is the rotation interval.
			rotate 12					<= 'rotate 12' signifies that 12 days worth of logs would be kept.
			compress					<= logfiles can be compressed using the gzip format by specifying 'compress'
			delaycompress				<= 'delaycompress' delays the compression process till the next log rotation. 
											'delaycompress' will work only if 'compress' option is specified.
			missingok					<= 'missingok' avoids halting on any error and carries on with the next log file.
			notifempty					<= 'notifempty' avoid log rotation if the logfile is empty.
			create 644 root root		<= 'create <mode> <owner> <group>' creates a new empty file with the 
											specified properties after log-rotation.
	}

	Though missing in the above example, 'size' is also an important setting if you want to control the sizing 
	of the logs growing in the system.
	A configuration setting of around 100MB would look like:
	size 100M
	
	# Cron Job setup
	0 0 * * * (or @daily)  /etc/cron.daily/logrotate
	$ cat /var/lib/logrotate/status 
	


	
	

74.  Apache Log

Check config file

	$ apachectl configtest
	https://httpd.apache.org/docs/2.4/programs/apachectl.html
	
	
$ while true; do tail -n0 -f /var/log/apache2/access.log > /tmp/tmp.log & sleep 2; \ #continue
  kill $! ; wc -l /tmp/tmp.log | cut -c-2; done 2> /dev/null

  # kill $! <= kill PID of the most recent background command
  
 
	A. View Apache requests per minute

$ grep -i "16/Mar/2015:06" example.com | cut -d[ -f2 | cut -d] -f1 | awk -F: '{print $2":"$3}' | sort -nk1 -nk2 | uniq -c | awk '{ if ($1 > 10) print $0}'

	# grep "16/Mar/2015:06" example.com	
	  <= Use the grep command to only show hits from today during the 06th hour 
		from our Apache access log.
	# cut -d[ -f2 | cut -d] -f1	
	  <=Use the cut command with the -delimiter set to an opening bracket [ and print 
		out the -field of data that shows up 2nd, then use the cut command again 
		with the -delimter set to a closing bracket ] and print out the -field of 
		data that shows up 1st which gives us just the time stamp.
	# awk -F: '{print $2":"$3}'	
	  <=Use the awk command with the -Field delimiter set to a colon :, then print out 
		the $2nd column which is the hour, followed by the $3th column which is 
		the minute.
	# sort -nk1 -nk2 | uniq -c	
	  <= Sort the hits numerically by the 1st column which is the hour, then by the 
		2nd column which is the minute.
	# awk '{ if ($1 > 10) print $0}'	
	  <= Finally use the awk command with an if statement to only print out data when 
		the $1st column which is the number of hits in a minute is greater than 10.	
	

	
	
------------------------------------------------------------------------------------	
# complete -pr [-DE] [name ...]
              Specify how arguments to each name should be completed.	
------------------------------------------------------------------------------------	
	$ alias     k=kubectl
	$ complete  -F __start_kubectl k
	
	
	

	
	
	
	
	
	
	
------------------------------------------------------------------------------------	
# Awk 
------------------------------------------------------------------------------------
	Built-in Variables – FS, OFS, RS, ORS, NR, NF, FILENAME, FNR
	http://www.thegeekstuff.com/2010/01/8-powerful-awk-built-in-variables-fs-ofs-rs-ors-nr-nf-filename-fnr/?ref=binfind.com/web
	
	1. Awk FS Example: Input field separator variable.
	gawk - pattern scanning and processing language

	
	awk는 파일로부터 레코드(record)를 선택하고, 선택된 레코드에 포함된 값을 조작하거나 데이터화하는 
	것을 목적으로 사용하는 프로그램입니다. 즉, awk 명령의 입력으로 지정된 파일로부터 데이터를 
	분류한 다음, 분류된 텍스트 데이터를 바탕으로 패턴 매칭 여부를 검사하거나 데이터 조작 및 
	연산 등의 액션을 수행하고, 그 결과를 출력하는 기능을 수행합니다
	https://recipes4dev.tistory.com/171
	
	
	
	
------------------------------------------------------------------------------------	
# Remove a space in string
------------------------------------------------------------------------------------
	https://stackoverflow.com/questions/13659318/how-to-remove-space-from-string
	sed 's/ //g'
	tr -d ' '
	$ echo "   3918912k " | sed 's/ //g'
		3918912k 
	
	
	
	
------------------------------------------------------------------------------------	
# .h file what is it?
------------------------------------------------------------------------------------
Files that contain the .h file extension are normally header files used with the C or C++ programming languages. 
.h files are commonly known by programmers as "header files". They may contain constants, function prototypes 
and external variable definitions.

The header files are included by source code files that need to use the constants or functions defined in 
the header file. For example, if you define a text string with your company name in a header file, 
this constant can be used everywhere you need to display or otherwise use the company name in your source code. 
Should you later decide to change the name, you only have to do it in the header file and recompile to make the 
change take effect everywhere.	
	
	
	
------------------------------------------------------------------------------------	
# Network Trobleshooting
------------------------------------------------------------------------------------
  # Wireshark Filter <= https://www.youtube.com/watch?v=68t07-KOH9Y
------------------------------------------------------------------------------------
	filters are here:
	ip.addr == 10.0.0.1
	tcp or dns
	tcp.port == 443
	tcp.analysis.flags
	!(arp or icmp or dns)
	follow tcp stream
	tcp contains facebook
	http.response.code == 200
	http.request
	tcp.flags.syn == 1
------------------------------------------------------------------------------------	
  # Tshark  
------------------------------------------------------------------------------------
	# Dump and analyze network traffic for CLI environment and READ from GUI WIRESHARK!!!! 
	https://linuxsimba.com/tshark-examples
	
	$ yum install wireshark
	$ tshark -D  				<= Print a list of the interfaces on which TShark can capture
	
	$ tshark -w /tmp/capture.log -f "port 22" -i eth0 -P     
			# -w  <= Write raw packet data to outfile 
			# -f  <= Set the capture filter expression.
			# -i  <= Network interface 
			# -P  <= Decode and display the packet summary
			
	# http://explainshell.com/explain?cmd=tshark+-w+%2Ftmp%2Fdhcp.pcap+-f+%22port+67+or+port+68%22+-i+eth1+-P
	
	$ tshark -r /tmp/capture.log
	
	$ tshark -D
		1. eth0
    $ tshark -i 1 -a duration:10 -w /tmp/10secs.pcap
	$ tshark -r /tmp/10secs.pcap
	
------------------------------------------------------------------------------------			
#  ldd 
------------------------------------------------------------------------------------	
	prints the shared objects (shared libraries) required by each program or shared object specified 
	on the command line.  An example of its use and output is the following:
	
	$ ldd /usr/sbin/sshd

------------------------------------------------------------------------------------	
# Debug
------------------------------------------------------------------------------------	
	$ yum groupinstall -y 'Development Tools'
	$ debuginfo-install openssh-server-6.6.1p1-33.el7_3.x86_64
	$ yum info openssl
	
	$rpm -q --changelog openssl | grep CVE-2015-319
		- fix CVE-2015-3197 - SSLv2 ciphersuite enforcement
		- fix CVE-2015-3194 - certificate verify crash with missing PSS parameter
		- fix CVE-2015-3195 - X509_ATTRIBUTE memory leak
		- fix CVE-2015-3196 - race condition when handling PSK identity hint

------------------------------------------------------------------------------------	
# Network 
# MTU resize size
------------------------------------------------------------------------------------
	$ ifconfig eth0 mtu 1000
	
	
	### check network RX size
	$ ifconfig eth0 | grep "RX " | awk '{print $2}' | cut -d: -f2
	$ ifconfig | grep enp* | grep "RX " | awk '{print $2}' | cut -d: -f2
	$ RX=`ifconfig eth0 | grep "RX " | awk '{print $2}' | cut -d: -f2`; echo $RX
    
	# MTU size check
	$ $cat /sys/class/net/enp*/mtu						<= CentOS enp* <- auto named Network card
	
	$ ifconfig | grep enp* | grep -o 'mtu ....'			<= .... == 1500 <-4 dots -o Only matching
	  => mtu 1500

	
	
------------------------------------------------------------------------------------	
# Ping
------------------------------------------------------------------------------------
	- Is the remote host alive?		=> Host reachability
	- Is the network speed good? 	=> Network congestion
	- Is the remote host far? 		=> Travel length
	
	# Ping MTU discovery
	http://muzso.hu/2009/05/17/how-to-determine-the-proper-mtu-size-with-icmp-pings
	
	https://openmaniak.com/ping.php
	
	ICMP Packet
	<-------------------->|<--------------------->|<---------------------->|<------------------->
	Mac Header(14-bytes)	IP Header(20-bytes)		ICMP Header(8-bytes)	ICMP Data Variable
	|<----------------------------------------------------------------------------------------->|
							Ethernet Frame
	|<----------------------------------------------------------------------------------------->|
											IP Packet	
	|<----------------------------------------------------------------------------------------->|	
														ICMP Packet
	
	
	$ ping -M  do -s 1500 remote_host
			-M pmtudisc_opt
              Select  Path  MTU Discovery strategy.  pmtudisc_option may be either 
				do   <-(prohibit fragmentation, even local one), 
				want <-(do PMTU discovery, fragment locally when packet size is large),
                dont <-(do not set DF flag).
			-s packetsize
              Specifies  the number of data bytes to be sent.  The default is 56, 
			  which translates into 64 ICMP data bytes when combined with the 8 bytes 
			  of ICMP header data.
			1500 MTU size
			
			
	$ ping -M do -s 1500 45.55.5.69
		PING 45.55.5.69 (45.55.5.69) 1500(1528) bytes of data.
		ping: local error: Message too long, mtu=1500

	$ ping -M do -s 1400 45.55.5.69
		PING 45.55.5.69 (45.55.5.69) 1400(1428) bytes of data.
		1408 bytes from 45.55.5.69: icmp_seq=1 ttl=52 time=9.05 ms
	
	$ ping -6 ipv6.google.com
	$ ping6   ipv6.google.com
	
	#------------------------------------------------------------------------------------
	#### Works solution ###
	$ vi /etc/ssh/ssh_config
	# uncomment on Ciphers and MACs under 
	#Host *
	Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc
	MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160
	#------------------------------------------------------------------------------------
	
	
	# TTL (Time TO LIVE)
	CentOS 	 64 TTL
	Windows 128 TTL
	Ubuntu  192 TTL
	
	### ICMP Packet 
	
	<----------><---------><-----------><--------->
	MAC Header 	 IP Header  ICMP Header  ICMP Data
	14-bytes	  20-bytes	 8-bytes
	<--------------------------------------------->     <= Ethernet Frame
				<--------------------------------->		<= IP Packet
				           <---------------------->		<= ICMP Packet
						   
						   
						   
						   
						   
						   
						   
------------------------------------------------------------------------------------						   
# tput
------------------------------------------------------------------------------------ 
		(http://linuxcommand.org/lc3_adv_tput.php)
		# When we start a terminal session on our Linux system, the terminal emulator sets the TERM environment 
		  variable with the name of a terminal type.	
		https://en.wikipedia.org/wiki/Tput		  
		# tput is a standard Unix operating system command which makes use of terminal capabilities.
		  Depending on the system, tput uses the terminfo or termcap database, as well as looking into the environment for the terminal type.
						   
						   
	### infocmp - compare or print out terminfo descriptions
		infocmp  can  be used to compare a binary terminfo entry with other terminfo entries, rewrite a
       terminfo description to take advantage of the use= terminfo field,  or  print  out  a  terminfo
       description  from  the  binary  file (term) in a variety of formats.  In all cases, the boolean
       fields will be printed first, followed by the numeric fields, followed by the string fields.				
						   
	$ echo $TERM
	$ infocmp screen
		#       Reconstructed via infocmp from file: /usr/share/terminfo/s/screen
		screen|VT 100/ANSI X3.64 virtual terminal,

	$ logname
	$ whoami
	
	The tput command can be used to test for a particular capability or to output the assigned value.
	$ tput logname
	xterm terminal emulator (X Window System)
	"screen" used by terminal multiplexers such as screen and tmux.
						   
	
	
------------------------------------------------------------------------------------	
# Screen( multi shells)  
------------------------------------------------------------------------------------	
	https://www.rackaid.com/blog/linux-screen-tutorial-and-how-to/
		1. Use multiple shell windows from a single SSH session.
		2. Keep a shell active even through network disruptions.
		3. Disconnect and re-connect to a shell sessions from multiple locations.
		4. Run a long running process without maintaining an active shell session.

	$ screen						<= launch a screen Shell
	
	$ 'Ctrl+a' then '?'				<= get help list from the inside of screen
		# 'ctrl+a' is a signal to send commands to 'Screen' instead of the 'Shell'
                                
								Command key:  ^A   Literal ^A:  a
		break       ^B b        history     { }         other       ^A          split       S
		clear       C           info        i           pow_break   B           suspend     ^Z z
		colon       :           kill        K k         pow_detach  D           time        ^T t
		copy        ^[ [        lastmsg     ^M m        prev        ^H ^P p ^?  title       A
		detach      ^D d        license     ,           quit        \           vbell       ^G
		digraph     ^V          lockscreen  ^X x        readbuf     <           version     v
		displays    *           log         H           redisplay   ^L l        width       W
		dumptermcap .           login       L           remove      X           windows     ^W w
		fit         F           meta        a           removebuf   =           wrap        ^R r
		flow        ^F f        monitor     M           reset       Z           writebuf    >
		focus       ^I          next        ^@ ^N sp n  screen      ^C c        xoff        ^S s
		hardcopy    h           number      N           select      '           xon         ^Q q
		help        ?           only        Q           silence     _
		.......
	
	# 'C'reating Windows
	$ Ctrl+a+c  or(||) Ctrl+a  c
	
	# Switching Between Windows 'N'ext & 'P'revious
	$ Ctrl+a+n  || Ctrl+a  n   			<= Next 
	$ Ctrl+a+p  || Ctrl+a  p 			<= Previous  
	
	# 'D'etaching From Screen to go back to ORIGINAL SCREEN
	$ Ctrl+a+d	|| 	Ctrl+a d
	
	# Listing screens
	$ screen -ls	
		There are screens on:
        31184.pts-8.i7  (Detached)
        30957.pts-8.i7  (Detached)
        30323.pts-8.i7  (Detached)
		3 Sockets in /var/run/screen/S-apark
	
	# 'R'eattach to Screen
	$ screen -r	 31184.pts-8.i7
	
	# 'D'etach(Log Out) screen
	$ Ctrl+a+d	or 	Ctrl+a d
	
	# history save to 'H'ome ($HOME)
	$ Ctrl+a  H(Capital H)
	
	# Getting Alerts 'M'essage
	$ Ctrl+a  M(Capital M)
	
	# To monitor for silence or no output use 
	$ Ctrl-A  _(Underscore)
	
	
	# Screen Examples
	$ screen					<= launch a screen Shell
		$ ping google.com		<= a New screen Shell
		$ 'Ctrl+a'  'd' 		<= detach from the New Screen

	$ screen -ls				<= screen list
		There are screens on:
        31184.pts-8.i7  (Detached)
        30957.pts-8.i7  (Detached)
        30323.pts-8.i7  (Detached)
		3 Sockets in /var/run/screen/S-apark		<= socket location
	
	or	$ ls -l /var/run/screen/S-apark/
			srw-------. 1 apark apark 0 Jul 27 09:47 30323.pts-8.i7
			srw-------. 1 apark apark 0 Jul 27 09:48 30957.pts-8.i7
	
	$ screen -r 31184.pts-8.i7			<= 'R'eattach PID 31184 
	
	$ exit  || Ctrl+d					<= exit from screen PID     31184
		
	$ screen -ls
		There are screens on:
        30957.pts-8.i7  (Detached)
        30323.pts-8.i7  (Detached)

	$ screen -dr	PID				<=	Reattach a session and if necessary to detach it first.
 		There are several suitable screens on:
        31184.pts-8.i7  (Detached)
        30957.pts-8.i7  (Detached)
        30323.pts-8.i7  (Detached)
		Type "screen [-d] -r [pid.]tty.host" to resume one of them.

	$ screen -dr 31184.pts-8.i7			<= reattach '$ping google.com' screen


------------------------------------------------------------------------------------	
# ETL https://en.wikipedia.org/wiki/Extract,_transform,_load
------------------------------------------------------------------------------------
	Extract, Transform, & Load 
	Three database functions that are combined into one tool to pull data out of one database 
	and place it into another database. Extract is the process of reading data from a database.						   
	
------------------------------------------------------------------------------------	
# UML	https://en.wikipedia.org/wiki/Unified_Modeling_Language
------------------------------------------------------------------------------------
	The Unified Modeling Language (UML) is a general-purpose, developmental, modeling language in the field 
	of software engineering, that is intended to provide a standard way to visualize the design of a system.

------------------------------------------------------------------------------------	
# RSS https://en.wikipedia.org/wiki/RSS
------------------------------------------------------------------------------------
	(Rich Site Summary; originally RDF Site Summary; often called Really Simple Syndication) uses a family of 
	standard web feed formats[2] to publish frequently updated information: blog entries, news headlines, 
	audio, video. An RSS document (called "RSS feed", "web feed", or "channel") includes full or summarized 
	text, and metadata, like publishing date and author's name.
	'RSS Feeds' enable publishers to syndicate data automatically. A standard XML file format ensures 
	compatibility with many different machines/programs. 
	RSS feeds also benefit users who want to receive timely updates from favourite websites 
	or to aggregate data from many sites
	
------------------------------------------------------------------------------------	
# build
### Nexus or Artifictory for a Maven Repo	
------------------------------------------------------------------------------------
	http://stackoverflow.com/questions/364775/should-we-use-nexus-or-artifactory-for-a-maven-repo
	
	
### /dev 		<= directory 
	http://unix.stackexchange.com/questions/18239/understanding-dev-and-its-subdirs-and-files
	crw-r-----. 1 root kmem 1, 1 Feb  2 17:50 /dev/mem
	
	^ c <= Character devices
	There are two types of device files: block devices (indicated by b as the first character in the output of ls -l), 
	and character devices (indicated by c).
	
	- Block Devices (indicated by b as the first character in the output of ls -l), and character 
	devices (indicated by c). The distinction between block and character devices is not 
	completely universal. Block devices are things like disks, which behave like large, 
	fixed-size files: if you write a byte at a certain offset, and later read from the device 
	at that offset, you get that byte back.  
	
	- Character devices are just about anything else, where writing a byte has some immediate 
	effect (e.g. it's emitted on a serial line) and reading a byte also has some immediate 
	effect (e.g. it's read from the serial port).	
	
------------------------------------------------------------------------------------	
# /var/log 
------------------------------------------------------------------------------------
	http://superuser.com/questions/565927/differences-in-var-log-syslog-dmesg-messages-log-files

	Log files from the system and various programs/services, especially login (/var/log/wtmp, which logs all 
	logins and logouts into the system) and syslog (/var/log/messages, where all kernel and system program 
	message are usually stored). Files in /var/log can often grow indefinitely, and may require cleaning at 
	regular intervals. Something that is now normally managed via log rotation utilities such as 'logrotate'. 
	This utility also allows for the automatic rotation compression, removal and mailing of log files. 
	Logrotate can be set to handle a log file daily, weekly, monthly or when the log file gets to a certain 
	size. Normally, logrotate runs as a daily cron job. This is a good place to start troubleshooting general 
	technical problems.

/var/log/messages 	– Contains global system messages, including the messages that are logged during system "START-UP". 
(up to start up)	  There are several things that are logged in /var/log/messages including mail, cron, daemon, kern, auth, etc.

------------------------------------------------------------------------------------
# dmesg vs /var/log/dmesg
------------------------------------------------------------------------------------

dmesg				- Realtime Kernel Log  <= Run from Prompt 
 					  Current content of the kernel syslog ring buffer messages
					  
/var/log/dmesg   	– Contains what was in that ring buffer when the boot process last completed
					  Contains kernel ring buffer information up till START-UP. 
					  When the system boots up, it prints number of messages 
					  on the screen that displays information about the hardware devices that the kernel 
					  detects during boot process. These messages are available in kernel ring buffer and 
					  whenever the new message comes the old message gets overwritten. You can also view the 
					  content of this file using the dmesg command.	  

					  
/var/log/secure		- CentOS: security login
/var/log/auth.log 	– Ubuntu: Contains system authorization information, including user logins and 
							  authentication machinsm that were used.

/var/log/boot.log 	– Contains information that are logged when the system boots
/var/log/daemon.log – Contains information logged by the various background daemons that runs on the system
/var/log/dpkg.log   – Contains information that are logged when a package is installed or removed using dpkg command
/var/log/kern.log   – Contains information logged by the kernel. Helpful for you to troubleshoot a custom-built 
						kernel. */var/log/lastlog – Displays the recent login information for all the u sers. 
						This is not an ascii file. You should use lastlog command to view the content of this file.
/var/log/mail.log   – Contains the log information from the mail server that is running on the system. For example, 
						sendmail logs information about all the sent items to this file
/var/log/user.log   – Contains information about all user level logs
/var/log/Xorg.x.log – Log messages from the X
/var/log/alternatives.log – Information by the update-alternatives are logged into this log file. On Ubuntu, 
						update-alternatives maintains symbolic links determining default commands.
/var/log/btmp 		– This file contains information about failed login attemps. Use the last command to view 
						the btmp file. For example, “last -f /var/log/btmp | more”
/var/log/cups 		– All printer and printing related log messages
/var/log/anaconda.log – When you install Linux, all installation related messages are stored in this log file
/var/log/yum.log 	– Contains information that are logged when a package is installed using yum
/var/log/cron 		– Whenever cron daemon (or anacron) starts a cron job, it logs the information about the 
						cron job in this file
/var/log/secure 	– Contains information related to authentication and authorization privileges. For example, 
						sshd logs all the messages here, including unsuccessful login.
/var/log/wtmp   	– Contains login records. Using wtmp you can find out who is logged into the system. 
/var/log/utmp		  who command uses this file to display the information.
/var/log/faillog 	– Contains user failed login attemps. Use faillog command to display the content of this file. 
						Apart from the above log files, /var/log directory may also contain the following 
						sub-directories depending on the application that is running on your system.
/var/log/httpd/ 
/var/log/apache2 	– Contains the apache web server access_log and error_log
/var/log/lighttpd/ 	– Contains light HTTPD access_log and error_log
/var/log/conman/ 	– Log files for ConMan client. conman connects remote consoles that are managed by conmand daemon.
/var/log/mail/ 		– This subdirectory contains additional logs from your mail server. For example, sendmail 
						stores the collected mail statistics in /var/log/mail/statistics file
/var/log/prelink/ 	– prelink program modifies shared libraries and linked binaries to speed up the startup 
						process. /var/log/prelink/prelink.log contains the information about the .so file that 
						was modified by the prelink.
/var/log/audit/ 	– Contains logs information stored by the Linux audit daemon (auditd).
/var/log/setroubleshoot/ – SELinux uses setroubleshootd (SE Trouble Shoot Daemon) to notify about issues in the 
						security context of files, and logs those information in this log file.
/var/log/samba/ 	– Contains log information stored by samba, which is used to connect Windows to Linux.
/var/log/sa/ 		– Contains the daily sar files that are collected by the sysstat package.
/var/log/sssd/ 		– Use by system security services daemon that manage access to remoA servlet is simply a 
						class which responds to a particular type of network request - most commonly an HTTP request. 
						Basically servlets are usually used to implement web applications - but there are also various 
						frameworks which operate on top of servlets (e.g. Struts) to give a higher-level abstraction 
						than the "here's an HTTP request, write to this HTTP response" level which servlets provide.



------------------------------------------------------------------------------------
# Servlets 
------------------------------------------------------------------------------------
	A servlet is simply a class which responds to a particular type of network request - most commonly an HTTP request. 
	Basically servlets are usually used to implement web applications - but there are also various frameworks which 
	operate on top of servlets (e.g. Struts) to give a higher-level abstraction than the "here's an HTTP request, 
	write to this HTTP response" level which servlets provide.

	Servlets run in a servlet container which handles the networking side (e.g. parsing an HTTP request, 
	connection handling etc). One of the best-known open source servlet containers is Tomcat.	
	
	
------------------------------------------------------------------------------------	
### Binary releases - contain computer OS readable version of the application, meaning it is compiled. 
### Source releases - contain human readable version(code) of the application, meaning it has to be compiled 
					  before it can be used.	
	
	
### primitive types <= basic types of data
					   byte, short, int, long, float, double, boolean, char
					   primitive variables store primitive values
		
### reference types <= any instantiable class as well as arrays
						String, Scanner, Random, Die, int[], String[], etc.
						reference variables store addresses	
	
------------------------------------------------------------------------------------	 
# Directory 
# . and .. means
------------------------------------------------------------------------------------
http://www.basicconfig.com/linux/linux_cd_command_tutorial	
	.  means the current directory.
	.. means the parent directory.
	e.g. when you type cd .. , you will move one level up. 
		 When you type cp /etc/resolv.conf  .  , you will copy file /etc/resolv.conf in the current directory.	

------------------------------------------------------------------------------------	
# Regression testing
------------------------------------------------------------------------------------
	is a type of software testing that verifies that software previously developed and tested still 
	performs correctly even after it was changed or interfaced with other software. Changes may 
	include software enhancements, patches, configuration changes, etc.	
	
------------------------------------------------------------------------------------	
# Makefile - build 
------------------------------------------------------------------------------------
	Make is a build automation tool that automatically builds executable programs and libraries from 
	source code by reading files called Makefiles which specify how to derive the target program. 
	Though integrated development environments and language-specific compiler features can also be 
	used to manage a build process, Make remains widely used, especially in Unix and Unix-like 
	operating systems.
	Besides building programs, Make can be used to manage any project where some files must be 
	updated automatically from others whenever the others change.	

------------------------------------------------------------------------------------	
### Node.JS
------------------------------------------------------------------------------------
	It lets you write web apps that use Javascript on both the server and the client, so you 
	don't need to know multiple programming languages to program your website. It's also 
	really good at handling real-time concurrent web applications, which makes it a great 
	choice for a lot of modern web apps.
	
	NVM - Node Version Manager

------------------------------------------------------------------------------------	
###	Paginators 
------------------------------------------------------------------------------------
	a feature of the SDK that act as an abstraction over this process to make it easier for developers 
	to use paginated APIs. A Paginator is essentially an iterator of results. They are created via the 
	getPaginator() method of the client.
	https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/paginators.html
	
------------------------------------------------------------------------------------
# SSL cert location
------------------------------------------------------------------------------------
public root certificates:

"/etc/ssl/certs/ca-certificates.crt", // Debian/Ubuntu/Gentoo etc.
"/etc/pki/tls/certs/ca-bundle.crt",   // Fedora/RHEL


	
------------------------------------------------------------------------------------	
# A bastion host 
------------------------------------------------------------------------------------
is a special purpose computer on a network specifically designed and configured to withstand attacks. 
The computer generally hosts a single application, for example a proxy server, and all other services are removed or 
limited to reduce the threat to the computer. It is hardened in this manner primarily due to its location and purpose, 
which is either on the outside of the firewall or in theDMZ and usually involves access from untrusted networks or computers.
Internet Protocol Security (IPsec) is a protocol suite for securing Internet Protocol (IP) communications by 
authenticating and encrypting each IP packet of a communication session. IPsec includes protocols for establishing
 mutual authentication between agents at the beginning of the session and negotiation of cryptographic keys to be 
 used during the session. IPsec can be used in protecting data flows between a pair of hosts (host-to-host), 
 between a pair of security gateways (network-to-network), or between a security gateway and a host (network-to-host).[1]
IPsec is an end-to-end security scheme operating in the Internet Layer of the Internet Protocol Suite, while 
some other Internet security systems in widespread use, such as Transport Layer Security (TLS) and Secure Shell (SSH), 
operate in the upper layers of the TCP/IP model. Hence, IPsec protects any application traffic across an IP network. 
Applications do not need to be specifically designed to use IPsec. Without IPsec, the use of TLS/SSL must be designed 
into an application to protect the application protocols.

------------------------------------------------------------------------------------
# AWS
------------------------------------------------------------------------------------
  # Hardware Virtualization Machine (HVM)
		Red Hat Enterprise Linux 6.4 (PV) - ami-6283a827 (64-bit) / ami-4a83a80f (32-bit)
		Red Hat Enterprise Linux version 6.4 (PV), EBS-backed
		Root device type: ebs Virtualization type: paravirtual

  # Paravirtual (PVM)
		A virtual machine(VM) mode in which operating systems do not require complete emulation of hardware devices. 
		Paravirtual mode uses an API to interact with the host virtualization platform. Paravirtual mode doesn't require 
		special virtualization technology hardware.


------------------------------------------------------------------------------------
# Certificate
------------------------------------------------------------------------------------
 A wildcard certificate is a public key certificate which can be used with multiple subdomains of a domain.[1]
Depending on the number of subdomains an advantage could be that it saves money and also could be more convenient. 
Limitation[edit]
Only a single level of subdomain matching is supported.[2]
It is not possible to get a wildcard for an Extended Validation Certificate.[3]
A workaround could be to add every virtual host name in the Subject Alternative Name (SAN) extension.
	[4][5][6] The major problem being that the certificate needs to be reissued whenever a 
	new virtual server is added.[7]
Wildcards can be added as domains in multi-domain certificates or Unified Communications Certificates(UCC).
	[8] In addition, wildcards themselves can have subjectAltName extensions, including other wildcards. 
	For example: The wildcard certificate *.wikipedia.org has *.m.wikimedia.org as an Subject Alternative Name. 
	Thus it secureshttps://www.wikipedia.org as well as the completely different website name 
	https://meta.m.wikimedia.org.[9] *.company.com

	
	
### Generate SSL Certificate Signing Request (CSR)
The first part of enrolling for your SSL Certificate is to generate a Certificate Signing Request (CSR). 
	CSR generation is wholly dependent on the software you use on your webserver. Select your webserver 
	software from the list after reading the following general points:
General Points to remember before creating your CSR:
•	The Common Name field should be the Fully Qualified Domain Name (FQDN) or the web address for which 
	you plan to use your Certificate, e.g. the area of your site you wish customers to connect to using SSL. 
	For example, an SSL Certificate issued forcomodogroup.com will not be valid for secure.comodogroup.com. 
	If the web address to be used for SSL is secure.comodogroup.com, ensure that the common name submitted 
	in the CSR is secure.comodogroup.com
Our SSL Certificate are compatible with almost all popular webserver software. If your webserver software 
	does not appear on the list, please contact support@comodo.com with full details of your webserver 
	software and we will contact you with further instructions.

------------------------------------------------------------------------------------
# Jscrambler
------------------------------------------------------------------------------------

JScrambler is an online JavaScript obfuscator and code optimization tool available as a Web application and Web API. 
	First release was in 2010 [1] by Auditmark, a startup based on a Portuguese business incubator known as UPTEC.
	[2][3] JScrambler is on the third version since April 17.[4] Features like locking down code to a specific domain, 
	setting the code to expire after a specific date, function outlining, changing the control flow of the program, 
	code obfuscation, code optimization, debugging code and assert elimination, deadcode elimination, constant 
	folding may be found at latest release of the software.[5][6]

A colocation centre or colocation center (also spelled co-location, collocation, colo, or coloc) is a type 
	of data centre where equipment, space, and bandwidth are available for rental to retail customers. 
	Colocation facilities provide space, power, cooling, and physical security for the server, storage, 
	and networking equipment of other firms—and connect them to a variety of telecommunications and network 
	service providers—with a minimum of cost and complexity.

OpenStack OpenStack is a global collaboration of developers and cloud computing technologists producing 
	the ubiquitous open source cloud computing platform for public and private clouds. The project aims 
	to deliver solutions for all types of clouds by being simple to implement, massively scalable, and 
	feature rich. The technology consists of a series ofinterrelated projects delivering various components 
	for a cloud infrastructure solution.

------------------------------------------------------------------------------------
# Hash function
------------------------------------------------------------------------------------
Hash functions are primarily used to generate fixed-length output data that acts as a shortened 
reference to the original data. This is useful when the original data is too cumbersome to use in its entirety.
One practical use is a data structure called a hash table where the data is stored associatively. 
Searching linearly for a person's name in a list becomes cumbersome as the length of the list increases, 
but the hashed value can be used to store a reference to the original data and retrieve constant time 
(barring collisions). Another use is in cryptography, the science of encoding and safeguarding data. 
It is easy to generate hash values from input data and easy to verify that the data matches the hash, 
but hard to 'fake' a hash value to hide malicious data. This is the principle behind the PGP algorithm for data validation.

Hash functions are also frequently used to accelerate table lookup or data comparison tasks such as finding 
items in a database, detecting duplicated or similar records in a large file and finding similar stretches in DNA sequences.
A hash function should be deterministic: when it is invoked twice on identical data (e.g. two strings 
containing exactly the same characters), the function should produce the same value. This is crucial 
to the correctness of virtually all algorithms based on hashing. In the case of a hash table, the lookup 
operation should look at the slot where the insertion algorithm actually stored the data that is being 
sought for, so it needs the same hash value.
Hash functions are typically not invertible, meaning that it is not possible to reconstruct the input datum x 
from its hash value h(x) alone. In many applications, it is common that several values hash to the same value, 
a condition called a hash collision. Since collisions cause "confusion" of objects, which can make exact 
hash-based algorithm slower and approximate ones less precise, hash functions are designed to minimize 
the probability of collisions. For cryptographic uses, hash functions are engineered in such a way that is 
impossible to reconstruct any input from the hash alone without expending great amounts of computing time 
(see also One-way function).
Hash functions are related to (and often confused with) checksums, check digits, fingerprints, randomization 
functions, error-correcting codes, and ciphers. Although these concepts overlap to some extent, each has its 
own uses and requirements and is designed and optimized differently. The Hash Keeper database maintained by the 
American National Drug Intelligence Center, for instance, is more aptly described as a catalog of file fingerprints 
than of hash values.




------------------------------------------------------------------------------------
### GridFS
------------------------------------------------------------------------------------
	GridFS is a specification for storing and retrieving files that exceed the BSON-document size limit 
	of 16MB. Instead of storing a file in a single document, GridFS divides a file into parts, or chunks, 
	and stores each of those chunks as a separate document.
  
------------------------------------------------------------------------------------
# OLAP
------------------------------------------------------------------------------------
In computing, online analytical processing, or OLAP, is an approach to answering multi-dimensional analytical(MDA) 
queries swiftly.[1] OLAP is part of the broader category of business intelligence, which also encompasses 
relational database, report writing and data mining.[2] Typical applications of OLAP include business 
reporting for sales, marketing, management reporting, business process management (BPM),
[3] budgeting andforecasting, financial reporting and similar areas, with new applications 
coming up, such as agriculture.[4] The term OLAP was created as a slight modification of the 
traditional database term OLTP (Online Transaction Processing).[5]
OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. 
OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, 
and slicing and dicing.[6] Consolidation involves the aggregation of data that can be 
accumulated and computed in one or more dimensions. For example, all sales offices are 
rolled up to the sales department or sales division to anticipate sales trends. By contrast, 
the drill-down is a technique that allows users to navigate through the details. For instance, 
users can view the sales by individual products that make up a region’s sales. Slicing and 
dicing is a feature whereby users can take out (slicing) a specific set of data of the OLAP 
cube and view (dicing) the slices from different viewpoints.
Databases configured for OLAP use a multidimensional data model, allowing for complex 
analytical and ad hoc queries with a rapid execution time.[7] They borrow aspects of 
navigational databases, hierarchical databases andrelational databases.

Underlying data

When we do a survey or experiment over a period of time we usually plot the results 
and present the information to the reader in this condensed form. If the reader wants 
to check how we actually got the results they will want to examine the data that we used ... 
the data that underlies the results.

------------------------------------------------------------------------------------
# MapReduce 
------------------------------------------------------------------------------------
MapReduce is a programming model for processing large data sets with a parallel, 
distributed algorithm on a cluster.[1]
A MapReduce program is composed of a Map() procedure that performs filtering and sorting 
(such as sorting students by first name into queues, one queue for each name) and 
a Reduce() procedure that performs a summary operation (such as counting the number 
of students in each queue, yielding name frequencies). The "MapReduce System" 
(also called "infrastructure" or "framework") orchestrates by marshalling the 
distributed servers, running the various tasks in parallel, managing all communications 
and data transfers between the various parts of the system, and providing for redundancy and fault tolerance.
The model is inspired by the map and reduce functions commonly used in functional 
programming,[2] although their purpose in the MapReduce framework is not the same 
as in their original forms.[3] The key contributions of the MapReduce framework are 
not the actual map and reduce functions, but the scalability and fault-tolerance 
achieved for a variety of applications by optimizing the execution engine once. 
As such, a single-threaded implementation of MapReduce will usually not be faster 
than a traditional implementation. Only when the optimized distributed shuffle 
operation (which reduces network communication cost) and fault tolerance features 
of the MapReduce framework come into play, the use of this model is beneficial.
MapReduce libraries have been written in many programming languages, with different 
levels of optimization. A popular open-source implementation is Apache Hadoop. 
The name MapReduce originally referred to the proprietaryGoogle technology but has since been genericized.

------------------------------------------------------------------------------------
# Nginx 
------------------------------------------------------------------------------------
Nginx (pronounced "engine-ex") is an open source reverse proxy server for HTTP, 
HTTPS, SMTP, POP3, and IMAP protocols, as well as a load balancer, HTTP cache, 
and a web server (origin server). The nginx project started with a strong focus 
on high concurrency, high performance and low memory usage. It is licensed 
under the 2-clause BSD-like license and it runs on Linux, BSD variants, Mac OS X, 
Solaris, AIX, HP-UX, as well as on other *nix flavors. It also has a proof 
of concept port for Microsoft Windows.[6]

------------------------------------------------------------------------------------
# Null (SQL)
------------------------------------------------------------------------------------
Null is a special marker used in Structured Query Language (SQL) to indicate that a data 
value does not exist in the database. Introduced by the creator of the relational database 
model, E. F. Codd, SQL Null serves to fulfill the requirement that all true relational 
database management systems (RDBMS) support a representation of "missing information and 
inapplicable information". Codd also introduced the use of the lowercase Greek omega (ω) 
symbol to represent Null in database theory. NULL is also an SQL reserved keyword used 
to identify the Null special marker.

Null has been the focus of controversy and a source of debate because of its 
associatedthree-valued logic (3VL), special requirements for its use in SQL joins, 
and the special handling required by aggregate functions and SQL grouping operators. 
Computer science professor Ron van der Meyden summarized the various issues as: "The 
inconsistencies in the SQL standard mean that it is not possible to ascribe any intuitive 
logical semantics to the treatment of nulls in SQL."[1] Although various proposals 
have been made for resolving these issues, the complexity of the alternatives has prevented 
their widespread adoption.

For people new to the subject, a good way to remember what null means is to remember that 
in terms of information, "lack of a value" is not the same thing as "a value of zero"; 
similarly, "lack of an answer" is not the same thing as "an answer of no". For example, 
consider the question "How many books does Juan own?" The answer may be "zero" 
(weknow that he owns none) or "null" (we do not know how many he owns, or doesn't own). 
In a database table, the column reporting this answer would start out with a value of null, 
and it would not be updated with "zero" until we have ascertained that Juan owns no books.
SQL null is a state (unknown) and not a value. This usage is quite different from 
programming languages, where null means not assigned to a particular instance


	
	
	
	
	
------------------------------------------------------------------------------------	
# Print 
------------------------------------------------------------------------------------
#!/bin/sh
print_usage() {
        echo -e "`basename $0` ssh_connexion local_script"
        echo -e "Remote executes local_script on ssh server"
        echo -e "For convinient use, use ssh public key for remote connexion"
        exit 0
}

[ $# -eq "2" ] && [ $1 != "-h" ] && [ $1 != "--help" ] || print_usage

INTERPRETER=$(head -n 1 $2 | sed -e 's/#!//')

cat $2 | grep -v "#" | ssh -t $1 $INTERPRETER	
#------------------------------------------------------------------------------------	

----------------------------------------------------------------------------------------
# bash built-in variable	
----------------------------------------------------------------------------------------	
https://coderwall.com/p/85jnpq/bash-built-in-variables
*You passed 3 parameters to your script.*

$# = number of total arguments. <= 'Usage' error report if any arg missing
$@ = what parameters were passed. 
$? = was last command successful. Answer is 0 which means 'True'
$$ = PID of this script

$ vi test.sh
------------------------------------------------------------
#!/bin/bash
echo 'Number of total argument is $#:' $#
echo 'Total argument past is $@:'      $@
echo 'Exit code of this script is $?:' $?
echo 'PID of this script is $$:'       $$
------------------------------------------------------------
$ bash ./test.sh 1 2 3 4
	output:
	Number of total argument is $#: 4
	Total argument past is $@: 1 2 3 4
	Exit code of this script is $?: 0
	PID of this script is $$: 2033


	
#------------------------------------------------------------------------------------	

Array
a=()
b=("apple" "banana" "cherry")   or  $b=(apple banana cherry)
$ echo $b						<= only returns apple
$ echo ${b[2]}
$ b[5]="kiwi"					<= add to 5th position in array
$ echo ${b[5]}
kiwi
$ b+=("mango")					<= add to the end of array
$ echo ${b[@]}					<= @ as whole array list	
apple banana cherry kiwi mango
$ b+=melon						<= add to the 0 position of array
$ echo ${b[@]}
applemelon banana cherry kiwi mango

$ echo ${b[@]}
applemelon banana cherry kiwi mango melon melon

#grep last
$ echo ${b[@]: -1}				<= grep last element on the array
melon				

# Bash 4 <, associate ARRAY
-----------------------------------------------
#!/bin/bash
declare -A my_array
my_array[brand]=BMW
my_array["model"]="X6"
echo I have a ${my_array["brand"]} ${my_array[model]} car.
-----------------------------------------------



	
	
	
------------------------------------------------------------------------------------	
# Stress test using 'stress'
------------------------------------------------------------------------------------
http://www.tecmint.com/linux-cpu-load-stress-test-with-stress-ng-tool/

# Install
$ wget ftp://fr2.rpmfind.net/linux/dag/redhat/el7/en/x86_64/dag/RPMS/stress-1.0.2-1.el7.rf.x86_64.rpm 
$ yum localinstall stress-1.0.2-1.el7.rf.x86_64.rpm
	
	
------------------------------------------------------------------------------------
# Internet speed test
------------------------------------------------------------------------------------
http://askubuntu.com/questions/104755/how-to-check-internet-speed-via-terminal

$ curl -s  https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py | python -	
$ wget --output-document=/dev/null http://speedtest.wdc01.softlayer.com/downloads/test500.zip	
	
# Ubuntu 16.04	
$ sudo apt install speedtest-cli
$ speedtest-cli	

$ pip install speedtest-cli	
	

------------------------------------------------------------------------------------
### SONAR
------------------------------------------------------------------------------------
	Open Source Project and Code Quality Monitoring
	https://www.sonarsource.com/
	http://www.methodsandtools.com/tools/tools.php?sonar

------------------------------------------------------------------------------------
### Anemometer[ӕnəmάmitər 애너마미털] 풍향게
------------------------------------------------------------------------------------
	https://github.com/box/Anemometer
	The MySQL Slow Query Monitor. This tool is used to analyze slow query logs collected 
	from MySQL instances to identify problematic queries.	

------------------------------------------------------------------------------------	
### OSI Layers - Quick Summary
------------------------------------------------------------------------------------
	Application 	<- Responsible for determining when access to the network is required
	Presentation	<- Ensures data is received in a useable format.
						Data encryption is done here!
	Session			<- Establishing & maintaing connections
						Responsible for ports and ensure queries for services.
	Transport		<- Breaks data into frames & assigns sequence numbers. Checks errors in data received.
						UPD and SPX
	Network			<- How systems on different network segments find each other.
						Source-Destination addresses, Subnets, Path determination.  
						IP & IPX
	DataLink		<- Frames.  Handles flow control.  MAC
	Physical		<- Transmissionof the raw bit stream. Electrical signalling.
	
	
------------------------------------------------------------------------------------	
***Troubleshoot***
------------------------------------------------------------------------------------
# YUM update not working - CentOS 7 
------------------------------------------------------------------------------------
	$echo "http_caching=packages" >>   /etc/yum.conf file
	
------------------------------------------------------------------------------------
### Received disconnect from server: 2: Too many authentication failures for username
------------------------------------------------------------------------------------
https://superuser.com/questions/187779/too-many-authentication-failures-for-username
resolution: config .ssh/config

	
	
###########################################
### Lynda.com - Unix for Mac OS X Users ###
###########################################

1-1 The Terminal Application
	/Applications/Utilities/Terminal
	# Shift + CMD + u	

	$ Ctrl + A 			<- Move cursor to start of line	
	$ Ctrl + E			<- Move cursor to end of line
	$ Option + Click 	<- Move cursor to click point
	
	
	
	
	
	
------------------------------------------------------------------------------------
# Terminology 		###
------------------------------------------------------------------------------------

# entitlement <= [궐리,자격] An entitlement is a single right granted to a particular app, tool, 
				or other executable that gives it additional permissions beyond what it would ordinarily 
				have. Some entitlements are represented by key-value pairs in the entitlements 
				file in your project (a file with a .entitlements file extension), and others 
				are enabled for an App ID in your developer account.

	
# Stack 		<- used for static memory allocation 
				int x=1
				int x=

# Heap 		<- dynamic memory allocation	
				frm Object
	int x=1
	int y=2
	Form1 frm=newForm1()
	
# Jetty
	Eclipse Jetty provides a Web server and javax.servlet container, plus support for HTTP/2, 
	WebSocket, OSGi, JMX, JNDI, JAAS and many other integrations. These components are open 
	source and available for commercial use and distribution.

	Eclipse Jetty is used in a wide variety of projects and products, both in development 
	and production. Jetty can be easily embedded in devices, tools, frameworks, application 
	servers, and clusters. See the Jetty Powered page for more uses of Jetty.
	
# Java Servlet
	A servlet is simply a class which responds to a particular type of network request - 
	most commonly an HTTP request. Basically servlets are usually used to implement web 
	applications - but there are also various frameworks which operate on top of servlets 
	(e.g. Struts) to give a higher-level abstraction than the "here's an HTTP request, 
	write to this HTTP response" level which servlets provide.

	Servlets run in a servlet container which handles the networking side (e.g. parsing an 
	HTTP request, connection handling etc). One of the best-known open source servlet 
	containers is Tomcat.




You can use the stack if you know exactly how much data you need to allocate before compile time 
and it is not too big.	You can use heap if you don't know exactly how much data you will need 
at runtime or if you need to allocate a lot of data.

In a multi-threaded situation each thread will have its own completely independent stack but 
they will share the heap. Stack is thread specific and Heap is application specific. The stack 
is important to consider in exception handling and thread executions.	
	
	
# Plist
	In the macOS, iOS, NeXTSTEP, and GNUstep programming frameworks, property list files are files 
	that store serialized objects. Property list files use the filename extension .plist
	
# Node.js 
	is a JavaScript platform for general-purpose programming that allows users to build network 
	applications quickly. By leveraging JavaScript on both the front and backend, Node.js makes 
	development more consistent and integrated.	
	
------------------------------------------------------------------------------	
Cross-Site Request Forgery (CSRF)
------------------------------------------------------------------------------------
https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF)	
Cross-Site Request Forgery (CSRF) is an attack that forces an end user to execute unwanted actions on a web 
application(e.g. Jenkins) in which they're currently authenticated. CSRF attacks specifically target 
state-changing requests, not theft of data, since the attacker has no way to see the response to the 
forged request. With a little help of social engineering (such as sending a link via email or chat), 
an attacker may trick the users of a web application into executing actions of the attacker's choosing. 

If the victim is a normal user, a successful CSRF attack can force the user to perform state changing 
requests like transferring funds, changing their email address, and so forth. If the victim is an 
administrative account, CSRF can compromise the entire web application.

------------------------------------------------------------------------------------	
Runtime
------------------------------------------------------------------------------------ 
	is the period of time when a program is running. It begins when a program is opened 
	(or executed) and ends with the program is quit or closed.
------------------------------------------------------------------------------------

------------------------------------------------------------------------------
# HTTPS?
------------------------------------------------------------------------------------
HTTPS is HTTP over a connection secured by TLS(SSL)
	<= HTTP wraped over 'TLS encryped' connection	

	   
	   
------------------------------------------------------------------------------
# Web site 'Random' unavailable error 
------------------------------------------------------------------------------------
nginx (lb) -> Apache(WWW) -> MySQL9DB)

$ cat /var/log/messages		(CentOS)
$ cat /var/log/syslog     	(Ubuntu)
------------------------------------------------------------------------------
### Problem: Nginx LB server is dropping packets in OS Kernel level.
#Jul 10 02:27:54 nlt-lb-01 kernel: nf_conntrack: table full, dropping packet
#.....
#Jul 10 02:27:54 nlt-lb-01 kernel: nf_conntrack: table full, dropping packet
#Jul 10 02:27:59 nlt-lb-01 kernel: net_ratelimit: 169 callbacks suppressed
------------------------------------------------------------------------------

------------------------------------------------------------------------------------
# Iptables
# After installed the IPTABLES, all netfilter related values are installed and set!
------------------------------------------------------------------------------------
## what is CURRENT Count?
$ /sbin/sysctl    net.netfilter.nf_conntrack_count	
$ /sbin/sysctl -a | grep netfilter | grep count
	31774	
$ cat /proc/sys/net/netfilter/nf_conntrack_count


## What is CURRENT 'nf_conntrack_max' value
$ cat /proc/sys/net/netfilter/nf_conntrack_max
	262144
$ /sbin/sysctl net.netfilter.nf_conntrack_max		
	262144

------------------------------------------------------------------------------------
# sysctl 							<= configure kernel parameters at runtime
------------------------------------------------------------------------------------
$ sysctl -a | grep netfilter
$ sysctl -a | grep conntrack
$ sysctl --names --all | grep -i conntrack
 
# 

### Increase the MAX connection issue 'kernel: nf_conntrack: table full, dropping packet"
https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt
http://antmeetspenguin.blogspot.com/2011/01/high-performance-linux-router.html

$ vi /etc/sysctl.conf	
	net.netfilter.nf_conntrack_max = 500000   <= add a VAR and value  

$ /sbin/sysctl -p  (or -f)    <= reload


## check again for "net.netfilter.nf_conntrack_max"
	# sysctl - configure kernel parameters at runtime

$ sysctl -a | grep netfilter | grep max
$ sysctl -a | grep conntrack | grep max

	sysctl: reading key "net.ipv6.conf.all.stable_secret"
	sysctl: reading key "net.ipv6.conf.default.stable_secret"
	sysctl: reading key "net.ipv6.conf.enp0s25.stable_secret"
	sysctl: reading key "net.ipv6.conf.lo.stable_secret"
	net.netfilter.nf_conntrack_expect_max = 1024
	net.netfilter.nf_conntrack_max = 500000
	net.netfilter.nf_conntrack_tcp_max_retrans = 3
	net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300
	net.nf_conntrack_max = 500000

net.netfilter.nf_conntrack_max = 500000   <- increase from 262144 to 500000
	
### Increase the "Hash-Table" Size	
$ cat /sys/module/nf_conntrack/parameters/hashsize
	65536				<= It should be around Max(5000000)/8 = 62500
	
	
	
### you can store about 32 times more conntrack entries than the default,	###
### and get better performance for conntrack entry access.					###
	
	
------------------------------------------------------------------------------
# sysctl
------------------------------------------------------------------------------------
       /sbin/sysctl -a
       /sbin/sysctl -n kernel.hostname
       /sbin/sysctl -w kernel.domainname="example.com"
       /sbin/sysctl -p/etc/sysctl.conf
       /sbin/sysctl -a --pattern forward
       /sbin/sysctl -a --pattern forward$
       /sbin/sysctl -a --pattern 'net.ipv4.conf.(eth|wlan)0.arp'
       /sbin/sysctl --system --pattern '^net.ipv6'
------------------------------------------------------------------------------










------------------------------------------------------------------------------
# CentOS 7 minimum setup Initial Setup
# iptables install
------------------------------------------------------------------------------------

1. dhclient
2. yum -y update
3. sudo yum install -y net-tools bind-utils yum-utils epel-release 
4. sudo yum install -y	ncdu iftop iotop mtr wget net-tools lsof nc vim tcpdump ntp


# Iptables install on CentOS7
sudo yum -y install iptables-services
sudo systemctl enable iptables 
sudo systemctl mask firewalld
sudo systemctl stop firewalld
sudo systemctl start iptables

# Edit IPtables
sudo vi /etc/sysconfig/iptables


# ntp
$ systemctl enable ntpd
$ ntpdate pool.ntp.org

# Change timezone
$ timedatectl set-timezone America/Los_Angeles


# VMware CentOS VM Change enoxxxxxx  to eth0
$ vi /etc/default/grub  
	GRUB_CMDLINE_LINUX line, add  at end of the line
    net.ifnames=0 biosdevname=0 
$ grub2-mkconfig -o /boot/grub2/grub.cfg
$ mv /etc/sysconfig/network-scripts/ifcfg-eno* to  ifcfg-eth* 
$ reboot



# yum clean all  <-clean all cache


# gateway (find out default gateway)
	https://linuxhandbook.com/find-gateway-linux/
	
	$ ip route | grep default

	$ route -n
	
	$ netstat -r -n
	
	
	

### Static IP setup

If you don't have a DHCP server in your network, you must set a static IP address. 
$ vim   /etc/sysconfig/network-scripts/ifcfg-eth0

BOOTPROTO=none
DEVICE=eth0
IPADDR=192.168.1.10 # your IP address
NETMASK=255.255.255.0 # your netmask
NETWORK=192.168.1.0 
ONBOOT=yes									<= Enabling DHCP when bootup

# Add GATEWAY 
$ vi /etc/sysconfig/network

NETWORKING=yes
NETWORKING_IPV6=yes
HOSTNAME=hostname.domainname
GATEWAY=192.168.1.1 # your gateway
Issue the following command to start network on boot:

$ chkconfig network on
Restart your network service:

$ service network restart
Take a look at your network interfaces


# Static DNS
$ cat /etc/resolv.conf
nameserver 8.8.8.8
nameserver 8.8.4.4

$ cat /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE='eth0'
TYPE=Ethernet
BOOTPROTO=none
ONBOOT='yes'
HWADDR=86:d8:f3:f7:d1:a1
IPADDR=162.243.102.23
NETMASK=255.255.255.0
GATEWAY=162.243.102.1
NM_CONTROLLED='yes'
IPADDR2=10.13.0.136
PREFIX2=16
DNS1=8.8.8.8
DNS2=8.8.4.4


########################################################################
# IPTables # iptables
########################################################################

*filter
:INPUT ACCEPT [91:15645]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [132:28522]
#
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
###
### BNGA office ###
-A INPUT -m state -s 65.87.26.0/24    --state NEW -j ACCEPT
-A INPUT -m state -s 64.95.137.172/32 --state NEW -j ACCEPT
#
###
### Apark Home ###
-A INPUT -m state -s 98.234.54.81/32 --state NEW -j ACCEPT
###
#
### BNER ###
-A INPUT -m state -s 87.243.5.176/28   --state NEW -j ACCEPT
-A INPUT -m state -s 89.37.122.128/29  --state NEW -j ACCEPT
-A INPUT -m state -s 89.37.124.86/29   --state NEW -j ACCEPT
-A INPUT -m state -s 162.243.3.59/32   --state NEW -j ACCEPT
#
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
###
COMMIT

---------------------------------------------------------------------------
# BNEA's New  # iptables 
---------------------------------------------------------------------------
# sample configuration for iptables service
# you can edit this manually or use system-config-firewall
# please do not ask us to add additional ports/services to this default configuration
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo   -j ACCEPT

### BNGA office ###
-A INPUT -m state -s 65.87.26.0/24    --state NEW -j ACCEPT
-A INPUT -m state -s 64.95.137.172/32 --state NEW -j ACCEPT
-A INPUT -m state -s 74.217.69.194    --state NEW -j ACCEPT
-A INPUT -m state -s 74.217.69.192    --state NEW -j ACCEPT
-A INPUT -m state -s 107.0.238.61    --state NEW -j ACCEPT

# June
-A INPUT -m state -s 182.72.103.190 --state NEW -j ACCEPT

#
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
---------------------------------------------------------------------------
# Ubuntu-18 Iptables install
https://www.digitalocean.com/community/tutorials/how-to-implement-a-basic-firewall-template-with-iptables-on-ubuntu-14-04

sudo apt-get update
sudo apt-get install iptables-persistent
vi /etc/iptables/rules.v4
sudo service netfilter-persistent start


# CentOS7 Iptables install

sudo yum -y install iptables-services
sudo systemctl enable iptables 
sudo systemctl mask   firewalld
sudo systemctl stop   firewalld

sudo systemctl start  iptables
sudo systemctl stop   iptables
sudo systemctl status iptables


###################################
# IPTables for BNEA Initial Setup #
###################################

*filter
:INPUT ACCEPT [91:15645]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [132:28522]
###
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
###
### BNGA office ###
-A INPUT -m state --state NEW -m tcp -p tcp -s 65.87.26.0/24    -m multiport --dports 22,80,443 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp -s 64.95.137.172/32 -m multiport --dports 22,80,443 -j ACCEPT
#
### BNER SSH Server###
-A INPUT -m state -s 162.243.3.59/32  --state NEW -j ACCEPT
#
### Apark Home ###
-A INPUT -m state -s 67.161.23.77 /32  --state NEW -j ACCEPT
#
###
-A INPUT   -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
###
COMMIT





########################################################################
# Unix System Calls
########################################################################

http://codeschool.org/
https://www.youtube.com/watch?v=xHu7qI1gDPA  (1/2)
https://www.youtube.com/watch?v=2DrjQBL5FMU	 (2/2)

System calls are functions that a programmer can call to perform the services of 
the operating system.

1. Process Control 
	(http://www.softpanorama.org/Internals/System_calls/processes_control.shtml )
	load
	execute
	create process (fork)
	terminate process
	get/set process attributes
	wait for time, wait event, signal event
	allocate, free memory
	
2. File management
	create file, delete file
	open, close, check status of file descriptors (select )
	read, write, reposition
	get/set file attributes; get information about inode  (Stat)
	Commit data to disk (sync)
	
3. Device Management
	request device, release device
	read, write, reposition
	get/set device attributes
	logically attach or detach devices
	execute device specific operation (ioctl)
	
4. Information Maintenance
	get/set time or date
	get/set system data
	get/set process, file, or device attributes
	
5. Communication
	create, delete communication connection
	send, receive messages
	transfer status information
	attach or detach remote devices


#--------------------------------------------------------------------------------------
 -----------------------
|	Kernel code			|
|-----------------------|
|	stack				|		<- starts empty, grows automatically
|-----------------------|
|	heap				|
|	heap				|		<- explicitly allocated during execution
|-----------------------|
|	uninitialized data	|		<- global variables without initial values
|	initialized data	|		<- global variables with initial values
|-----------------------|		<- global variables with initial values
|	code				|		<- a.k.a the "text"
 -----------------------


- process:
	address space					| Stack	|
	user ids						| heap	|
	file descriptors				| heap	|	
	environment						| code	|
	current and root dir
	
- files
- networking sockets			( talk to other program sockets)
- signals						( OS to process)
- inter-process communication	( process to communicate one another)
- terminals						( cmd shell)
- threads						( 
- I/O devices					( storage devices, vga, key/mouse, etc)

e.g. languages
# C
	ssize_t read(int fd, void *buf, size_t count);
# Python
	read(fd)

	
mmap 	<- 'memory map' pages to the process address space
munmap 	<- opposit

address = mmap(5000)
... # do stuff with memory at address
nunmap(address)


mmap fails when NOT enough space if HEAP is too big

### Garbage Collection
 

new process
# FORK
if fork() == 0:							<- ***copy only MEMORY TABLE, Not actual files
	.. //new (child) process
else:
	... // original (parent) process

	# FORK	
	stack	->		RAM		->	fork	-> stack
	heap	->		RAM		->	fork	-> heap
	heap 	->		RAM		->	fork	-> heap
	code	->		RAM		->	fork	-> code
	
# EXEC
	executable --> CODE  load a new program

if fork() == 0:							
	.. //new (child) process
	exec('/games/pong')
else:
	... // original (parent) process

				pid 1 ( init), user 0
			/            \
	pid 2,user 3		pid 3, user 5
	/ 			\			\
pid 4, user id  pid 8  		pid6. user ID
								\
								pid 10, user ID

# Terminate the process
	_exit	
	_exit(0)    <- exit code

# WAIT (Block the process until child process terminates)
	pid = fork()
	if pid == 0:
		// new (child) process
		exec('/games/pong')
	else:
		// original (parent) process
		code = wait(pid)
		
# Environment variables		
	#name=value
TERM=xterm
SHELL=/bin/bash		
USER=greys
MAIL=/var/mail/ted
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin
PWD=/home/ted
EDITOR=vi

# USER ID
User owns the process 

# User account
	/etc/passwd
	superuser/root = user id 0
	privileged to d anything it wants

	echo process has thress user ids:
	"REAL" id: the owning user
	"EFFECTIVE" id: determines privileges
	"SAVED" id: set by exec to match the effective id

	each file and directory is owned by a single user

exec : sets effective and saved IDs when binary file has setuid bit
	
seteuid : sets effective user id

setuid	: sets real, effective, and saved user ids

non-supersuser can only directly set effective id to match the real or saved id.



# When Unix system starts with user account

PID 1(init), user 0		| 
	|					V   fork, exec
pid 2(login), user 0
	|					|	for, setuid, exec
pid 3(shell), user 1780 V

# user groups 
	/etc/group
	- user may belong to multiple groups but has one "primary" group
	- each file and directory is owned by one group
	- each process has a real, effective, and saved group ID
	- binary files have setgid bit
	- setegid and setgid
	

#  9 Permissions
	RWX	 	RWX		RWX
	user 	group 	other

if file_user_id == effective_user_id:
	user class
else if file_group_id == effetive_group_id:
	group class
else:
	other

	# File permission
	read:    can read bytes of files
	write:   can modify bytes of files
	execute: can exec file

	# Directory Permission
	read:    can get names of files
	write:   can add/remove/rename files
	execute: can use in file paths


open  : open/create a file and return a file descriptor
close : release file descriptor
read  : copy bytes from a file to memory (blocks)
write : copy bytes of memory to a file (blocks)

# Syscall WRITE
				Process		  -- |
			 ---------------	 |
			| Write Buffer  | <<-         <- process don't need to wait to write
	HDD	<-	| 		OS      |
			 ---------------

f = open('/alice/tim')
write(f, 'bla, bla')
write(f, 'bla, bla, bla')
close(f)


# Syscall READ
				Process		  <<-|
			 ---------------	 |
	HDD	->	| Read  Buffer   | --         <- process don't need to wait to read
			|OS data and code|
			 ---------------

f = ('/alice/tim')
data = read(f)
close(f)


f = open('/alice/tim')
data = read(f)
while len(data) != 0:
	print(data)
	data = read(f)
close(f)


		marker
		   |
	--------------------------------
	|								|
	byte 0						late byte

								marker
									|
	--------------------------------
	|								|
	byte 0						late byte
	
	Move with Truncate
    Move with lseek
--------------------------------	
descriptor
		\
		description
				\
				buffer
					\
					file on disk
--------------------------------	
	f = open('/alice/tim')
	f2 = open ('/alice/tim')
	write(f, 'bla bla')
	data = read(f2)


pid 75 write 'test'		pid 442 read
	\					/
----------------------------------------	
|       |				|				|
byte 0  pid 580 write  pid46 write  last byte
		'bla bla'

	
# umask	
	get/set default permissions for new files/directories
	oldmask = umask(newmask)
	f = open('/alice/tim')
	write(f, 'bla bla')

# chmod
	change mode: set permissions of an existing file/directory
	chmod('/alice/tim', mask)

# chown
	change owner: set owner of an existing file/directory)
	chown('/alice/tim', user, group)
	
	
### Directories
HD1: partition 1, partition 2, partition 3
HD2: partition 4
SD : partition 5
CD : partition 6, partition 7
	
inode number
	each file and directory in a PARTITION is known by unique inode number
	inode 0		<- no pointer for absent
	inode 1 	<- tracking and keeping of bad section
	inode 2 	<- root directory

An inode is an entry in inode table, containing information ( the metadata ) about 
a regular file and directory


# mkdir



# mktemp <= create a temporary file or directory
	
	(https://stackoverflow.com/questions/11636790/why-do-we-need-mktemp)
	https://www.cyberciti.biz/tips/shell-scripting-bash-how-to-create-empty-temporary-file-quickly.html
	Create  a  temporary  file  or directory, safely, and print its name.  TEMPLATE must contain 
	at least 3 consecutive 'X's in last component.  If TEMPLATE is not specified, 
	use tmp.XXXXXXXXXX,	and --tmpdir is implied.  Files are  created  u+rw,  
	and directories u+rwx, minus umask restrictions.
	
	$ mktemp		<= create a temp directory in /tmp 
	  /tmp/tmp.Am5s8aDQpL
	
	$ mktemp -d -t 
			 -d   <= create a directory, not a file
			 -t   <= interpret  TEMPLATE  as  a  single file name component, 
	
	### From Mac OS ##
	$ temp_dir=`mktemp -d -t floatsign`
	$ echo $temp_dir
	  /var/folders/72/4j_6w5xx23n76_0kxl_ck5jh0000gs/T/floatsign.B9CqI61B


--------------------------------------------------------------------------
# rmdir
--------------------------------------------------------------------------


--------------------------------------------------------------------------
# link 
	link - call the link function to create a link to a file
-------------------------------------------------------------------------- 
	add directory entry
	link('/alice/ian', '/ben/jill')

# unlink
	unlink('/alice/ian')
--------------------------------------------------------------------------

--------------------------------------------------------------------------	
# getdents
	get directory entries
--------------------------------------------------------------------------	
	f = open('/alice')
	entries = getdents(f)
	while len(entries) !=0:
		print(entries)
		entries = getdents(f)
	close(f)
--------------------------------------------------------------------------	
### Directories
--------------------------------------------------------------------------
HD1: /alice/tim
HD2: /
SD : /tim/jim
FD : /jess

--------------------------------------------------------------------------
# mount
--------------------------------------------------------------------------
# umount
--------------------------------------------------------------------------

--------------------------------------------------------------------------
# Absolute path vs relative path
--------------------------------------------------------------------------
# chdir
	chdir('/ben/ian')
	f = open('/alice/tim')
	f2 = open('alice/tim')

# regular file
# directory
# symbolic link
# character device file <- network card, flow in & out
# block device file  	<- storage, e.g. HDD 
# pipe  	- communications 
# socket 	- cross networks


# Partition composed of blocks
--------
block 0
block 1 inode 86
block 2 inode 86    <--> block buffer <--> read/write
block 3		<- a block may only read/written as a whole
.....
block 512
---------

# block device file  
block 0		<- byte 0
block 1
block 2
block 3
block ...     <- last byte

# character device  (buffers)

		<-read		|input buffer |     <- input	
process									    		device
		write ->	|output buffer |	output ->

--------------------------------------------------------------------------		
# FIFOs 
--------------------------------------------------------------------------
	permanent objects and can be created using the mkfifo(1) or mknod(1) command. 
	Inside the program, the FIFO can be created using the mknod command, 
	then opened and read from or written to just like a normal file. 
	The FIFO is normally in blocking mode when attempting to perform read operations.
	
	FIFO( input/output buffers(first in, first out)
							write appends data here	
							|
 input buffer |DDDDDDDDDDDDDD_________________|
			   |
			   device takes data starting here
	
		Block device buffers backed by Storage (HDD)
		Character device buffers not backed by storage
			   
# /dev  directory of device files
	bf = open('/dev/sda1')
	cf = open('/dev/lp0')     <- lp0 printer
	lseek(bf, 100)
	bdata = read(bf)
	cdata = read(cf)
	
# pseudo-device files
	/dev/zero		<- returns zeroed bytes
	/dev/random		<- returns random data
	/dev/null
	
--------------------------------------------------------------------------	
# PIPE (a FIFO as a file)

		  <-read	|FIFO|  <- Write	
process-A							  Process-B
		  write ->	|FIFO|	read ->
		  
	$ ls | tee ls.txt
	$ ls | tee -a ls.txt 			<= -a append		  
--------------------------------------------------------------------------
# mknode 
	make node: create a regular file, device file, or named pipe
--------------------------------------------------------------------------	
	mknod('/ryan/kim', BLOCK, deviceNum)
	mknod('/ryan/erin', CHR, deviceNum)
	mknod('/ryan/tina', FIFO)
--------------------------------------------------------------------------	
# pipe 
	creates a new, anonymouse pipe and returns two file descriptors
--------------------------------------------------------------------------	
	fs = pipe()
	f1 = fs[0]
	f2 = fs[1]

--------------------------------------------------------------------------	
# memory-mapped files
--------------------------------------------------------------------------
	kernel code
	stack
	heap					/ byte n
	mmap'd file		 <- file
	code					\ byte 0


f = open('/brad/mike')
address = mmap(500, f, 200)
... # reading/writeing the allocated memory reads/writes the file
munmap(address)
close(f)

--------------------------------------------------------------------------
### Signals
	signals are sent by KERNEL
--------------------------------------------------------------------------	
  The receiving process
	- performs a default action
	- invokes a handler function
	- blocks it
	- ignores it

  4 types
	- SIGSEGV
	- SIGFPE
	- SIGSTOP
	- SIGCONT
	
	Signal Name	Number	Description
	SIGHUP	1	Hangup (POSIX)
	SIGINT	2	Terminal interrupt (ANSI)
	SIGQUIT	3	Terminal quit (POSIX)
	SIGILL	4	Illegal instruction (ANSI)
	SIGTRAP	5	Trace trap (POSIX)
	SIGIOT	6	IOT Trap (4.2 BSD)
	SIGBUS	7	BUS error (4.2 BSD)
	SIGFPE	8	Floating point exception (ANSI)
	SIGKILL	9	Kill(can't be caught or ignored) (POSIX)
	SIGUSR1	10	User defined signal 1 (POSIX)
	SIGSEGV	11	Invalid memory segment access (ANSI)
	SIGUSR2	12	User defined signal 2 (POSIX)
	SIGPIPE	13	Write on a pipe with no reader, Broken pipe (POSIX)
	SIGALRM	14	Alarm clock (POSIX)
	SIGTERM	15	Termination (ANSI)
	SIGSTKFLT	16	Stack fault
	SIGCHLD	17	Child process has stopped or exited, changed (POSIX)
	SIGCONT	18	Continue executing, if stopped (POSIX)
	SIGSTOP	19	Stop executing(can't be caught or ignored) (POSIX)
	SIGTSTP	20	Terminal stop signal (POSIX)
	SIGTTIN	21	Background process trying to read, from TTY (POSIX)
	SIGTTOU	22	Background process trying to write, to TTY (POSIX)
	SIGURG	23	Urgent condition on socket (4.2 BSD)
	SIGXCPU	24	CPU limit exceeded (4.2 BSD)
	SIGXFSZ	25	File size limit exceeded (4.2 BSD)
	SIGVTALRM	26	Virtual alarm clock (4.2 BSD)
	SIGPROF	27	Profiling alarm clock (4.2 BSD)
	SIGWINCH	28	Window size change (4.3 BSD, Sun)
	SIGIO	29	I/O now possible (4.2 BSD)
	SIGPWR	30	Power failure restart (System V)

--------------------------------------------------------------------------
# kill
	send a signal to a process
--------------------------------------------------------------------------

--------------------------------------------------------------------------
# signal
	set a signal to be handled, ignored, or trigger its default action
	kill (35, SIGSTOP)
	signal(func, SIGFPE)
	
	

####################################################################
HashiCorp Packer 
https://www.packer.io/intro/index.html
https://www.digitalocean.com/community/tutorials/how-to-create-digitalocean-snapshots-using-packer-on-ubuntu-16-04
####################################################################
Packer, by Hashicorp, is a command-line tool for quickly creating identical machine 
images for multiple platforms and environments. With Packer, you use a configuration 
file, called a template, to create a machine image containing a preconfigured operating 
system and software. You can then use this image to create new machines. You can even 
use a single template to orchestrate the simultaneous creation of your production, 
staging, and development environments.

Packer is an open source tool for creating identical machine images for multiple platforms 
from a single source configuration. Packer is lightweight, runs on every major operating system, 
and is highly performant, creating machine images for multiple platforms in parallel. 

Packer does not replace configuration management like Chef or Puppet. In fact, when building images, 
Packer is able to use tools like Chef or Puppet to install software onto the image.

A machine image is a single static unit that contains a pre-configured operating system 
and installed software which is used to quickly create new running machines. Machine 
image formats change for each platform. Some examples include AMIs for EC2, VMDK/VMX 
files for VMware, OVF exports for VirtualBox, etc.






What's special about 169.254.169.254 IP address for AWS? 
https://stackoverflow.com/questions/42314029/whats-special-about-169-254-169-254-ip-address-for-aws

169.254.169.254 is an IP address from the reserved IPv4 Link Local Address space 169.254.0.0/16 
(169.254.0.0 through 169.254.255.255). Similar to the private address ranges in RFC-1918 (10.0.0.0/8, 
172.16.0.0/12, and 192.168.0.0/16) in the sense that this block also can't be used on the Internet, 
Link Local is further restricted to being unreachable via any router¹ -- by design, they only exist 
on the directly-connected network.

AWS needed to create a service endpoint accessible from any system and the selection of an address 
in this block causes it to conflict with no commonly used IP address space. Clever choice.

Presumably this specific address within the block was chosen for its aesthetic appeal or being easy to remember.




####################################################################
Ansible vs Terraform: What are the differences?
####################################################################
Every growing startup or tech organization wants to automate apps and IT infrastructure. 

Ansible - is a simple way to do that. An open-source software provisioning, configuration management(CM), 
		  and application-deployment tool comes with its own declarative language. Ansible is an 
		  automation tool that helps drive complexity away and accelerate DevOps initiatives. 
		  

Terraform - is more of an infrastructure provisioning tool. Terraform talks to VMWare, AWS, GCP, 
		and deploys infrastructure. From the house of HashiCorp, Terraform allows the creation, 
		management and improvement of infrastructure. An open source code drives APIs into 
		declarative configuration files.
		Backed by RedHat Terraform acts like an orchestrator, using Packer for automation. 


What are some alternatives to Ansible and Terraform?
Puppet Labs
Puppet is an automated administrative engine for your Linux, Unix, and Windows systems 
	and performs administrative tasks (such as adding users, installing 
	packages, and updating server configurations) based on a centralized specification.

Chef
Chef enables you to manage and scale cloud infrastructure with no downtime or interruptions. 
	Freely move applications and configurations from one cloud to another. Chef is integrated 
	with all major cloud providers including Amazon EC2, VMWare, IBM Smartcloud, Rackspace, 
	OpenStack, Windows Azure, HP Cloud, Google Compute Engine, Joyent Cloud and others.

Salt
Salt is a new approach to infrastructure management. Easy enough to get running in minutes, 
	scalable enough to manage tens of thousands of servers, and fast enough to communicate 
	with them in seconds.
   Salt delivers a dynamic communication bus for infrastructures that can be used for 
	orchestration, remote execution, configuration management and much more.

Jenkins
	In a nutshell Jenkins CI is the leading open-source continuous integration server. 
	Built with Java, it provides over 300 plugins to support building and testing virtually any project.

AWS CloudFormation
	You can use AWS CloudFormation’s sample templates or create your own templates to 
	describe the AWS resources, and any associated dependencies or runtime parameters, 
	required to run your application. You don’t need to figure out the order in which 
	AWS services need to be provisioned or the subtleties of how to make those dependencies work.
####################################################################



### YAML Valicator 
	https://codebeautify.org/yaml-validator






https://cloud.google.com/blog/products/gcp/sre-fundamentals-slis-slas-and-slos
1. Service-Level Objective (SLO)
2. Service-Level Agreement (SLA)
3. Service-Level Indicator (SLI)




DC/OS (the Distributed Cloud Operating System) is an open-source, distributed operating 
		system based on the Apache Mesos distributed systems kernel. DC/OS manages multiple 
		machines in the cloud or on-premises from a single interface; deploys containers, 
		distributed services, and legacy applications into those machines; and provides networking, 
		service discovery and resource management to keep the services running and 
		communicating with each other.





----------------------------------------------------------------------------------------------------	   
Pritunl VPN Server setup on Ubuntu 18

----------------------------------------------------------------------------------------------------
Installation
https://docs.pritunl.com/docs/installation
----------------------------------------------------------------------------------------------------
sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list << EOF
deb https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.0 multiverse
EOF

sudo tee /etc/apt/sources.list.d/pritunl.list << EOF
deb http://repo.pritunl.com/stable/apt bionic main
EOF

sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com --recv 9DA31620334BD75D9DCB49F368818C72E52529D4
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com --recv 7568D9BB55FF9E5287D586017AE645C0CF8E292A
sudo apt-get update
sudo apt-get --assume-yes install pritunl mongodb-server

sudo systemctl start pritunl mongodb
sudo systemctl enable pritunl mongodb
----------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------
Configuration
https://docs.pritunl.com/docs/configuration-5
----------------------------------------------------------------------------------------------------
# Increase Open File Limit

sudo lsof -p `pgrep pritunl-web` | wc -l 

sudo sh -c 'echo "* hard nofile 64000" >> /etc/security/limits.conf'
sudo sh -c 'echo "* soft nofile 64000" >> /etc/security/limits.conf'
sudo sh -c 'echo "root hard nofile 64000" >> /etc/security/limits.conf'
sudo sh -c 'echo "root soft nofile 64000" >> /etc/security/limits.conf'

----------------------------------------------------------------------------------------------------
Database Setup
https://localhost
----------------------------------------------------------------------------------------------------

pritunl setup-key
cb9edec21b5249cdb72fe0877dcb0a6c

sudo pritunl default-password
	[undefined][2019-10-08 21:29:19,310][INFO] Getting default administrator password
	Administrator default password:
	  username: "pritunl"
	  password: "VoWfKxbAqrpz"
	  apark/linuxpassword
	  
Add Server
Add / attach / Restart	  
https://www.howtoforge.com/how-to-setup-a-vpn-server-using-pritunl-on-ubuntu-1804/   
	   
----------------------------------------------------------------------------------------------------


Rabbitmq

Publisher  <======> Rabitmq(jobs1~4) <======>   Comsumer job1 
												Comsumer job2 
												Comsumer job3 
												Comsumer job4 

	Consumer <- In general in messaging a consumer is an application (or application instance) that 
				consumes messages. The same application can also publish messages and thus be 
				a publisher at the same time.
	   
	
	
	
	

UUID (Universally Unique IDentifier) <- Is on a per-app basis. identifies an app on a device. 
		As long as the user doesn’t completely delete the app, then this identifier will persist 
		between app launches, and at least let you identify the same user using a particular app 
		on a device. Unfortunately, if the user completely deletes and then reinstalls the app 
		then the ID will change.


UDID (Unique Device Identifier) 	 <- A sequence of 40 hexadecimal characters that uniquely 
		identify an ios device. This value can be retrieved through iTunes, or found using 
		UIDevice -uniqueIdentifier. Derived from hardware details like MAC address.	
	
	
	
	
	
https://gist.github.com/centminmod/e050cf02794fb5bcdf3090c28efab202	
python-conflict-fix.sh
-------------------------------------------------------------------------------------------------
#!/bin/bash
# fix python34/python34u and python36/python36u conflicts

if [[ "$(rpm -qa python34u)" ]]; then
  # remove ius community python34u
  yum -y remove python34u python34u-devel python34u-pip python34u-setuptools python34u-tools python34u-libs python34u-tkinter
  # install epel python34
  yum -y install python34 python34-devel python34-pip python34-setuptools python34-tools python34-libs python34-tkinter
fi
# only apply to centos 7 as centos 6 epel doesn't have python36
if [[ -f /bin/systemctl && "$(rpm -qa python36u)" ]]; then
  # remove ius community python36u
  yum -y remove python36u python36u-devel python36u-pip python36u-setuptools python36u-tools python36u-libs python36u-tkinter
  # install epel python36
  yum -y install python36 python36-devel python36-pip python36-setuptools python36-tools python36-libs python36-tkinter
fi
if [[ ! "$(rpm -qa cmake3)" ]]; then
  # reinstall removed dependencies from above removed ius community packages
  yum -y install cmake3 cmake3-data
fi	
-------------------------------------------------------------------------------------------------	
	
	
	cd 
	
iTerm2
split 
	CMD+d	
	
	
	
	
Slack
Webhook testing
$>>> curl -X POST -H 'Content-type: application/json' --data '{"text":"Connection Testing!"}' https://hooks.slack.com/services/T0L71L0HZ/BA64RKL3
	
	
	
	
	
	
	
	
	
	
