아무리 바빠도 해야할 일 - 책 
돈이 많아도 하지 말아야 할일 - 도박, 마약
1.역사-역사의식 책임 | 2.문화-감성 | 3.경영-논리 | 4.철학-생각에 방법 | 5.사회학 인문학-인간 관계

"얼마나 아는가 보다  어떻게 실행하는가?"

S.M.A.R.T.	( ¯\_(ツ)_/¯ )
1.Specific	2.Measureable	3.Action-Oriented	4.Realistic	 5.Time-Limited

### Drucker's 5 Questions ###
1. What is  our mission?
2. Who  is  our customers?
3. What is  the customer value?
4. What are our result?
5. What is  our plan?


---------------------------------------
### Windows CMD kill process
---------------------------------------
http://tweaks.com/windows/39559/kill-processes-from-command-prompt
c:\taskkill /IM pdflite.exe /F 

# Clear Type Text Tuner
C:\Windows\System32\cttune.exe

# Rename file 'F2'

# Pin to Taskbar  			<= on program, right click, pin to taskbar

# bring up the Task Manager 
	Ctrl+Shift+Esc 
	Windows + R   <= run
	Windows + E	  <= Explorer
	Windows + D   <= Show Desktop
	Windows + L   <= Lock
	Windows + F   <= search

Windows 10: Narrator Voice
Start: 	Windows Key + Enter
Stop:  	Caps Lock   + Esc
	
### ABBYY Temp files process	
How to change the drive location so ABBYY Temp files process to a chosen drive.
When scanning documents through the ABBY process component there are Bitmap files being stored in the "C:\Windows\Temp"
Bitmap Image files are stored within the directory while being processed in the ABBY component. The Bitmap Image files are removed automatically when the documents are successfully processed. When the jobs fail to process or processing these images are stored in the Temp folder without being removed and causes space to be taking.
Resolution / Workaround
You can redirect these files to be processed onto another directory by changing the environmental variable settings:

1. Right click on "my computer" and hit properties.
2. Go to the advanced tab and click environmental variables.
3. Click on the TMP variable under "User variables for Administrator ," and click edit.
4. Change the value to the specified directory you wish to use.

Carriage Return(CR) <= 캐리지 리턴 문자 (0x0D, \r)는 커서를 다음 줄로 진행하지 않고 행의 시작 부분으로 이동합니다. 
						이 문자는 Commodore와 초기의 Macintosh 운영 체제 (OS-9 및 이전 버전)에서 줄 바꿈 문자로 사용됩니다. 
Line Feed (LF)		<= 라인 피드 문자 (0x0A, \n)는 행의 시작 부분으로 돌아 가지 않고 커서를 다음 행으로 이동합니다. 
						이 문자는 UNIX 기반 시스템 (Linux, Mac OSX 등)에서 줄 바꿈 문자로 사용됩니다.
End Of Line (EOL)	<= 라인 끝  문자 (0x0D0A, \r\n)는 두개의 ASCII 문자이며 CR과 LF 의 조합입니다. 
						커서를 다음 줄과 그 줄의 시작 부분으로 이동합니다. 이 문자는 Microsoft Windows, Symbian OS를 
						포함한 기타 대부분의 비 UNIX 운영 체제에서 줄 바꿈 문자로 사용됩니다.

--------------------------------------- 
### Notepad++  (Settings > Short cut Mapper Menu)
---------------------------------------

1. 	Ctrl + L					Delete Current line
	Ctrl + D					Copy(Duplicate) line
	Ctrl + Alt + Enter			Insert blank line Above ^
	Ctrl+Alt+Shift+Enter		Insert blank line Below v
	
2. Auto-completion
	Ctrl+Space			Function completion
	Ctrl+Enter			Word completion
	Ctrl+Shift+Space	Function parameter hint
3. Move
	Ctrl+End			Bottom of page
	Ctrl+Home			Top of page
4. Basic File Management
	Ctrl+N				Create new document.
	Ctrl+P				Display Print... dialogue box.
	Ctrl+O				Display Open File... dialogue box.
	Ctrl+S				Save current document.
**	Ctrl+Shift+S		SAVE all open documents **
	Ctrl+Alt+S			Display Save As... dialogue box.
	Ctrl+W				Close current document.
	Alt +F4				Exit Notepad++
	
---------------------------------------
### Chrome ###
--------------------------------------- 
Clean Cache - a hard refresh of the page (ctrl+shift+R)
Plugins install
	# Revolver 			<- Rotate Tabs
	# AdBlock
Alt + Home				Open your homepage.
Alt + Tab				Toggle between browser windows
Alt + Left 0Arrow		Back a page.
Alt + Right Arrow		Forward a page.
F11						Full screen
Esc						Stop page or download from loading.
Ctrl + Shift + I		Development Tools*** 
Ctrl + Shift + O		Open the Bookmark manager.
Ctrl + Shift + N		New Incognito Window
Ctrl + H				Open history in a new tab
Ctrl + K or Ctrl + E	Perform a Google search
Ctrl + T				Opens a new tab.
Ctrl + U				View a web page's source code

---------------------------------------
### MS Word ###
---------------------------------------
Removing empty lines
Ctrl+f -> replace -> More => Special => Paragraph Mark => ^p   <= single line ^p^p  <= double empty lines


---------------------------------------------------------------------------------
### IPv4 
32bit is 2^32 =~ 4.3 billion    <=  IPv4 address is 32bit 
---------------------------------------------------------------------------------
just without the prefix(e.g. www) as 'something.com' is a 'naked domain.'

-----------------------------------------------------------------------------------	
### Terminology
-----------------------------------------------------------------------------------	

### I
IFTTT	<= IF This Then That

### M
MEAN stack <= MongoDB, Express.js, AngularJS, and Node.js

### S
SSL 	<= Secure Sockets Layer

SMACK Stack
	Spark - fast and general engine for distributed, large-scale data processing
	Mesos - cluster resource management system that provides efficient resource
			isolation and sharing across distributed applications
	Akka -  a toolkit and runtime for building highly concurrent, distributed, 
			and resilience message-driven app on the JVM
	Cassandra - distributed, highly available DB designed to handle large amounts of data
			across multiple datacenters
	Kafka - a high-throughput, low-latency distributed messaging system designed for
			handling real-time 'data feeds'

### T ###
TLS 	<= Transport Layer Security (TLS) – and its predecessor, Secure Sockets Layer (SSL)




-----------------------------------------------------------------------------------	
-----------------------------------------------------------------------------------	
### LINUX ###
-----------------------------------------------------------------------------------	
-----------------------------------------------------------------------------------	

### HELP ###

# apropos 			<= Search the manual page names and descriptions
	$ apropos grep
	$ apropos -r REGEXofUNIXCOMMAND or Description

$ whatis 			<= Searches a set of database files containing short descriptions
	$ whatis grep

$ help
	$ man --help
	
$ man -k REGEX or UNIXCOMMAND

$ info				<= Longer document
	$ info grep
-----------------------------------------------------------------------------------	
### Command and file Locations ###
-----------------------------------------------------------------------------------	
# Find the command location
	$ whereis  cmd				<= <= Find binary /source / manual for commandD
		$ whereis ls
			ls: /usr/bin/ls /usr/share/man/man1/ls.1.gz
		$ whereis ssh_config
			ssh_config: /usr/share/man/man5/ssh_config.5.gz
		
	$ which    cmd				<= shows the full path of (shell) commands.
		$ which ls
         alias ls='ls --color=auto'
         /usr/bin/ls
		
	$ find / -name  cmd
		$ sudo find / -name ls	<= SUDO requires to scan all /(root) subdirectories
		
	$ locate  cmd				<= CentOS7 $yum install mlocate then $updatedb
		$ updatedb
		$ locate ls
		  scan all files with 'ls' 
	
		$ locate ssh_config		 				<= DB based Find file 
		  /etc/ssh/ssh_config
		  /etc/ssh/ssh_config.rpmnew
		  /usr/share/man/man5/ssh_config.5.gz


-----------------------------------------------------------------------------------	
Daily Linux CMD List
-----------------------------------------------------------------------------------	
###############
1. SHELL	  #
###############


### Font Type <= Lucia Console, 12pt

### Shell's Color, History, Time stamp, Size ### 

### Green prompt
echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'"  >> ~/.bashrc && source ~/.bashrc 
echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'"  >> ~/.bashrc && 		. ~/.bashrc 

### Green & Red promt
========================================================================================
echo 'export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"' >> ~/.bashrc	&& source ~/.bashrc	 
========================================================================================
### change .bashrc within shell to actiavate a new alias
$ source ~/.bashrc	

### History Control Options
echo 'export HISTCONTROL="ignorespace:erasedups:ignoredups"' >> ~/.bashrc	<= Ignore Duplication
echo 'export HISTIGNORE="history*:ll*:ls*:"' >> ~/.bashrc					<= Ignore list
echo 'export HISTTIMEFORMAT="%F > "' >> ~/.bashrc							<= History time stamp(man date)
echo 'export HISTSIZE=10000' >> ~/.bashrc									<= Default is 1000
###

### Bash Log Out Message
	
	$ vi ~/.bash_logout			<= user account
		echo "You are logging out! GoodBye ^^*
	$ vi /etc/bash.bashrc


### History with time stamp
	export into ~/.bash_profile as well as /root/.bash_profile
	$ export HISTTIMEFORMAT="%F %T "
	
	# Modify .bashrc Path and reload
	http://docs.aws.amazon.com/cli/latest/userguide/awscli-install-linux.html#awscli-install-linux-path
	$ export PATH=~/.local/bin:$PATH		<= export  - Modify
	$ source ~/.bash_profile				<= source  - reLoad
	$ . ~/.bash_profile						<= source  - reLoad
	$ source .bashrc						<= reload .bashrc
	
-----------------------------------------------------------------------------------
### Copy & Paste or create a 'shellenv.sh' script and PSSH to all servers ###
	<< EOF here document method  << STDIN, >> STDOUT 
-----------------------------------------------------------------------------------
cat << EOF >> ~/.bashrc
	export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"				# Color
	export HISTCONTROL="ignorespace:erasedups:ignoredups"		# Ignore space, dups, 
	export HISTIGNORE="history*:ll*:ls*:"						# Ignore ls ll
	export HISTTIMEFORMAT="%F > " 								# Date format
	export HISTSIZE=10000
	EOF

# Reload .bashrc 
	$ source ~/.bashrc 
# Copy to servers  
	$ pssh -O StrictHostKeyChecking=no -h serverList.txt -l apark -A -I < ./shellenv.sh
-----------------------------------------------------------------------------------
	
### 9x Color Codes	for Fonts
	Reset   = 0
	Black   = 30
	Red     = 31
	Green   = 32
	Yellow  = 33
	Blue    = 34
	Magenta = 35
	Cyan    = 36
	White   = 37
	
	$ echo -e "\e[1;31m This is red text. \e[0m"	<- \e[1;31m (\)escape string sets(-e ON) color
		This is red text.							<- \e[0m reset to default color
	
	$ echo -e '\e[1;31m	"Red text" \e[0m'
	
### Color Codes	for Colored Background
	
	$echo -e "\e[1;42m This is green background! \e[0m"
	    This is green background!
	
	Reset   = 0
	Black   = 40
	Red     = 41
	Green   = 42
	Yellow  = 43
	Blue    = 44
	Magenta = 45
	Cyan    = 46
	White   = 47
	
	
-----------------------------------------------------------------------------------------	
########################
###   Boot Process   ###  
########################

Firmware(POST or UEFI) => Boot loader => Kernel => Initialization Stages

1. POST(UEFI | BIOS) => Exec Boot Loader

2. Boot Loader(GRUB2) => load Kernel
	BIOS: /boot/grub2/grub.cfg
	UEFI: /boot/efi/EFI/rehat/grub.cfg

3. Kernel
	Loads the RAMDISK(temp root file system) to RAM
	Loads device drivers and config files from RAMDISK
	Unmounts RAMDISK and mounts root filesystem
	Starts the initialization stage

4. Initialization Stages
	Kernel starts the 1st process ( init(oldest) => upstart => systemd(latest) )
	Systemd starts system services
	Systemd starts Login shells and GUI interface

5. OS is ready to be used
-----------------------------------------------------------------------------------------
	
# CentOS 7 uses theSystemd Targets
	Target is a specific configuration(run level)
	Default target is graphical.target
	Systems can be booted into different targets. 
		e.g. rescue mode
		

# CentOS 7 uses the GRUB2
$ cat /boot/grub2/grub.cfg				<= BIOS system
$ cat /boot/efi/EFI/centos/grub.cfg		<= UEFI system

$ file   /etc/grub2.cfg					<= use 'file' command to find out the link
/etc/grub2.cfg: symbolic link to `../boot/grub2/grub.cfg'

$ sudo less -N /boot/grub2/grub.cfg

   # It is automatically generated by grub2-mkconfig using templates
   # from /etc/grub.d and settings from /etc/default/grub

### BEGIN /etc/grub.d/00_header ###  	<= grub menu header
### BEGIN /etc/grub.d/10_linux ###		<= Actual Kernel Choice

linux16 /boot/vmlinuz-3.10.0-693.17.1.el7.x86_64 root=UUID=b3...aa 
ro console=tty0 console=ttyS0,115200n8 crashkernel=auto console=ttyS0,115200 LANG=en_US.UTF-8

#Init ramdisk
initrd16 /boot/initramfs-3.10.0-693.17.1.el7.x86_64.img

### BEGIN /etc/grub.d/40_custom ###
# This file provides an easy way to add custom menu entries.  Simply type the
# menu entries you want to add after this comment.  Be careful not to change
# the 'exec tail' line above.
### END /etc/grub.d/40_custom ###


$ cat /etc/default/grub
GRUB_TIMEOUT=1
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=saved					<= Default Kernel is saved in another file.
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL="serial console"
GRUB_SERIAL_COMMAND="serial --speed=115200"
GRUB_CMDLINE_LINUX="console=tty0 crashkernel=auto console=ttyS0,115200"   <=KERNEL OPTIONS
GRUB_DISABLE_RECOVERY="true"

$ cat /boot/grub2/grubenv
# GRUB Environment Block
saved_entry=CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core)
###############################################################  <= CAN'T READ AWK to print out

### Check boot up list menu
$ awk -F\' '/menuentry/{print $2}' /boot/grub2/grub.cfg

CentOS Linux 7 Rescue 08...04 (3.10.0-693.17.1.el7.x86_64)		<= 0
CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core)				<= 1
CentOS Linux (3.10.0-693.11.6.el7.x86_64) 7 (Core)				<= 2
CentOS Linux (0-rescue-60...9c)           7 (Core)              <= 3

# Change menu
$ grub2-set-default 1

# Check back in 
$ /etc/default/grub

GRUB_CMDLINE_LINUX="console=tty0 crashkernel=auto console=ttyS0,115200" <= Kernel option
	# change with " " 

# Reconfigure GRUB
$ grub2-mkconfig -o /boot/grub2/grub.cfg			<= BIOS
$ grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg   <= UEFI


#### Custom GRUB menu

$ sudo cat /etc/grub.d/40_custom  /boot/grub2/grub.cfg > ~/40_custom
$ vi ~/40_custom     <= edit
$ sudo cp ~/40_custom   /etc/grub.d/40_custom




 
 
 
 



# Discovering Which Bootloader the Droplet Uses
https://www.digitalocean.com/community/tutorials/how-to-update-a-digitalocean-server-s-kernel
### Grub 2 ###
Debian 7, 8
Ubuntu 12.04, 14.04, 16.04
CentOS 7
Fedora 23
### Grub 1 ###
CentOS 5, 6





-----------------------------------------------------------------------------------------	
########################
### System services  ### 
########################

	SysV init process tree( from ATT Linux 80's)

	init --> abrtd
		  |-> agetty
		  |--> auditd --{gdbud}
		  
	Runlevel 3	  
		cups, httpd, iptables  
	
	Cons
		slow startup
		No service dependencies
		No persistent network
	to => Launchd & Upstart but ==> SystemD
	
	
	
	### SystemD ###
	https://www.digitalocean.com/community/tutorials/how-to-use-systemctl-to-manage-systemd-services-and-units
	https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files	
	# Units(service name) <= target of most of actions which are resources, defined by unit files
 	
	$ systemctl						<=
	
	$ systemctl list-units			<= only active units
		UNIT: The systemd unit name
		LOAD: Whether the unit's configuration has been parsed by systemd. 
				The configuration of loaded units is kept in memory.
		ACTIVE: A summary state about whether the unit is active. 
				This is usually a fairly basic way to tell if the unit has started 
				successfully or not.
		SUB: This is a lower-level state that indicates more detailed information 
			about the unit. This often varies by unit type, state, and the actual 
			method in which the unit runs.
		DESCRIPTION: A short textual description of what the unit is/does.
		
	$ systemctl list-units	--all
	
	**$ systemctl list-unit-files  <= displaces all 
	
	$ systemctl list-unit-files -at service
		UNIT FILE                             STATE
		arp-ethers.service                    disabled
		atd.service                           enabled

	$ systemctl list-units -at service
		UNIT                                LOAD      ACTIVE   SUB     DESCRIPTION
		atd.service                         loaded    active   running Job spooling tools
		auditd.service                      loaded    active   running Security Auditing Service
	
	$ systemctl list-units -t service --state running
		UNIT                     LOAD   ACTIVE SUB     DESCRIPTION
		atd.service              loaded active running Job spooling tools
		auditd.service           loaded active running Security Auditing Service
		nginx.service            loaded active running The nginx HTTP and reverse proxy server
		ntpd.service             loaded active running Network Time Service
	
	$ systemctl enable | disable | status iptables
		# /etc/systemd/system/basic.target.wants/iptables.service
	
	$ systemctl cat sshd				<= 'cat' concatenate the Unit file for 'sshd.service'
		# /usr/lib/systemd/system/sshd.service
		[Unit]
		Description=OpenSSH server daemon
			$ systemctl is-enabled nginginx 
		disabled
	
	$ sudo systemctl enable nginx
		Created symlink from '/etc/systemd/system/multi-user.target.wants/nginx.service' to 
			'/usr/lib/systemd/system/nginx.service.'
	
	$ cat /etc/systemd/system/mucat lti-user.target.wants/nginx.service
		[Unit]
		Description=The nginx HTTP and reverse proxy server
		After=network.target remote-fs.target nss-lookup.target

		[Service]
		Type=forking
		PIDFile=/run/nginx.pid
	
	$ systemctl is-enabled sshd	
	$ systemctl is-active  sshd 
	$ systemctl is-failed  sshd 
	$ systemctl reload-or-restart sshd.service
	
	$ systemctl list-dependencies sshd.service
	
	$ ls -l /etc/systemd/system/multi-user.target.wants
		lrwxrwxrwx. 1 root root 36 Jan 25 10:14 sshd.service -> /usr/lib/systemd/system/sshd.service

	$ systemctl list-units --all --state=active
	$ systemctl list-units --all --state=inactive
	$ systemctl list-units --type=service
	
	# low level
	$ systemctl show sshd.service
	
	# Masking and unmasking units( making disable from start up by linking to /dev/null)
	$ systemctl mask | unmask sshd.service  		<= start will fail due to masked

	# Edit Unit file
	$ systemctl edit sshd.service
			
	$ sudo systemctl edit nginx
	Editing "/etc/systemd/system/nginx.service.d/override.conf" <= precedence over nginx.conf
	
	$ sudo systemctl edit --full nginx
	# to Remove
	$ sudo rm -r /etc/systemd/system/nginx.service.d	<= snippet
	$ sudo rm /etc/systemd/system/nginx.service			<= full
	# Restore default
**  $ sudo systemctl daemon-reload						<= Restore to system default
	
	
	### Journald ###
	$ journalctl			<= log information from applications and the kernel
		$ vi /etc/systemd/journald.conf
	
	$ journalctl -b					<= current boot
	$ journalctl -k	  <=> dmesg     <= Kernal
	$ journalctl -xe				<= -x  Augment log lines with explanation texts from the message 
										   catalog.
									<= -e  jump to the end of the journal 
	
	
	
	
	
	
	
8.	PROCESS
	$ ps 				<= Unix, BSD, GNU types
		1 UNIX 		<= ps -ef (Unix/POSIX)    which may be grouped and must be preceded by a dash.
		2 BSD 		<= ps aux (Ubuntu/Debian) which may be grouped and must not be used with a dash.
		3 GNU 		<= ps -ef (CentOS)		  Standard, long options, which are preceded by two dashes.
	$ ps -e			<= everything
	$ ps -eH		<= Hierachical 
	$ ps ax | ps -ef 		<= BSD|Unix for every process (GNU e.g. CentOS)
	$ ps aux		<= every process + -u user (BSD e.g. Ubuntu)
	$ ps -elF		<= Long, 15 columes of options	
	
	$ ps -e --format uid,pid,ppid,%cpu,cmd  <= use '--format' for custom option
	$ ps -G  group_name
	$ ps -C nginx --format pid,ppid,%cpu,cmd --sort %cpu
		PID  PPID %CPU CMD
		23976 23968  0.0 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf
		23984 23976  0.0 nginx: worker process

	$ ps -e --format uid,pid,ppid,tty,%cpu,cmd --sort %cpu
	$ ps -e --format uid,pid,tty,rss,cmd --sort rss   <= memory usage, cmd sorted
	$ ps -U root				<= -u & -U same(user)
	$ ps -U apark --format %mem | awk '{memory +=$1};END {print memory}'  	 <= user memory usage
	
	### Process tree format
	$ ps axjf  		<= BSD PS tree format
	$ ps -ejH		<= CentOS 
	
	$ ps auxf

# pstree
	https://www.cyberciti.biz/faq/unix-linux-pstree-command-examples-shows-running-processestree/
	$ yum -y install psmisc
	$ pstree
	$ pstree pid
	$ pstree user
	
	$ pstree -p					<= show PIDS for each process name
		systemd(1) ─┬─agetty(1166)
					├─agetty(1167)
					├─auditd(435)───{auditd}(436)

	$ pstree -n				<= To sort processes with the same ancestor by PID instead of by name 
	$ pstree -np				i.e. numeric sort, pass the -n options as follows:
		systemd(1) ─┬─systemd-journal(357)
					├─auditd(435)───{auditd}(436)
					├─systemd-logind(552)
					├─dbus-daemon(553)───{dbus-daemon}(554)

	
	
9. Process Niceness/Priority
	$ nice -2 top			<= if only 1 '-' +2,  '--' -2
	$ ps -C top --format pid,nice,cmd,%cpu		<= -C  command name
		PID  	NI CMD
		21112   2  top
	$ sudo nice --2 top		<= If no sudo, it will set to nice value '0'<- max permission
	$ sudo renice -5 top	<= sudo requires
	
10. Manage Process Jobs
	$ watch -n 5 'ps -C dd --format pid,cmd,%cpu'		<= Term1    
	
	$ dd if=/dev/zero of=/dev/null						<= Term2    
	
	$ Ctrl+z											<= Term2 "Stopping process"    
		$ jobs
			[1]+  Stopped                 dd if=/dev/zero of=/dev/null
		$ bg 1
			[1]+ dd if=/dev/zero of=/dev/null &
		$ jobs
			[1]+  Running                 dd if=/dev/zero of=/dev/null &
		$ fg
			dd if=/dev/zero of=/dev/null
		$ Ctrl+z
		$ bg
		$ Ctrl+c
		$ dd if=/dev/zero of=/dev/null &
			[1] 24933
		$ killall dd

### pidof
	pidof -- find the process ID of a running program.
	$ pidof nginx
		19177 19176 19175 19174 19173 6535 6534 6533 6531 6315

	
	
	
10. Kill Signal
	$ kill -l 			<=list of singals
	kill, killall, pkill
	http://www.thegeekstuff.com/2009/12/4-ways-to-kill-a-process-kill-killall-pkill-xkill/
	https://unix.stackexchange.com/questions/317492/list-of-kill-signals
	$ pidof    sleep
	$ pgrep    sleep
	$ kill     15558 15559 15560		<= same as kill -15(greacefull)
	$ kill -9  15558 15559 15560		
	$ pkill   sleep
	$ killall sleep					
	$ killall -u user_name				<= Kill all related to user
	
	
9. Monitoring TOP
	$ top
			l 	<= load avg & Uptime
			1 	<= All CPU usage
			t 	<= toggle CPU states
			m	<= toggle betwen memory states
			f	<= FIELD management
				select and spacebar to enable
			c	<= toggle between cmd name & cmdline
			u	<= user name
			k	<= kill task
			r	<= renice,  choose PID, and value
			
		shift + m	<= Memory usage
		shift + p	<= Process usage
		shift + t	<= Time usage
		shift + n	<=(PID Number)
		
	### Monitoring Scripts		
	https://bash.cyberciti.biz/shell/monitoring/

	
	
### wait: wait [id]
    Wait for job completion and return exit status.
	https://unix.stackexchange.com/questions/42287/terminating-an-infinite-loop/121391#121391
	$ wait $!

	
#!/bin/bash
# The script uses trap to catch ctrl-c (or SIGTERM), kills off the command 
# (I've used sleep here as a test) and exits.

cleanup (){
kill -s SIGTERM $!
exit 0
}

trap cleanup SIGINT SIGTERM

while [ 1 ]
do
    sleep 60 &
    wait $!
done












--------------------
### BASH Startup ###
--------------------
# Login Shell Process when everytime login to shell
----------------------------------------------------------------------------------------------------
# Login Shell Process
	### Steps of Loading profiles ###
	1. /etc/profile 			<= systemwide env and shell vars
	2. /etc/profile.d/*.sh  	<= systemwide env and shell vars
	3. ~/.bash_profile			<= User env and shell vars
	4. ~/.bashrc				<= Execute /etc/bashrc
	5. /etc/bashrc				<= Systemwide alias and shell functions
	6. ~/.bashrc				<= Execute /etc/bashrc
	### User Successfully LOGIN to the SHELL ###

# Non-Login Shell Login Process
	1. ~/.bashrc				<= Execute /etc/bashrc
	2. /etc/bashrc				<= Systemwide alias and shell functions
	3. ~/.bashrc				<= Execute /etc/bashrc

# Variables for ALL USERS add to 
	/etc/profile 			<= systemwide env and shell vars
	/etc/profile.d/*.sh  	<= systemwide env and shell vars

# Variables for individual USERS 
	~/.bash_profile			<= User env and shell vars






----------------------------------------------------------------------------------------------------
$ /home/apark/.bash_profile  <= settings, environment profile
$ /home/apark/.bashrc		 <= current & new vars  e.g. alias

	# .bash_profile <= is read when bash is invoked as login shell.
	# .bashrc       <= is executed when a new shell is started.

PATH=$PATH:/usr/local/bin

Keyboard => Terminal => SHELL	=> |Kernel|
screen 	 <= Terminal => SHELL	<= |Kernel|

1. Shell checks if it's a built-in command.
2. Shell checks if it's an alias of a command.
3. Shell checks if the command is on the hard disk.
sh(Bourne Shell)
	- basic shell
	- POSIX-compliant shell
	- Expose array indices
	- RegEx 
	- Increment assignment operator
	
Other shells
zsh, ksh, csh

-----------------------------------------------------------------------------------
https://mywiki.wooledge.org/BashGuide/CommandsAndArguments
# Alias  
	a word that is mapped to a string. Whenever that word is used as a command, 
	it is replaced by the string it has mapped.

# Function 
	a name that is mapped to a set of commands. Whenever the function is used as a 
	command, it is called with the arguments following it. Functions are the basic 
	method of making new commands.

# Builtin
	certain commands have been built into Bash. These are handled directly by the 
	Bash executable and do not create a new process.

# Executable 
	a program that can be executed by referring to its file path (e.g. /bin/ls), 
	or simply by its name if its location is in the PATH variable.


-----------------------------------------------------------------------------------
http://vimdoc.sourceforge.net/htmldoc/syntax.html#%3ahighlight
### Vi environment set-up (setting)
	Create a .vimrc file in home directory
	
$ vi .vimrc
----------------------------------------------------
:set nu
:set tabstop=4
syntax on
colorscheme desert
-------------------------------------------------------
	
	####	Vim color scheme enable   ###
	http://www.server-world.info/en/note?os=CentOS_7&p=initial_conf&f=7

	1. Install vim-enhanced
	$ yum -y install vim-enhanced 

	2. E dit profile(global profile configuration for all users)
	$ vi /etc/profile
	# add at the last line
	$ alias vi='vim'

	3. reload without relogin
	$ source /etc/profile 

	4.Configure vim. 
	( Apply to a user below. If you applly to all users, Write the same settings in '/etc/vimrc',  
	some settings are applied by default though. )

	# Add following 
	syntax on
	colorscheme desert
	---------------------------------------------------------------------------------------------------
esc highlight


# VI - number line
	$ vi -V 			<= version check
	:set nu 			<= set number On
	:set nu!   			<= set number Off
	:e file_name        <= open & create  file
 	:w new_name 	    <= save as to new_name
	
	# Cut and Paste(v/V-d/y-p/P)

		 a. Position the cursor where you want to begin cutting.
	V    b. Press v to select characters (or V - whole lines).
		 c. Move the cursor to the end of what you want to cut.
	d    d. Press d to cut (or y to copy).
		 e. Move to where you would like to paste.
	p    f. Press P to paste before the cursor, or p to paste after.

	yy 	<=	shift+v to copy 
	pp	<=	shift+p to paste


	# Undo
	esc then uu
	
	# Change String like "SED" stream editor
	http://vim.wikia.com/wiki/Search_and_replace
	
	# within VI, change 'foo' to 'bar'
	:%s/foo/bar/g				<=
	
	# Search with in VI
	/search_word
	

----------------------------------------------------------------------------------------------------	
6. Environment variable check
	### to check the environment variables
	$ printenv     			
	$ env					
	### env == printenv ### they are same
		
# Check $SHELL variable	
	$ printenv SHELL
	$ printenv | grep SHELL
	$ env | grep SHELL
	$ echo $SHELL		  	
	$ set | grep SELL					<= check current SHELL's variables
=> they all same '/bin/bash <= 

	
# env <= modified the environment variables
		-  Set each NAME to VALUE in the environment and run COMMAND.
	
# set <= is a SHELL command to set the value of a shell attribute variable; 
		 these are internal variables used by the SHELL, Not Environment.
		- 'Set' or 'unset' values of shell options and positional parameters.
		- Change the value of shell attributes and positional parameters, or
			display the names and values of shell variables.

# printenv <= print all or part of environment
	
	$ set -o 				<= -o Options,  Inherite from BASH or other Shells
	$ set -o posix			<= Turn on POSIX, see shell variables, remove all shell functions
	$ set +o posix			<= +o removes POSIX mode

	
# Environment VS SHELL Variables 	
	1. Environment Variables
		Variables that are defined for the current shell and are inherited by any child shells
		or processes
		
	2. SHELL Variables 
		Variables that are contained exclusively within the shell in which they were defined.

		
### export 	<= (default same as '-p')Set export attribute for shell variables.
			Marks each NAME for automatic export to the environment of subsequently
			executed commands.  If VALUE is supplied, assign VALUE before exporting.
			Options:
			-f   shell functions
			-n   remove the export property from each NAME
			-p   display a list of all exported variables and functions

	$ VAR=TEST				<= creating variable 'VAR'
	$ set | grep VAR		
		VAR=TEST
	$ unset VAR				<= Removing 'VAR'
	
	$ env | grep VAR
	** No Result
	$ printenv | grep VAR
	** No Result 			<= VAR is not part of environment
	
# To put it into Env variables, use 'export'
	$ export VAR			
	$ env | grep VAR		
	  VAR=TEST
	$ export -n VAR			<= 	


	
	
	
	
	
	
### socat - Multipurpose relay (Socket CAT)

	
### shopt						<= specific options for BASH Shell
	$ shopt						<= list of Shell Option
	$ shopt -s extglob			<= turn set  (on) extended glob
	$ shopt -u extglob			<= turn unset(off) extended glob
	$ shopt | grep extglob
	
	
	
	$ env
	
	### Environment Variable
	# Value pair with = to SORTING
	# var=value
	The environment variables of a process exist at runtime, and are not stored in some file or so. 
	They are stored in the process's own memory (that's where they are found to pass on to children). 
	But there is a virtual file in
	/proc/pid/environ

	This file shows all the environment variables that were passed when calling the process 
	(unless the process overwrote that part of its memory — most programs don't). The kernel makes 
	them visible through that virtual file. One can list them. For example to view the variables 
	of process 3940, one can do

	$ cat /proc/3940/environ | tr '\0' '\n'   <= 'tr' replaces the 'zero' into a newline.
		Each variable is delimited by a 'binary zero' from the next one. 'tr' replaces the zero into a newline.
	
	# Example
	$ pgrep bash
		3179
	$ cat /proc/3179/environ
	USER=aparkLOGNAME=aparkHOME=/home/aparkPATH=/usr/local/bin:/usr/binMAIL=/var/mai
	l/aparkSHELL=/bin/bashSSH_CLIENT=10.100.5.190 23898 22SSH_CONNECTION=10.100.5.19
	0 23898 10.100.5.155 22SSH_TTY=/dev/pts/0TERM=xtermSELINUX_ROLE_REQUESTED=SELINU
	X_LEVEL_REQUESTED=SELINUX_USE_CURRENT_RANGE=XDG_SESSION_ID=14840XDG_RUNTIME_DIR=	
	
	$ cat /proc/3179/environ | tr '\0' '\n'  <= 'tr' translate(replaces) the binary zero into a newline.
											 <=	'\n' <- new line
											 <= '\0' <- NULL value, no space???
		USER=apark
		LOGNAME=apark
		HOME=/home/apark
		PATH=/usr/local/bin:/usr/bin
		MAIL=/var/mail/apark
		SHELL=/bin/bash
		......


	### Environment var search the PATH
	$ echo $PATH
	$ env | grep PATH
	$ set | grep PATH
	
	$ PATH="$PATH:/home/apark/bin
	$ export PATH
	$ echo $PATH


--------------------------------------------------------------------------
### Invocation(호출, 발동) Modes
https://mywiki.wooledge.org/bash/invocation/mode
--------------------------------------------------------------------------	
MODE				BASH								POSIX Shell
Login Shell			-bash[options]						-sh
					bash -l[options]					sh -l
					bash --login[options]				sh --login
	
Command String		bash -c 'command'[Options]			sh [options] -c 'command
	
Interactive Shell	bash -i								sh -i	
Shell Script		#!/bin/bash [option]				#!/bin/sh [option]
Command File		bash [options] file [args]			sh [options] file [args]
Command Stream		bash [options] -s [SHELL [ARGS]]	sh [options] -s [SHELL [ARGS]]	
	
--------------------------------------------------------------------------	
# Switch user
	$ su   user_id					<= just switch user but same current directory
	$ su - user_id					<= go to user's HOME directory
--------------------------------------------------------------------------	
# CD command
	.. 	parent  directory
	.	current directory
	~	User's home directory
	-	Previous directory
	
	$ mkdir -p /1/2/3/4/5			<= creating parent directories(sub dirs)
	$ cd ../..						<= from /1/2/3/  =>  /1   		
	$ cd ../2						<= from /1/2/3   =>  /1/2 		
	$ cd -							<= move to previous directory
	$ cd ~-							<= same
	$ echo ~-						<= out put previous directory
	

--------------------------------------------------------------------------
# History	
	$ ls /var/log
	$ cd  !*						<= cd into last command directory
		$ cd /var/log
	
	$ !!   or	UP arrow key 		<= Previous command
	
	$cat /etc/shadow
		cat: /etc/shadow: Permission denied
		
	$ sudo !!
	
	$ crtl+r 			<= Reverse i search  
	
	$ cat vs tac
	$ tac <= concatenate and print files in reverse
		
	
	# Not recrod if there is a sapce at front
	$ export HISTCONTROL=ignorespace
	$ <space> clear						<= not record in history
	$ export HISTCONTROL=ignoredups
	$ export HISTCONTROL=erasedups 		<= delete all duplicated 
	
	*** add all in one line
	$ export HISTCONTROL="ignorespace:erasedups:ingreodups"
	$ echo 'export HISTCONTROL="ignorespace:erasedups:ingreodups"' >> ~/.bashrc
	
	$ export HISTIGNORE="history*:ll*:ls*" 
	$ echo 'export HISTIGNORE="history*:ll*:ls*:"' >> ~/.bashrc
	*** History Time stamp
	$ export HISTTIMEFOREMAT="%F>"
	$ echo 'export HISTTIMEFOREMAT="%F>"' >> ~/.bashrc
	*** History size
	$ export HISTSIZE=10000
	$ echo "export HISTSIZE=10000" >> ~/.bashrc
	
--------------------------------------------------------------------------	
### Touch 
	$ touch apple banana cherry			<= creating multiple files
	$ touch file_{01..1000}
	$ echo {1..10..2}
	  1 3 5 7 9
	$ echo {A..Z}
	$ echo {A..z}						<= Capital 1st
	$ echo {w..d..2}					<= backward + every 2nd letter
	$ touch cherry_{01..100}{w..d}.txt
--------------------------------------------------------------------------
### nl <= number line
	$ nl file.txt
	$ cat file.txt | nl 

	$ grep -i 'tcp' /etc/services | awk '{print $1}' | sort | less

	
	### mkfifo - make FIFOs (named pipes)
	From 1st shell
	$ mkfifo named_pipe 
	$ echo "hi" > named_pipe
	
	From 2nd shell
	$cat named_pipe
	# meta data
	prw-rw-r--. 1 apark apark 0 Jun  1 15:10 named_pipe

--------------------------------------------------------------------------	
### STDIN, STDOUT, STDERR
	$ sort < unsorted.txt > sorted.txt
	$ ls | tee ls.txt
	$ ls | tee -a ls.txt 			<= -a append
	$ find /etc |sort|tee etcSort.txt|wc -l
	$ find /etc 2> etcErr.txt |sort|tee etcSort.txt|wc -l
	$ find /etc &> /dev/null   			<= NO OUT PUT
	
--------------------------------------------------------------------------
### '\' back slash
	long command using  \  for extent command
	# escape character \
	
--------------------------------------------------------------------------
### Putty, mRemote, Cygwins, Shell, Terminal
	
3. SSH connection login
	$ ssh -v root@172.16.248.xx > result.txt					<= -v debugging mode
	$ ssh -v root@172.16.248.xx 2>&1 > result.txt	
	$ ssh id@x.x.x.x -p 2222									<= different port 2222
	$ ssh -l ubuntu ip_address (or hostname)					<= -l login user name

--------------------------------------------------------------------------
### List of host in ~/.ssh/config file  
											** using Alias is OLD way ***
	https://www.cyberciti.biz/faq/create-ssh-config-file-on-linux-unix/
	
	$ vi ~/.ssh/config
	-------------------------------------------------------------------------------------------
	Host netops
		HostName 45.55.5.69
		User apark
		Port 22
		IdentityFile ~/.ssh/id_rsa
	Host puppet
		HostName IP
		User apark
		Port 22
		IdentityFile ~/.ssh/id_rsa
	
	$ ssh apark@netops
--------------------------------------------------------------------------	
### Login to internal lan server 192.168.0.251 via our public UK office ssh based gateway using ##
	## $ ssh uk.gw.lan ##
	Host uk.gw.lan uk.lan
		HostName 192.168.0.251
		User nixcraft
		ProxyCommand  ssh nixcraft@gateway.uk.cyberciti.biz nc %h %p 2> /dev/null
	-------------------------------------------------------------------------------------------
	
	
	 
	ProxyCommand : Specifies the command to use to connect to the server. The command string extends to the end
				   of the line, and is executed with the user’s shell. In the command string, any occurrence 
				   of %h will be substituted by the host name to connect, %p by the port, and %r by the remote 
				   user name. The command can be basically anything, and should read from its standard input 
				   and write to its standard output. This directive is useful in conjunction with nc(1) and 
				   its proxy support. For example, the following directive would connect via an HTTP proxy at 
				   192.1.0.253:  ProxyCommand /usr/bin/nc -X connect -x 192.1.0.253:3128 %h %p

--------------------------------------------------------------------------	
	# Creating SSH config file and single Ciphers option
	$ ssh -c aes256-ctr apark@45.55.5.69
	$ ssh -c 3des-cbc   apark@45.55.5.69
	$ ssh -Q [cipher | cipher-auth | mac | kex | key]
	
	$ vi .ssh/config
	----------------------
	Host *
	Ciphers aes256-ctr
	----------------------
	


--------------------------------------------------------------------------	
	### Set SSH debugging mode on server
	https://en.wikibooks.org/wiki/OpenSSH/Logging_and_Troubleshooting
	$ systemctl stop sshd 					<= your connection is still live
	$ /usr/sbin/sshd -ddd					<= running debugging mode on screen	

	## Client can't connect
		error msg: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
		Authentication refused: bad ownership or modes for directory .ssh
	
	# Resolution: change permission on user .ssh folder to 700
	$ chown 700 -R .ssh
	

	
	# MD5 (Message-Digest algorithm 5)
	https://help.ubuntu.com/community/HowToMD5SUM
	$ md5sum file_name
	When one has downloaded an ISO file for installing or trying Ubuntu, it is recommended to test 
	that the file is correct and safe to use. The MD5 calculation gives a checksum (called a hash value), 
	which must equal the MD5 value of a correct ISO.
	
--------------------------------------------------------------------------	
###  basename 		<= 	strip directory and suffix from filenames
	basename prints filename NAME with any leading directory components removed. 
	It can optionally also remove any trailing suffix.
	
	EXAMPLES
       basename /usr/bin/sort
              => "sort"

       basename include/stdio.h .h
              => "stdio"

       basename -s .h include/stdio.h
              => "stdio"

       basename -a any/str1 any/str2
              => "str1" followed by "str2"
			  
--------------------------------------------------------------------------
### System OS start up  
--------------------------------------------------------------------------
	Check list
	$ ls -l  /etc/init.d/
	
	# Modify
	### CentOS 6
	$ chkconfig --list
	$ chkconfig ssh on
	$ chkconfig ssh off
	
	
	# Ubuntu Upstar(Debian)
	$ apt-get install openssh-server openssh-client
	$ update-rc.d ssh defaults			<= put it into start up
	$ update-rc.d ssh enable 			# sets the default runlevels to on 
	$ update-rc.d ssh disable 			# sets all to off

### SSH 
	-v  <=verbose 
	-V  <=Version 
	
	$ ssh -vvvv nozatech@192.168.221.129						<= Max verbose debugging mode
    
	OpenSSH_6.7p1, OpenSSL 1.0.1k 8 Jan 2015
    debug1: Reading configuration data /etc/ssh_config
    debug1: Connecting to 192.168.221.129 [192.168.221.129] port 22.
    debug1: Connection established.
    debug1: identity file /home/apark/.ssh/id_rsa type 1

	### SSL is TLS ###
	SSL(Secure Socket Layer) is the old name. 
	TLS(Transport Layer Security) is for now a days.
	https://curl.haxx.se/docs/sslcerts.html

--------------------------------------------------------------------------	
### User management	
--------------------------------------------------------------------------
# useradd vs adduser
	# https://askubuntu.com/questions/345974/what-is-the-difference-between-adduser-and-useradd
	
	addduser / deluser 			<= Perl script that creates /home/user & ssh in shell
								   add user with full profile and info (pass, quota, permission, etc.)
	
	useradd / userdel /usermod 	<= low level utilities at add user name Only, No Shell, 
		# https://www.tecmint.com/add-users-in-linux/
	
	
	# Create these two users, and use the --no-create-home and --shell /bin/false options 
		so that these users can't log into the server.
	$ useradd --no-create-home --shell /bin/false prometheus
	$ useradd --no-create-home --shell /bin/false node_exporter
	
# Change user name
	https://askubuntu.com/questions/34074/how-do-i-change-my-username
	
	$ sudo usermod -l newUsername oldUsername
	
	$ sudo usermod -d /home/newHomeDir -m newUsername
	
	$ ln -s    /home/newname   /home/oldname   
	

	

### FROM CLIENT ###
### Add user without password but ssh key only
https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2

$ cat ~/.ssh/id_rsa.pub | ssh id@ip   "mkdir -p ~/.ssh && chmod 700 ~/.ssh && cat >>  ~/.ssh/authorized_keys"
$ cat ~/.ssh/id_rsa.pub | ssh apark@162.243.144.182  \
		"mkdir -p ~/.ssh && \
		chmod 700 ~/.ssh && \ 
		cat >>  ~/.ssh/authorized_keys"

		
		

## FROM SERVER ###
https://unix.stackexchange.com/questions/210228/add-a-user-wthout-password-but-with-ssh-and-public-key
# login to the remote server first
$ sudo su -
$ useradd -m -d /home/ipetcu -s /bin/bash  apark2			<= new user name 'apark2'
					-m --create-home		<= Create the user's home directory if it does not exist. 
					-d --home-dir			<= The new user will be created using HOME_DIR as the value 
												for the user's login directory.
					-s --shell				<= default SHELL 'BASH'

### Login as apark, switch user to ROOT.
					
useradd -m -d /home/ipetcu -s /bin/bash ipetcu	&& cd /home/ipetcu
mkdir .ssh && chmod 700 .ssh  && chown ipetcu: .ssh 
touch /home/ipetcu/.ssh/authorized_keys && chmod 600 .ssh/authorized_keys && chown ipetcu:  .ssh/authorized_keys
vi .ssh/authorized_keys

#cat id_rsa.pub >> .ssh/authorized_keys
sudo visudo
ipetcu  ALL=(ALL)       NOPASSWD:ALL
BNER IP: 89.37.124.86
 
### Edit sshd config file	### 
$ vi /etc/ssh/sshd_config
----------------------------
RSAAuthentication 			yes
PubkeyAuthentication		yes
AuthorizedKeysFile 			.ssh/authorized_keys
PasswordAuthentication 		no	<= NO!

----------------------------	
$ /etc/init.d/sshd restart							<= Restarts the sshd   service
$ systemctl restart sshd 		
	
--------------------------------------------------------------------------	
### /etc/passwd	

	userID:pw:uid:gid:gecos:home             : shell
	icinga:x:992:987:icinga:/var/spool/icinga2:/sbin/nologin
	
	# Get GECOS 
	:FullName,RomAddress,WorkPhone,HomePhone,Others:
	https://superuser.com/questions/1031615/the-other-finger-gecos-fields-at-etc-passwd
	$ awk -F ":" '{print $5}' /etc/passwd	
	
	



	
	
--------------------------------------------------------------------------	
### PSSH tool includes parallel versions of OpenSSH and related tools such as:

	pssh – is a program for running ssh in parallel on a multiple remote hosts.
	
	pscp – is a program for copying files in parallel to a number of hosts.
		   Copy/Transfer Files Two or More Remote Linux Servers
	
	prsync – is a program for efficiently copying files to multiple hosts in parallel.
	
	pnuke – kills processes on multiple remote hosts in parallel.
	
	pslurp – copies files from multiple remote hosts to a central host in parallel.

	
	$ yum install python-pip
	$ pip install pssh


	
		
----------------------------------------------------------------------------------------------------
### Run command on remote
http://unix.stackexchange.com/questions/19008/automatically-run-commands-over-ssh-on-many-servers
----------------------------------------------------------------------------------------------------
### Run command on remote
	$ ssh apark@192.241.190.57 	   'cat /etc/motd	'			<= w/o apark user id
	$ ssh puppet sudo              iptables -nL					<= using .ssh/config  file
	
	$ ssh apark@138.68.10.194 'sudo iptables -nL'				<= Doesn't work
		sudo: sorry, you must have a tty to run sudo	
	$ ssh -t apark@192.241.190.57  'sudo iptables -nL'			<= -t  Force pseudo-tty allocation
	


	
### run script remotely

	$ ssh apark@remote_server  'bash -s' < from_local_script.sh		<= run a script remotely from local using sudoer
	$ ssh apark@45.55.5.69     'bash -s' < do_agent.sh              <= installing as sudo	

	
### Create a server list text file and a update script first
	
	$ for r in $(cat nrErrorServers.txt); do ssh -v $r 'sudo su && bash -s' < updateHosts.sh ; done
	
	$ for i in $(cat doUpdateList.txt); do ssh -t apark@$i 'sudo curl -sSL https://agent.digitalocean.com/install.sh | sh'; done
	$ for i in `cat doUpdateList.txt` ; do ssh -t apark@$i 'sudo curl -sSL https://agent.digitalocean.com/install.sh | sh'; done
	
	### Ignoring 'adding remote host (yes/no)'
	$ for i in $(cat doUpdateList.txt); do ssh -t -o "StrictHostkeyChecking no" apark@$i 'sudo curl -sSL https://agent.digitalocean.com/install.sh | sh'; done

	
### PSSH ###
#	$ pssh -O StrictHostKeyChecking=no -h HostList.txt -l apark -A -i   -x '-t -t'    "sudo /etc/init.d/newrelic-sysmond restart"
### 
### PRSYNC ###	
# 	$ prsync -h /cdn/usEdgeServers.txt -l apark -v -x '-avz --delete' /cdn/data/ /cdn/data/
###	
	
	http://unix.stackexchange.com/questions/87405/how-can-i-execute-local-script-on-remote-machine-and-include-arguments
	
	$ for host in $(cat host_list.txt); do ssh "$host" "$command" > "output.$host"; done
		Authenticating with name/password is really no good idea. 
		You should set up a private key for this:
	
	$ ssh-keygen && for host in $(cat hosts.txt); do ssh-copy-id $host; done
	
	
	
----------------------------------------------------------------------------------------------------	
4.  RSA KEY( 1024bit, 2k, 4k bit)
	### You need to login to the server !!!first!!!
	$ ssh-keygen -t rsa							<= Generate  id_rsa (Private Key ) & id_rsa.pub (Public Key)	
	
    $ ssh-copy-id user_id@remote_server			<= Transferring Public Key
	
	$ cat ~/.ssh/id_rsa.pub | ssh user@ip "mkdir -p ~/.ssh && cat >>  ~/.ssh/authorized_keys"

	$ ssh remote_server "cat .ssh/*.pub"

	
	
	
	
----------------------------------------------------------------------------------------------------
5. WGET to download a file using user_id & PW
	$ wget --user=user_id --password='my_passwd' http://download.com/foo.pdf
	$ wget http://www.gnu.org/software/gettext/manual/gettext.html  or file_name	
	$ time wget http://www.gnu.org/software/gettext/manual/gettext.html  			<= measuring download time

	# download web folder files
	$ wget -r --no-parent http://www.download.com/folder_name/
	
	
----------------------------------------------------------------------------------------------------	
### Finding my ip (Gateway IP)
	$ curl ifconfig.io	
	$ curl icanhazip.com

	https://opensource.com/article/18/5/how-find-ip-address-linux
	$ curl ifconfig.me
	$ curl -4/-6 icanhazip.com
	$ curl ipinfo.io/ip
	$ curl api.ipify.org
	$ curl checkip.dyndns.org
	$ curl ident.me
	$ curl bot.whatismyipaddress.com
	$ curl ipecho.net/plain
	
	$ dig +short myip.opendns.com @resolver1.opendns.com
	$ host myip.opendns.com resolver1.opendns.com

	### The following commands will get you the private IP address of your interfaces:
	$ ifconfig | grep -o '[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}'|head -1  <= RegEx 
	$ ifconfig -a
	$ ip addr 
	$ ip a
	$ hostname -I | awk '{print $1}'
	$ ip route get 1 | awk '{print $NF;exit}'
	$ ip route get 8.8.4.4 | head -1 | awk '{print $7}'
	$ ip route get 8.8.4.4 | head -1 | cut -d' ' -f7
	
	# nmcli 	<= command-line tool for controlling NetworkManager
	$ nmcli -p device show
	
----------------------------------------------------------------------------------------------------	
### jq - Command-line JSON processor
	curl -s http://127.0.0.1:4567/clients | jq .   	<= -s slient 
	
----------------------------------------------------------------------------------------------------	
### cURL   ### 
	Testing on REST API
	A. Content of the URL and display it in the STDOUT (i.e on your terminal)
		$ curl http://www.centos.org
	
	B. To store the output in a file, you an redirect it.
		$ curl http://www.centos.org > /tmp/centos-org.html  <= save to file
	
	C. Save the cURL Output to a file
		-o (lowercase o) saved in the file name e
		-O (uppercase O) the file name in the URL will be taken and it will be used as the file name to store the result
		-l --list-only
		-L --location
	  $ curl -o saving_name.html http://www.gnu.org/text_info.html							
	  $ curl -O http://www.gnu.org/software/gettext/manual/gettext.html or file_name		<= save the file to local 
	
	D.  Follow HTTP Location Headers with -L option  
		$ curl -L http://google.com
	
	E.	
		$ curl --limit-rate 1000B -O http:
	
	F. Proxy
		$ curl -p 
	
	G.	$ curl -u username:password URL
	
	H.  Verbose
		$ curl -v http://google.com
	
	I. $ curl dict://dict.org/d:bash
	

	# to get the HEADER info	
	$curl -i http://address:3000/json-test 
  	
	HTTP/1.1 403 Forbidden
	Date: Wed, 19 Oct 2016 02:07:18 GMT
	Server: Apache/2.4.6 (CentOS)
	...
	Content-Length: 4897
	Content-Type: text/html; charset=UTF-8
	
	
	### Check multiple site
	$ curl -sSF http://site.{one,two,three}.com

----------------------------------------------------------------------------------------------------
*C*R*U*D*

POST method
	$ curl -d "first=Alber&last=park" http://address:port/method_name

PUT method
	$ curl -X PUT -d "first=Alber&last=park" http://address:port/method_name

Delete
	$ curl -X DELETE  http://address:port/method_name

	$ curl -u 

	$ curl -X GET -H 



	
----------------------------------------------------------------------------------------------------
### curl can download or upload through any protocol, http, ftp, sftp, scp, ldap, telnet
https://curl.haxx.se/docs/httpscripting.html

curl 					www.yahoo.com 		-o 		output.html					# -o output to
curl -sL 	https://deb.nodesource.com/setup_6.x -o nodesource_setup.sh			# -s(silence)L(location) -o(output)
curl -O 				www.yahoo.com/index.html
curl -# -u id:pw 		ftp://ftphost/fuzicast/sample.txt -o sample.txt 		# the -# adds progress bar
curl -r 0-99 -u id:pw 	ftp://ftphost/fuzicast/sample.txt						# get first 100 bytes
curl -r -500 -u id:pw   ftp://ftphost/fuzicast/sample.txt 						# get last 500 bytes
echo "Hello World" | curl -T - -u id:pw \
						ftp://ftphost/fuzicast/sample2.txt
curl -T UNIX-1.14 -u id:pw ftp://ftphost/fuzicast/unix
curl -T localfile1 servername/remotefile1 -T localfile2 servername/remotefile2
curl -T UNIX-1.14 -u id:pw -a ftp://ftphost/fuzicast/unix 						# append to FTP file
curl --ftp-create-dirs -T UNIX-1.14 -u id:pw ftp://ftphost/fuzicast/unix/test.txt
curl --limit-rate 10240 -u id:pw ftp://ftphost/fuzicast/sample.txt 				# limit number of bytes per second .curlrc # curl configuration file
curl -u id:pw -z sample.txt ftp://ftphost/fuzicast/sample.txt 					# download remotefile only if it's newer than localfile
curl -z "Jan 12 2012" -u id:pw  ftp://ftphost/fuzicast/sample.txt 				# download remote file only if it's newer than Jan 12 2012
curl -B -u id:pw 				ftp://ftphost/fuzicast/sample.txt 								# enforces ASCII transfer during FTP download 
curl -u id:pw 					ftp://ftphost/fuzicast/sample.txt --create-dirs -o sampledir/sample.txt		# create directory if not exist
curl --key id_rsa # use SSH key
curl -u id:pw ftp://ftphost -Q 'RNFR /fuzicast/sample.txt' -Q 'RNTO /fuzicast/sampleyue.txt'	 # rename a remote file in FTP protocol
curl -u id:pw ftp://ftphost -Q 'rename /fuzicast/sample.txt /fuzicast/sampleyue.txt' 			# rename in SFTP is different from FTP
curl -R -u id:pw ftp://ftphost/fuzicast/sample.txt -o output.txt # reserve original file timestamp
curl -l -u id:pw ftp://ftphost/fuzicast/ 										# list remote filenames
curl -m 1800 -Y 3000 -y 60 servername/filename 									# speed must be greater than 3000 bytes per second for a minute and 
																				download process must be completed within 1800 seconds, otherwise the 
								  												download will abort
### Verify the API Token
$ curl -H "X-HockeyAppToken: 756408....da91"   https://rink.hockeyapp.net/api/2/apps



----------------------------------------------------------------------------------------------------	
### text browser ###
	$ sudo apt-get install -y links
	$ links http://www.google.com



	
----------------------------------------------------------------------------------------------------
### base64 - base64 encode/decode data and print to standard output
	http://stackoverflow.com/questions/201479/what-is-base-64-encoding-used-for
	
	# Encode
	$ printf id:pw | base64
		$ base64 data.txt > data.b64
	# Decode using -d option
	$ base64 -d 
		$ base64 -d data.b64 						<= on to SCREEN(STDOUT)
		$ base64 -d data.b64 > data.txt				<= on to Files
	https://www.base64decode.org/	

----------------------------------------------------------------------------------------------------
### How to check SHELL type
	$ echo $SHELL
		/bin/bash
	$ echo $0
		-bash
	$ env | grep SHELL
		SHELL=/bin/bash
	$ set | grep SHELL
		SHELL=/bin/bash
	$ ps | grep $$
		3179 pts/0    00:00:00 bash
	$ ps -ef | grep $$

	# checking which shell
	$ ps 		<= All shells PID
	$ ps -p $$  <= Current shell PID
 

	### install tcsh and login
	$ yum install tcsh -y
	$ tcsh
	$ echo $0
		tcsh
		


### Copy, Move, Rename
### cp






### mv 
	https://superuser.com/questions/901183/who-deals-with-the-star-in-echo
	e.g. test.txt test1(dir) test2(dir) test3(dir)
	$ mv *
	$ ls
		test3(dir)   <= cuz 'mv' test.txt test1(dir) test2(dir) => test3(dir)
	
	

### rename	
	

### yes 		
	$ yes Testing 				<= output a string repeatedly until killed




	
### rm 
	$ rm -rf data.{1..9}				<= delete data.1 ~ data.9
	$ rm "the file name.mp3"			<= if there are spaces in file name
	

	
### grep

	# Finding a string word using grep
	$ grep -i word *		<= current dir
	$ grep -i word */*		<= sub dirs
	$ grep -i word */*/*    <= sub sub dirs

	
7. File Globs
	1) * (asterist)
		$ file.*			<= matches any number of any characters
		$ file*.txt
		$ *.jpg
	
	2) ? Matches one(1) of any character
		$ ls -l file?.txt
		$ ls -l ?.jpg
		
	3) [ ] Character Sets
		$ ls -l file[0-9].txt			<= file1.txt
		$ ls -l file[a-z].txt
		$ ls -l file[abc123].jpg		<= any One from [abc123];  a.jpg, b.jpg, 3.jpg
	
	4) [ - ] Matches a '-' (hyphen)
		$ file[-9-0].txt		
		  file-.txt 
		  file1.txt
	
	5) [! ] Negate(NOT) a match; Do Not Match 0-9.txt
		$ file[!0-9].txt		
		  filea.txt
		  fileb.txt
	
	6) [::] Matches on character of a certain type
		Claass		Match
		[:digit:]	Numbers
		[:uppper:]	Upper case characters
		[:lower:]	lower case characters
		[:alpha:]	Upper and lower case
		[:alnum:]	Upper and lower case plus numbers
		[:space:]	Space, tabs, and newlines
		[:graph:]	Printable characters, not including spaces
		[:print:]	Printable characters, including spaces
		[:punct:]	Punctuation
		[:cntrl:]	Non-printable control characters
		[:xdigit:]	Hexadecimal characters
		
	$ ls file[0-9].txt
	$ ls f*[[:digit:]].txt
	
	$ ls file[[:digit:][:space:].txt
	$ ls file[![:digit:][:space:].txt			<= ! Negate(but this)
	
	$ ls file[![:digit:]].txt
	$ ls file[![:digit:][:space:]].txt 
		
	$ ls {*.jpg,*.gif,*.png}		<= works on 2 or more
		a.jpg, b.jpg, c.png, d.gif
	$ ls -l {b*,c*}
	$ rm *[^1].txt					<= delete all but *1.txt
	$ rm *[!1].txt					<= range, delete all but *1.txt
	$ cat myfile | grep '^s.*n$'	<= end of the line




	
### Extended Globs
	$ shopt
	$ shopt -s extglob				<= put into .bashrc

	1) +(match)
		$ file+(abc).txt			<= fileabc.txt, fileabcabc.txt

	2) +(match|match)
		$ ls +(photo|Photo)*+(.jpg|.gif)		
			photo.jpg, Photo.gif
		
	3) *(match)
		$ ls test*(1).*				<= test1.jpg, testfile1.png, testUp1.txt
		$ ls test*1.*				<= test1.jpg, testFile1.png, testUp1.txt
		
	# Invert match	
	4) !(match)						<= Don't match 
		ls -l !(*.jpg|*.png)
		
	5) !(+(match)*+(match))			<= group match
		$ ls !(+(photo|file)*+(.jpg|.gif)) 	<= All files that do not have photo or file, and 
												don't end with jpg or gif.
			
### Shell Meta-Characters			
	< >, $, *, {}, [], +, '', ^, ., ?, |, (), \, ""
	$ USER=apark
	$ echo  $USER
	$ echo  My name is $USER
	$ echo "My name is $USER"     	<= apark
	$ echo 'My name is $USER'		<= $USER
	
	
	### /etc/services http://www.penguintutor.com/linux/network-services-ports
	maps port numbers to named services(SSH 22/tcp).
	a simple database that associates a human friendly name to a machine friendly service port.

	$ grep '*./tcp' /etc/services | grep ssh
		ssh  22/tcp      # The Secure Shell (SSH) Protocol
	
	# Socket(IPP) is the combination of IP address, port and protocol.
	


	


	
		
4. free  
	 
	### Most memory used by process
	
	$ ps aux --sort=-%mem | awk 'NR<=10{print $0}'	

		  <= Check memory
	
	### Displays the total amount of free and used physical and swap memory in the system, 
		as well as the buffers used by the kernel. The shared memory column should be ignored; it is obsolete.
		
	$ free -m | xargs | awk '{ print "Free/Total memory " $10 "/" $8 "MB" }'

	$ free -m | grep Mem | awk '{print $4 "/" $2 "MB free"}'

	$ free -tom
         -m MB 
         -t switch displays a line containing the totals.
         -o switch disables the display of a "buffer adjusted" line.  If the -o
            option is not specified, free subtracts buffer memory from the  used
            memory and adds it to the free memory reported.
	$ egrep --color 'Mem|Cache|Swap' /proc/meminfo

5. vmstat   
	# virtual memory statistics 
	# reports information about processes, memory, paging, block IO, traps, and cpu activity.	
	
	# Check memroy size first
	$ free -m		<=MB
					total        used        free      shared  buff/cache   available
		Mem:         7756         439        3244         148        4072        6744
		Swap:        7935           0        7935
	
	# top <- real time system monitoring
	$ top
	  # 1   <= show list all CPUs
		top - 11:39:06 up 91 days,  1:23,  1 user,  load average: 0.60, 0.29, 0.25
		Tasks: 197 total,   2 running, 195 sleeping,   0 stopped,   0 zombie
		%Cpu0  :  0.3 us,  0.7 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
		%Cpu1  :  0.0 us,  1.0 sy,  0.3 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
					user	system	nice	idle	wait time(i/o) 	
		KiB Mem :  7942812 total,  3318900 free,   453520 used,  4170392 buff/cache
		KiB Swap:  8126460 total,  8126460 free,        0 used.  6903032 avail Mem
	  
	  # shift + .  <= ( > ) top memory usage
	  # shift + ,  <= ( < ) top CPU usage	

	# Stress Testing CPU 1
	$ stress --help
		Example: stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 10s

	$ stress -c 1
	$ top
		1
	$ stress -m 3   <= 3GB  
	  
	$ vmstat 1 20 		<= one (1) second twenty (20) times:
	$ vmstat 30 		<= ongoing report for intervals of 30 seconds
	$ vmstat -S k 1 10  <= S-switch unit (k kilobyte, K, m, M)
	$ vmstat 10(sec) 6(times)  	
	$ vmstat -a (active)
		procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
		r  b   swpd   free  inact active   si   so    bi    bo   in   cs us sy id wa st
		1  0      0 600324 140380 175672    0    0     5     2    8   13  0  0 100  0  0
		
		procs	<= umber of processing jobs waiting to run
		memory  <= same as free -m
		swap	<= memory is sent to or retrieved from the swap system
		io		<= input and output activity per second  in terms of blocks read and blocks written
				   bi -block in, bo-block out
		system	<= number of system operations per second
		cpu		<= always add to 100 and reflect “percenDATEe of available time”.	
			
	$ vmstat -s    <=-s switch displays summary of various event counters and memory statistics.
      1009992 K total 	 memory
       410888 K used 	 memory
       176468 K active 	 memory
       140376 K inactive memory

	$ vmstat -d  		<=  -d option display all disks statistics.
	    disk- ------------reads------------ ------------writes----------- -----IO------
				total merged sectors      ms  total merged sectors      ms    cur    sec
		ram0       0      0       0       0      0      0       0       0      0      0
		ram1       0      0       0       0      0      0       0       0      0      0
		ram2       0      0       0       0      0      0       0       0      0      0

		



		
6. Creating swap space in CentOS7
	http://www.cyberciti.biz/faq/linux-add-a-swap-file-howto/
	
	$ swapon -s   <= -s, summary
	$ free -m
	
#### Create swap space	
https://www.2daygeek.com/add-extend-increase-swap-space-memory-file-partition-linux/#
#--------------------------------------------------------------------------------
#4GB with 1MB bitesize
dd if=/dev/zero of=/swapfile count=4096 bs=1MiB   

#2GB with 512k bitesize 
dd if=/dev/zero of=/swapfile bs=2048 count=512k		<- created 1GB, Not 2GB ???

$ sudo chmod 600 /swapfile
$ mkswap /swapfile
$ swapon /swapfile

# To enable it at boot up, add to /etc/fstab
$ echo "/swapfile  swap  swap  defaults  0 0"  >>  /etc/fstab   

#--------------------------------------------------------------------------------	
#### Increase the exisiting Swap space	




### How do I verify swap is activated or not?
	$ free -h
	$ swapon
	$ swapon --show
	$ less /proc/meminfo
	$ top           									<= check in KiB Swap
	$ cat /proc/swaps
	$ cat /proc/meminfo | grep -i swap
	
	
	### bug, not working	###
#	$ sudo fallocate -l 2G /swapfile   <= 2GB space for swap
#	$ sudo chmod 600 /swapfile
#	$ ls -lh /swapfile
#	$ sudo mkswap /swapfile
#	$ sudo swapon /swapfile
###	

### fallocate - preallocate space to a file

	$ fallocate -l 1G   1gb_file		<= 1078 size ??
	$ fallocate -l 1GB  1gb_file		<= 1000 size ??
	
	
### stat			<=display file or file system status

	$ stat file.name
	
	File: ‘file.name’
	Size: 80              Blocks: 8          IO Block: 4096   regular file
	Device: fd02h/64770d    Inode: 1074617621  Links: 1
	Access: (0664/-rw-rw-r--)  Uid: ( 1000/   apark)   Gid: ( 1000/   apark)
	Context: unconfined_u:object_r:user_home_t:s0
	Access: 2018-01-25 14:37:35.037545396 -0800
	Modify: 2018-01-25 14:37:32.473485310 -0800
	Change: 2018-01-25 14:37:32.483485544 -0800
	Birth: -
	
	$stat -f /
	File: "/"
    ID: fd0000000000 Namelen: 255     Type: xfs
	Block size: 4096       Fundamental block size: 4096
	Blocks: Total: 13100800   Free: 9197614    Available: 9197614
	Inodes: Total: 52428800   Free: 52251919

### Type command
	The type command can be used to get a description of the command type:

	$ type rm
		rm is hashed (/bin/rm)
	$ type cd
		cd is a shell builtin
	
	
7. sysstat	Linux Performance Monitoring package

	Collective CPU usage
	Individual CPU statistics
	Memory used and available
	Swap space used and available
	Overall I/O activities of the system
	Individual device I/O activities
	Context switch statistics
	Run queue and load average data
	Network statistics
	Report sar data from a specific time

	
	
$ iperf
	Measures TCP bandwidth; 
	Reports on maximum segment size and maximum transmission unit;
	Support for TCP Window size;
	Multithreaded for multiple simultaneous connections;
	Creates specific UDP bandwidth streams;
	Measures packet loss;
	Measures delay jitter;
	Runs as a service or daemon; and
	Runs under Windows, Linux OSX or Solaris.
	
	$ iperf -s
	$ iperf -c remote_server     				<= -c host


	
https://www.server-world.info/en/note?os=CentOS_7&p=sysstat
$ yum install -y sysstat
$ systemctl start sysstat
$ systemctl enable sysstat

$ sudo cat /etc/cron.d/sysstat
	# Run system activity accounting tool every 10 minutes
	*/10 * * * * root /usr/lib64/sa/sa1 1 1
	# 0 * * * * root /usr/lib64/sa/sa1 600 6 &
	# Generate a daily summary of process accounting at 23:53
	53 23 * * * root /usr/lib64/sa/sa2 -A
	
#	
#	$ vi /etc/default/sysstat   						<= change to true for data collection
# 	$ service sysstat restart
#	$ sar -A > $(date +`hostname`-%d-%m-%y-%H%M.log)    <=save the statistics
	
http://www.tecmint.com/sysstat-commands-to-monitor-linux/
	
	### iostat  <= displays CPU and I/O statistics of all partitions as shown below.
	$ iostat
		Linux 3.16.0-23-generic (puppet.nozatech.com)   03/24/2016      _x86_64_        (8 CPU)
		avg-cpu:  %user   %nice %system %iowait  %steal   %idle
				0.01    0.01    0.08    0.05    0.00   99.86
		Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
		sda               2.01        36.44        11.34     236588      73606
		dm-0              2.98        33.48        11.34     217373      73592
		dm-1              0.04         0.17         0.00       1080          0
	
	$ iostat -N 			<= With -N (Upper-case) parameter displays only LVM statistics as shown.
	$ iostat -p sda 		<= By default it displays statistics of all partitions, with -p and 
								device name arguments displays only disks I/O statistics for specific device only as shown.
	$ iostat -d				<= -d arguments displays only disks I/O statistics of all partitions as shown.
		
	$ mpstat -P ALL
	
	$ pidstat
	
	$ cifsiostat 
		
		
		
		
		
8.	slabinfo - kernel slab allocator statistics
	cat /proc/slabinfo
	Slab allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects.
	
	
	
	
9. Utility to check real time processing
    A. top 
        $ top -bn 1 			<= Top Batch Number 1			    
								<= capture into single page file  
								<= -b <= batch mode, n <= number, 1 <= count 
		
		$ top -bn 1 | awk "/$1/ {tot =+ \$6; n++} END {print tot\" \"n}"      <=??
		
		### A Top % memory usage app value
		$ top -o %MEM -bn 1 |  awk '8 <=NR && NR <=8' | awk '{print $10}'
		
		
		### Top 5 CPU processes
		
		### Using head & tail
		$ top -bn 1 | head -n 12 | tail -n 6  	<= head displays head of 12 lines 
												   tail from bottom 6 lines ( Displays 7,8,9,10,11,12 lines)
		  
		  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.74 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.13 kworker/u128:0

		### using sed
		$ $top -bn 1 | sed -n '6,12p'

			PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.74 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.14 kworker/u128:0
		
		### Using awk for specific line 6 to 12
		$ top -bn 1 | awk '6 <=NR && NR <=12'

			PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
			1 root      20   0  193632   6760   3944 S   0.0  0.7   0:00.76 systemd
			2 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kthreadd
			3 root      20   0       0      0      0 S   0.0  0.0   0:00.01 ksoftirqd/0
			5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
			6 root      20   0       0      0      0 S   0.0  0.0   0:00.15 kworker/u128:0

						
		### Top commands short cuts:
	***	1 – To display or hide all other CPU’s
		A - Split into multiple CPU screen
		b - bache mode sending output from top to other programs or to a file
		c – To display or hide command full(absolute) path
		d - display rate from 3sec to any sec
		n - number of iteration 
		l – To display or to hide load average line
		t – To display or to hide task/cpu line
		m – to display or to hide RAM and SWAP details
		s – To change the time interval for updating top results(value is in sec’s)
		R – To sort by PID number
		u — Press u then user name to get only that user process details
		P – To sort by CPU utilization
		M – To sort by RAM utilization

		r – To renice a process, press r then the PID no then the renice value to renice a process.
	***	k – To kill a process, press k then PID number then enter to kill a process
		w – To save the modified configuration permanently.
		
		q – To quit the top command.
	***	h – for getting help on top	

		
	shift + m 		<= list process according to memory usage
	shift + p		<= list the process according to cpu usage
	shift + w		<= for saving the top output in a file(/root/.toprc.)
	shfit + o		<= for sorting the process as per requirement
		
	# User process list	
	$ top -u apark
	
	# quit after 1 iteration
	$ top -n 1 	
		
	# Batch mode( capture as text file)
	$ top -bn 1	 > top_process.txt
	
	$ top -d300 -b > top_log.txt
	
	
		
	B. atop
		Atop is an ASCII full-screen performance monitor which can log and report 
		the activity of all server process up to 28days log.
	
		$ atop -a <= sort in order of most active resource.
		$ atop -c <= revert to sorting by cpu consumption (default).
		$ atop -d <= sort in order of disk activity.
		$ atop -m <= sort in order of memory usage
		$ atop -n <= sort in order of network activity
  
    C. htop
		Up/Down arrow keys to select a process, and then you can kill it with the F9 key

		
	## Memory Usage  
	https://unix.stackexchange.com/questions/128953/how-to-display-top-results-sorted-by-memory-usage-in-real-time
	$ top -o %MEM -bn 1 > topMemoryUsage.txt
	$ top -o %MEM -bn 1 | awk '8 <=NR && NR <=8
		
		
10. Disk usage / folder size / disk space
	$ du -hs * | sort -rh | head -5
	$ find / -type f -exec du -Sh {} + | sort -rh | head -n 5
	
	$ du -ah              					<= all total size files & directories, -a all, -h human 
	$ du -ah *   							<= No total size
	$ du -sh /home/noza*					<= total holder size including subfolders
	$ du -sh /home/noza*  | sort -nr		<= Folder list usage, not files
	$ du -sh /home/noza*  | sort -nr		<= Folder list usage, not files
	
######################################################################################
				total       used       free     shared    buffers     cached
	Mem:           482        304        178          0         51        137
	-/+ buffers/cache:        115      **367** <= Actual Free Memory


	$ free -m | awk 'NR==3 {print $4 " MB"}'
	#
	# When thinking about 'how much memory is really being used' :
	# 'used' - ('buffers' + 'cached')
	Actual Memory usage 116MB = 304-(51+137)
	#
	# When thinking about 'how much memory is really free' :
	# 'free' + ('buffers' + 'cached')
	Actual Free memory   336MB = 178+(51+137)
######################################################################################


	$ iotop  				<=check I/O usage


### exec
	https://askubuntu.com/questions/525767/what-does-an-exec-command-do
	exec serves to also replace current shell process with a command, 
	so that parent goes a way and child owns pid

	### Exectute and exit
	$ ps -p $$
	PID TTY          TIME CMD
	2441 pts/1    00:00:00 bash			<= current shell
	
	$ exec ps -p $$
	PID TTY          TIME CMD
	2441 pts/1    00:00:00 bash			
	Connection to 10.100.5.155 closed.	<= Exit right after execute from the current shell
	
	### Exec and Fork
	$ ps
		PID TTY          TIME CMD
		8386 pts/1    00:00:00 bash				<= only current shell
	$ bash										<= launch a new bash shell	
	$ ps -p $$
		PID TTY          TIME CMD
		8414 pts/1    00:00:00 bash				<= newly lauched bash shell
	
	$ exec > newShell.log						<= STDOUT redirected to newShell.log file, not on screen
	$ date										<= NO STDOUT
	$ exit
	$ ps -p $$
		2441 pts/1    00:00:00 bash				<= current shell
	$ cat newShell.log
	
### fork
	Normal programs are system commands that exist in compiled form on your system. When such a 
	program is executed, a new process is created. This child process has the same environment 
	as its parent, only the process ID number is different. This procedure is called forking.
	
	It provides a way for an existing process to start a new one. But there may be situation that 
	child process is not the part of same program as parent process is. In this case exec is used. 
	exec will replace the contents of the currently running process with the information from a 
	program binary.
	
	After the forking process, the address space of the child process is overwritten with the new 
	process data. This is done through an exec call to the system.
	
	
	
### Date format 
	$ date
	  Tue Jan 15 11:29:01 PST 2019

	$ date +%F
	  2016-08-11
	
	$ timestamp=`date +%F`  			<= set variable name 'timestamp'
	
	$ date +%Y-%m-%d-%H-%M-%S
	  2019-01-15-11-30-02

	$ DATE=`date +%Y-%m-%d-%H-%M-%S`  	 <= %Y is 2016, %y is 16
	$ echo $DATE
	  2016-03-24-22-15-39

	$ DATE=`date +%H:%M:%S`				 <= `command.....`      <- Legacy way
	$ DATE=$(date +%H:%M:%S)			 <= $(command....)
	$ DATE="$(date +%H:%M:%S)"			 <= $(command....)
	
	$ echo $DATE
		22:18:35
	$ DATE='$(date +%H:%M:%S)'			<= NO Single Quote XXX
		$(date +%H:%M:%S)				<= Literal with Single Quotes

	 
	# time stamp for script
	$ date +%s                 <= %s     seconds since 1970-01-01 00:00:00 UTC
	
	# milliseconds since 1-1-1970(Use %3N to truncate the nanoseconds to the 3 most significant digits)
	$ date +%s%3N
	1397392146866 
	 
	$ date +%Y-%m-%d-hour-%H:%M:%S
	$ date +Name:`hostname`/Date:%Y-%m-%d/hour:%H:%M:%S 
	  Name:PC_Name/Date:2016-10-28/hour:11:37:43

	$ date +"%m-%d-%Y"   		<= 12-17-2014
	$ date +"%H:%M:%S"   		<= 10:20:18
	$ date +"Today's date is %m-%d-%Y and time is %H:%M:%S."
	$ date +%T
	$ date +%D
	$ date +%F   				<= 2018-01-09  'F'ull date; same as %Y-%m-%d
	
	
	
###	ls
	http://unix.stackexchange.com/questions/21638/show-only-hidden-files-dot-files-in-ls-alias
# list directory
	$ ls -dl */					<= -d directory
	$ ls -Xl    <= sort alphabetically by entry extension
	$ ls -1     <= list one file per line <= number 1, showing names only 

# list hidden files in current directory  
	$ ls -d  .*	  				<= list hidden .files only
	$ ls -ld .*	  				<= list hidden .files only
	$ ls -l  .*?				<= list hidden .files only
	$ ls -Ad .*
	$ ls -a | grep "^\."    	<= show hidden files start with . file

	
	$ hidden() { ls -a "$@" | grep '^\.'; }		<= function
	$ alias hid="ls -a | grep '^\.'"			<= alias

	  
### Route or IP

	$ route
	$ ip route
	
	$ /sbin/route -n					<=Numeric value
	Kernel IP routing table
	Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
	0.0.0.0         192.168.1.1     0.0.0.0         UG    0      0        0 eth0
	192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0

	Add / set-up a new route
	
	$ route add default gw {IP_ADDRESS} {INTERFACE_NAME}
	$ route add default gw 192.168.1.254 eth0
	
	$ ip route add 192.168.1.0/24 dev eth0
	$ ip route add 192.168.1.0/24 via 192.168.1.254
	

### Compression 	
	
    # tar (compression for backup)

	A. Creating tar file 
	# tar -c(create)v(verbose)p(preserve permission)z(compression)f(filename)
		  -cvpzf
	# tar -czvf archive-name.tar.gz directory-name	
	
	$ tar -czvf  backup.tar.gz /home/nozatech
	$ tar -cvpzf backup.tar.gz /(root)        --exclude=/mnt /
	$ tar -cvpzf backup.tar.gz /home/nozatech
	
	Extract tar file
	# tar -x(extract)v(verbose)p(permission)z(uncompress)f(file name) backup.tar.gz -C(change to different directory) /recover
	$ tar -xvzf  backup.tar.gz /home/nozatech
	$ tar -xvpzf backup.tar.gz /home/nozatech	
	$ tar -xzvf nexus-3.2.0-01-unix.tar.gz   -C   /opt


	$ tar -zcvf  <= create tar zip file
	$ tar -zxvf  <= extract tar zip file
	$ tar -tf    <= extract tar file
	
	#  Zipping or archiving
	$ bzip2 ls.txt &
	 [1] 28072
	$ jobs
	 [1]+  Done                    bzip2 ls.txt
	$ ll
	 -rw-r--r--. 1 root root  932 Jun  9 14:42 ls.txt.bz2
	

	
	
#-----------------------------------------------------------------------------------------------	
### Cron Jobs
	$ crontab -l (check teh crontab list)
	$ crontab -e (first time ask a editor - choose vim)
		* * * * *
		min         hr         day of month       month          day of month          command
		0-59 mins   0-23 hrs   1-31days           1-12           0(sun)-6(sat)
		30	        2     	   * 	              *              2(tuesday)
		
		
		
cron job search from command line
* * * * *  (*)<-Not used
| | | | | 	| 
| | | | | 	+-- Year            (range: 1900-3000)         
| | | | +---- Day of the Week   (range: 1(Monday)-7)
| | | +------ Month of the Year (range: 1-12)
| | +-------- Day of the Month  (range: 1-31)
| +---------- Hour              (range: 0-23)
+------------ Minute            (range: 0-59)

# examples 
	0 0 * * 1,3,5   			   <= 12AM at every Mon, Wed, Fri
	@reboot        				   <= Run once, at start up.  Put start up script!!!!
	0 0 1 1 *		@yearly        <= Run once a year
	@annually       @yearly 
	0 0 1 * *		@monthly       <= Run once a month
	0 0 * * 0		@weekly        <= Run once a week
	0 0 * * *		@daily         <= Run once a day
	@midnight       @daily 
	0 * * * *		@hourly        <= Run once an hour
	
	* * * * * /sbin/ping -c 1 192.168.0.1 > /dev/null
	0 0,12 1 */2 * 				   <= every 12am & 12pm on 1st day of every 2nd month
	30 08 10 06 * 				   <= June 10th 8:30AM (every year)
	00 09-18 * * 1-5 			   <= Everyday 9am to 6pm Mon ~ Fri
	
	
https://en.wikipedia.org/wiki/Cron
Entry	Description	Equivalent to
@yearly (or @annually)	Run once a year at midnight of 1 January		0 0 1 1 *
@monthly	Run once a month at midnight of the first day of the month	0 0 1 * *
@weekly		Run once a week at midnight on Sunday morning				0 0 * * 0
@daily		Run once a day at midnight									0 0 * * *
@hourly		Run once an hour at the beginning of the hour				0 * * * *
@reboot		Run at startup	N/A	
	

### Jenkins Cron jobs	
http://stackoverflow.com/questions/12472645/how-to-schedule-jobs-in-jenkins
15  13  *    *    *    				<= run 15th minute of the 13th hour of the day
min hr day month week

0 0 * * * for a dozen daily jobs will cause a large spike at midnight.
| |
H H * * * would still execute each job once a day, but not all at the same time, 
		  better using limited resources.
		The "H" symbol can be thought of as a random value over a range, but it actually is 
		a hash of the job name, not a random function, so that the value remains stable for 
		any given project.
H 8 * * *				<= run every 8AM
H 8 * * 0				<= run on Sunday 8AM
H/15 * * * * 			<= every fifteen minutes
H(0-29)/10 * * * *		<= every ten minutes in the first half of every hour 
							(three times, perhaps at :04, :14, :24)
H 9-16/2 * * 1-5 		<= once every two hours every weekday 
							(perhaps at 10:38 AM, 12:38 PM, 2:38 PM, 4:38 PM)
H H 1,15 1-11 * 		<= once a day on the 1st and 15th of every month except December	


0 12 * * 1-5			<= 12:00 every weekday (Mo-Fr)
H 2 * * *				<= Every 2am



	
# Install crontab
	$ crontab cron_jobs.txt
	
	###
	$ cat cron_jobs.txt
	* * * * *         /sbin/ping -c 1 192.168.0.1  > /dev/null 2>&1
	* * * * *         /sbin/ping -c 1 192.168.0.1  2>&1 >> /tmp/log   <= save to log file
	
	0 0,12 1 */2 *    /cmd				    # every 12am & 12pm on 1st day of every 2nd month
	30 08 10 06 * 	  /cmd					# June 10th 8:30AM
	00 09-18 * * 1-5  /cmd			   		# Everyday 9am to 6pm Mon~Fri
	5 * * * *								# every 5th min each hour
	*/5 * * * *								# every 5 mins
	
	###
	
	$ crontab -l				<= current user's list of cron jobs 
	$ crontab -e				<= edit cron job
	
	### Finding cron job list for specific user ### 	
	$ crontab -u user_id -l					<= list jobs 
	$ crontab -u user_id -e					<= edit
	
	### Finding cron job list for all users ###
	####################################################################################
	###   $ for user in $(cut -f1 -d: /etc/passwd); do crontab -u $user -l; done     ###
	####################################################################################
	
	# permission issue
	$ sudo "for user in $(cut -f1 -d: /etc/passwd); do echo $user; crontab -u $user -l; done"

	
	a. Getting all user id info from /etc/passwd using 
		$ cut -f1 -d:  /etc/passwd
    
	b. using crontab utility to check list of cron job list
		$ crontab -u $user -l

	##########################################
	# #!/bin/bash
	# for user in $(cut -f1 -d: /etc/passwd); 
	# do 	
	#    echo $user; crontab -u $user -l; 
	# done
    ##########################################
#-----------------------------------------------------------------------------------------------

	
	
15. List files by modified date 
    ls -atlh   			 	 <= -a all, -t modified time, -l per line 
    ls -atlhr   			 <= -r reverse
	
	list directory only
	$ echo */ 
	$ ls -d */
	$ ls -dl */
	$ ls -l | grep "^d"
	$ for i in $(ls -d */); do echo ${i%%/}; done
	$ for i in $(ls -d */); do echo ${i}; done

	$ ls -l
-rwxrw-r--    10    root   root 2048    Jan 13 07:11 afile.exe
	
?UUUGGGOOOS   00  UUUUUU GGGGGG ####    ^-- date stamp and file name are obvious ;-)
^ ^  ^  ^ ^    ^      ^      ^    ^
| |  |  | |    |      |      |    \--- File Size
| |  |  | |    |      |      \-------- Group Name (for example, Users, Administrators, etc)
| |  |  | |    |      \--------------- Owner Acct
| |  |  | |    \---------------------- Link count (what constitutes a "link" here varies)
| |  |  | \--------------------------- Alternative Access (blank means none defined, anything else varies)
| \--\--\----------------------------- Read, Write and Special access modes for [U]ser, [G]roup, and [O]thers (everyone else)
\------------------------------------- File type flag
	
	type_descriptor types:-
	-rw-rw-r--.    	<= f: regular file
	drw-rw-r--. 	<= d: directory
	lrw-rw-r--. 	<= l: symbolic link
	crw-rw-r--. 	<= c: character devices
	brw-rw-r--. 	<= b: block devices	

	
### Filesystem type  
	$ df -T						<= disk free
	$ df -Th
	$ df -Tha
		Filesystem              Type         Size  Used Avail Use% Mounted on
		rootfs                  rootfs        18G  1.1G   17G   7% /

###########################################
### Add new hard drive or partition
###########################################

	1. 	$ fdisk -l
		$ ls /dev/sd*      (Ubuntu /dev/xvd*)
		
	2. 	$ fdisk /dev/sdb			<= Create new partitions
		$ command(m for help): p
			p	 print the partition table
		   Device Boot      Start         End      Blocks   Id  System

		$ command(m for help): n
			n   add a new partition
			Partition type:
			p   primary (0 primary, 0 extended, 4 free)
			e   extended
			Select (default p): p
			Partition number (1-4, default 1): 1
			
		$ command(m for help): p
			Device Boot      Start         End         Blocks   Id  System
			/dev/sdb1        2048        10485759     5241856   83  Linux
		
		$ Expert command (m for help): w
		The partition table has been altered!
		
		
		$ fdisk -l
	    Device Boot      Start         End      Blocks   Id  System
		/dev/sdb1            2048    10485759     5241856   83  Linux
		
		$ file -sL /dev/xvd*
		  /dev/xvda1: Linux rev 1.0 ext4 filesystem data  					<= ext4
		
	3. 	$ mkfs.ext3 /dev/sdb1
	    $ mkfs.ext4 /dev/xvdb                                               <= ext4 

	4. 	$ mount -t ext3 /dev/sdb1 /mnt/mysql -rw
	    $ mount -t ext4 /dev/xvdb /puppet/ -rw
		# $ umount /mnt/mysql

	$ cat /etc/fstab					<= check hdd list
	5. 	$ echo '/dev/sdb1  /mnt/mysql  ext3  defaults 0 0' >> /etc/fstab
	
	
	6. $ mount | column -t				<= Check mount list
	   $ cat /proc/mounts
	
	# Mount recovery disk
	$ fdisk -l
	$ mount /dev/xvdf /mnt/dir or /tmp/dir
	
	# Unmount
	$ umount /mnt/dir
	 -l, --lazy              detach the filesystem now, and cleanup all later


	# tmpfs    /dev/shm 		<=Shared Memory, it is a file system, which keeps all files in virtual memory
	https://www.cyberciti.biz/tips/what-is-devshm-and-its-practical-usage.html
	/dev/shm (filesystem-level shared memory) also known as 'tmpfs'
	
	
	
	
	$ df -h
	tmpfs           246M     0  246M   0% /dev/shm
	
	
	$ mount -t tmpfs -o size=5G,nr_inodes=5k,mode=700 tmpfs /disk2/tmpfs

	-o opt1,opt2 	   : Pass various options with a -o flag followed by a comma separated string of options. 
	remount 		   : Attempt to remount an already-mounted filesystem. In this example, remount the system and increase its size.
	size=8G or size=5G : Override default maximum size of the /dev/shm filesystem. he size is given in bytes, 
						and rounded up to entire pages. The default is half of the memory. The size parameter 
						also accepts a suffix % to limit this tmpfs instance to that percentage of your pysical RAM: 
						the default, when neither size nor nr_blocks is specified, is size=50%. In this example 
						it is set to 8GiB or 5GiB. The tmpfs mount options for sizing ( size, nr_blocks, and nr_inodes) 
						accept a suffix k, m or g for Ki, Mi, Gi (binary kilo, mega and giga) and can be changed on remount.
	nr_inodes=5k 	   : The maximum number of inodes for this instance. The default is half of the number of your physical 
						 RAM pages, or (on a machine with highmem) the number of lowmem RAM pages, whichever is the lower.
	mode=700 	       : Set initial permissions of the root directory.
	tmpfs 			   : Tmpfs is a file system which keeps all files in virtual memory.
	
	# Permanent
	$ vi /etc/fstab
	none      /dev/shm        tmpfs   defaults,size=8G        0 0
	 
	$ man mount 
	nodev 	<= Do not interpret character or block special devices on the file system. 
	noexec 	<= Do not allow execution of any binaries on the mounted file system.
	nosuid 	<= Do not allow set-user-identifier or set-group-identifier bits to take effect. 
	 
	 
	File descriptor
	https://en.wikipedia.org/wiki/File_descriptor
	
#---------------------------------------------------------------------	
### lsof and fuser
#---------------------------------------------------------------------

	# fuser - which processes using files or sockets
			- user owning the process and the type of access
			- displays the process id(PID) of every process using the specified files or file systems.
	https://www.digitalocean.com/community/tutorials/how-to-use-the-linux-fuser-command
	
	$ fuser -l
	
	$ fuser -v .	(current dir)<= gives information about the USER, PID, ACCESS and COMMAND
        USER        PID ACCESS COMMAND
	/home/apark/localVariable:
        apark     10233 ..c.. bash
        apark     26462 ..c.. bash
	
	$ fuser -v    /  (root dir)
		USER        PID ACCESS COMMAND
	/:  root     kernel mount /
        apark     10028 .r... bash
			............
        apark     26462 .r... bash

	$ fuser -vm   /								<= -v verbose, -m mount
	
	# show all processes having open files on the filesystem mounted on / (root directory)
	$ fuser -vm / 2>&1 | awk '$3 ~ /f|F/' | less -N 		<=?????

### Network 
	$ nc -l -p 80									<= Netcat -l listen -p port 
	$ fuser -v -n tcp 80							<= -n name space
					USER        PID ACCESS COMMAND
		80/tcp:   	apark     20168 F.... nc

### Kill
	$ fuser -k -i 80/tcp						<= -i ask before kill
	
### Find The Process Accessing A File System
	$ fuser -v -m /var/log/dmesg						<= -v verbose, -m mount
	
					USER        PID ACCESS COMMAND
	/var/log/dmesg: root     kernel mount /                
                    root          1 .rce. systemd
                    root          2 .rc.. kthreadd
                    root          3 .rc.. ksoftirqd/0
					............
					nginx     19176 Frce. nginx
                    root      19295 .rce. sshd
                    apark     19310 .rce. sshd
					icinga    26921 Frce. icinga2
                    root      27046 F.... python
                    root      27784 .rc.. kworker/u8:1

	
	
	
	
------------------------------------- ------------------------------------- 		
### lsof ###				<= list open files using by PID or Program
	http://www.thegeekstuff.com/2012/08/lsof-command-examples
	
	$ lsof 					<= list all opened files start 'Init 1'

	# Which processes have this file open
	$ lsof  /var/log/nginx/access.log
		COMMAND   PID  USER    FD   TYPE DEVICE SIZE/OFF      NODE NAME
		nginx   13786  root    5w   REG  253,0        0 135700508 /var/log/nginx/access.log
	
	# list opened files under a directory
	$ lsof +D  /var/log  ==   $ lsof /var/log/
	
	# List opened files based on process names starting with
	$ lsof -c ssh
	
	# Which files does PID have open?
	$ ps aux | grep nginx |
	$ lsof -p nginx_PID							<= Process ID (PID)

	$ lsof -p 1									<= all files are opened with Init or SystemD
	$ lsof -p `pgrep systemd`
		
	# Where is the Binary for this process?
	$ lsof -p `pgrep ngnix` | grep bin
	
	# Which shared Libraries is this program using? (manually upgrading software, i.e. openssl)
	$ lsof -p PID | grep .so
	
	# Where is this thing logging to?
	$ lsof -p ABC | grep log
	
	# Which processes still have this old library open?
	$ lsof | grep libname.so
	
	# Which files does user_id have open?
	$ lsof -u user_id
	$ lsof -u user_id -i 					<= network only
	
	** # Kill all process that belongs to a particular user
	$ kill -9 `lsof -t -u user_name`
	
	# Network connection - Which process is listening on Port/Protocol
	$ lsof -i :80
	$ lsof -i tcp
	$ lsof -i tcp; lsof -i udp;
	
	
	# Finding which Process uses port 80
	$ lsof -i :80  
	COMMAND PID     USER            FD   TYPE DEVICE SIZE/OFF NODE NAME
	java    477   """logstash"""   79u  IPv6  22585      0t0  TCP *:lxi-evntsvc (LISTEN)

	$ lsof -Pni :3306
	
	/dev/mapper/U14--vg-root on / type ext4 (rw,errors=remount-ro)
	
	

	
### What Kernel supports filesystem
	$ cat /proc/filesystems
		nodev   devpts
        ext3
        ext2
        ext4
		
	$ cat $(which sys-unconfig) 	<=newer / older=>  $ cat `which sys-unconfig`
	
	### sys-unconfig  <= shutdown the <SYSTEM>!!!
	
	
	$ df -h    <= report file system disk space usage (Check Disk Free)
	$ df -T   <= file system type
	$ df -i   <= inodes
		Filesystem                Inodes IUsed    IFree IUse% Mounted on
		/dev/mapper/centos-root 18358272 26054 18332218    1% /

	df -h | xargs | awk '{ print $11 " / " $9 " are free" }'

	df -h | grep /dev/mapper | awk '{print $4 "/" $2 "GB is free"}'

	df -h | awk '{print $5}' | grep % | grep -v Use |  sort -n | tail -1 | cut -d "%" -f1 -

 
  169  cat /proc/mounts
  
  170  ls /mnt
  
  171  mount -l
  
  172  history | grep mount
  
  173  ps aux | grep jarvis
  
  174  ls -la /
  
  175  ls -la /mnt/
  
  176  history
		$ 
		$ !ls
  
  
  
  
  
  
  
  
### chmod 

https://en.wikipedia.org/wiki/Chmod
In "chmod aw" the "a" means "All" (everyone) and the "w" means "write" (edit) the file.
chmod + x
chmod + xu

u = user that owns the file
g = group that owns the file
o = other (everyone else)
a = all (everybody)

r = 4   <= read aces to the file
w = 2	<= write access
x = 1	<= execute (run) access
  
### Numerical permissions
#	Permission	rwx
7	rwx		read, 	write, 	execute	
6	rw- 	read, 	write	
5	r-x		read, 	execute	
4	r--		read 		
3	-wx		write,	execute	 
2	-w-		write 		
1	--x 	execute 
0	---		none	
  
$ chmod a-x 	text.txt			<= -x 'REMOVES' Execute permission for all classes
$ chmod a+rx 	test.sh				<= adds read and execute permissions for all classes
$ chmod -R u+w,go-w  text.txt		<= adds write permission to the directory docs and all its contents 
						(i.e. Recursively) for owner, and removes write permission for group and others
$ chmod ug=rw groupAgreements.txt	<= sets read and write permissions for owner and group  
  
  
  
  177  lsblk
  
  178  sudo mount /dev/xvdf /mnt
  179  ls -la /mnt/
  180  sudo vim /etc/fstab
  181  history | grep start
  182  sudo start jarvis-01 && sudo start jarvis-02 && sudo start jarvis-03 && sudo start jarvis-04
  183  less /mnt/log/jarvis-01/jarvis.log
  184  sudo service mongos start
  185  sudo stop jarvis-01 && sudo stop jarvis-02 && sudo stop jarvis-03 && sudo stop jarvis-04 && \ 
	   sudo start jarvis-01 && sudo start jarvis-02 && sudo start jarvis-03 && sudo start jarvis-04
  186  less /mnt/log/jarvis-01/jarvis.log
  187  ps aux | grep nginx
  188  curl 'http://localhost:53213/tcg/api/1/status'
  189  less /mnt/log/jarvis-01/jarvis.log
  190  exit
	
	


### Count and Check Active connections
	# ss			<= another utility to investigate sockets

	$ ss  		<= is  used to dump "Socket Statistics"(replacement of NETSTAT.)
				http://www.binarytides.com/linux-ss-command/
	$ ss -t 	<= tcp("established = connected)  
	$ ss -tna 	<= n <- not to resolve hostname, -a <- all
	$ ss -u 	<= udp
	$ ss -ltn
	
				
	$ netstat -na | grep ESTA | wc -l				<== n no reverse lookup(quick), t tcp, a all, p program
	$ netstat -na | grep ESTABLISHED.*sshd		<= sshd connection list
	$ ps auxwww | grep sshd:						<= sshd connection list
	$ netstat -plunt								<= which ports are open and which program(application) is listening
	$ netstat -punt								<= program, udp, no lookup, tcp
	$ netstat -ntl								<= no lookuop, tcp, LISTEN port

	$ watch -d -n1 "netstat -ntap | grep ESTA"  		<= Print active connections n1 <= every 1sec
	
	$ watch -n5 'netstat -ntap | grep ESTA | wc -l'	<= Print active connections n1 <= every 1sec
	
	$ watch -n 0.1 "dmesg | tail -n $((LINES-6))"
	
	$ while true; do dmesg -c ; sleep 1 ; done

### Infinite loop - while loop
https://unix.stackexchange.com/questions/42287/terminating-an-infinite-loop/121391#121391

>>>	$ while true; do ping google.com; done  	
	
	
	$ Ctrl +c 			<= hold down UNTIL '^C'
	$ Ctrl +z 			<= stop the job
	$ jobs
		[1]+  Stopped                 ping google.com
	$ kill %!
		[1]+  Terminated              ping google.com
	
	$ while [ 1 ]; do COMMAND || break; done;
	
	

### vmstat - report virtual memory statistics
	- vmstat reports information about processes, memory, paging, block IO, 
	  traps, disks and cpu activity.
	vmstat is a tool that collects and reports data about your system’s memory, 
	swap, and processor resource utilization in real time. It can be used to 
	determine the root cause of performance and issues related to memory use.
	$ vmstat [interval] [count]
	$ vmstat 1 20 	<= 1sec  20 times
	$ vmstat -S k 1 10
		In the default operation, vmstat displays memory statistics in kilobytes. 
		vmstat considers a single kilobyte equal to 1024 bytes. To generate vmstat 
		reports where 1 kilobyte is equal to 1000 bytes, use the following form:
	
	

	
	
### Alias
	#only used in interactive shells and not in scripts!!
	https://mywiki.wooledge.org/BashGuide/CommandsAndArguments
	
	$ nmap -Pn -A --osscan-limit 192.168.0.1  	<= from shell
	
	$ alias nmapp="nmap -Pn -A --osscan-limit"	<= alias
	$ nmapp 192.168.0.1							<= usage
	
	$ cd; vi .bash_profile
	$ alias server1='ssh root@ip_address -p 3404'   <= add your alias in .bash_profile
	$ alias lp='ls -al ../'							<= ls -al parent directory  


### Time NTP

	alias today='date +"%A, %B %-d, %Y"'

	date  => Thu Dec  4 10:11:00 EST 2014
	today => Thursday, December 4, 2014
	date +"%m-%d-%Y" => 12-04-2014

	### CentOS7 ###
	$ timedatectl - Control the system time and date
 	   --adjust-system-clock
           If set-local-rtc is invoked and this option is passed, the system
           clock is synchronized from the RTC again, taking the new setting
           into account. Otherwise, the RTC is synchronized from the system
           clock.

	To correct zone and time
	$ systemctl restart  ntpd.service
	
	# TimeZone changes to PST 
	https://www.cyberciti.biz/faq/centos-linux-6-7-changing-timezone-command-line/
	$ date
	$ ls -l /etc/localtime
	$ timedatectl
	$ timedatectl list-timezones
	$ timedatectl list-timezones | grep -i los
		America/Los_Angeles
	$ timedatectl set-timezone America/Los_Angeles

	
	
	
### cat and strings
	$ cat /usr/bin/bash XXXX
    
	$ zcat file.archived.tar.gz			<= reading gzip (view zip) file without extract first 
	
	$ strings /usr/bin/bash  <= human readable content buried inside the program.
 

   
### uptime		<= uptime, number of users,        CPU load 1,   5,  15 mins
	09:35:45 up 10 days, 21:18,  2 users,  load average: 0.03, 0.03, 0.05
	
	### How many days it has been running
	$ uptime | awk '{print $3}'    | cut -f1 -d,
		10  <= 10days
		
	$ uptime | awk '{print $3,$4}' | cut -f1 -d,
		10 days	
   
### Script 	<= to TEXT file		
	# Record and Replay Linux Terminal Sessions
	https://www.tecmint.com/record-and-replay-linux-terminal-session-commands-using-script/ 
	$ script userInfo.log
		$ w
		$ whoami
		$ exit		<= when the recording is done
	
	$ cat userInfo.log				<= view
	$ vi  userInfo.log				<= edit
	
	# Append(adding more session)
	$ script -a userInfo.log		<= -a append
	
	### Scriptreplay  <= Movie file ###
	$ script 	   --timing=timeline.txt movieRecording.log
	$ scriptreplay --timing=timeline.txt movieRecording.log
	
### List of services running
	$ /usr/sbin/service --status-all
	$ /sbin/service --status-all
	$ service --status-all
	
### mkdir
	$ mkdir -p production/{modules,manifests}      <= creating multi SUB folders
	
	mkdir and cd into directory
	$ mkdir ~/docker-registry && cd $_	

	
	

### Echo 
# echo -n opntion
	$ echo -n Hello World\$			<= -n no return line
		Hello World$apark@i7~$		<= \$ ignore special char   
	
	$ echo -n "Hello World\n\n"
		Hello World\n\napark@i7~$   <= -n NO return line
									<= \n\n prints as string
# echo -e option	
	$ echo -e "Hello World\n\n"		<= -e enable interpretation of backslash escapes
		Hello World
		empty line					<= \n prints a new empty line 
		empty line					<= \n prints a new empty line 

	$ echo ~						<= prints user home directory
	$ echo ~root					<= prints root home directory
	$ echo ~+  == pwd
	$ echo ~-  == cd -
	$ echo s{pe,pi}ll				<= don't use quote ""
		spell spill

# double & single quotes	
    $ echo 'Single quotes "protects" the double quotes.'
        Single quotes "protect" double quotes.
		  
	$ echo "My name is $USER"			<= apark  variable
	$ echo 'My name is $USER'			<= $USER  same
	
	$ pdir="/tmp/files/today"
	$ fname="report"
	$ mkdir -p $pdir/$fname
	$ ls $pdir
		report
	$ touch $pdir/$fname_jan		<= Not working
	$ touch $pdir/${fname}_jan		<= user {} to protect the variable $fname
	$ ls $pdir/$	/tmp/files/today/report_jan

	$ echo  My name is $(whoami)	
	$ echo "My name is $(whoami)"
	$ echo  My name is `whoami`
	$ echo "My name is `whoami`"
	$ chown -R $(whoami) /home/$(whoami)/test
	
### Nested command substitution
	$ echo " Permissions for find are $(ls -l $(which find))"
		4th			3rd					2nd			1st(/bin/find)
		Permissions for find are -rwxr-xr-x. 1 root root 199200 Nov 20  2015 /usr/bin/find
	
	$ HOSTNAME=hostname							<=variable HOSTNAME into command hostname
	$ echo "PC name is $HOSTNAME." 				<= PC name is PC.
	$ echo "PC name is $(hostname)."			<= PC name is PC.
	$ echo "PC name is $(HOSTNAME)."			<= PC name is PC.

# \ ingnore special character
    $ echo "PC name is \"($HOSTNAME)\"."		<= \ ignores " as a string 
    


    $ echo "Well, isn’t that \"special\"?"
    $ Well, isn’t that "special"?

    $ echo "You have `ls | wc -l` files in `pwd`"
		You have 43 files in /home/bob
 
    $ x=100
	$ echo "The value of \$x is $x"
        The value of $x is 100

###
	# echo -e    						 <= Enables interpretation of backslash escapes
	$ echo -e "Inserting blank\n\n\n" 					<= \n  adding blank newline lines to text
		
	$ echo -e "worlds\tseperate\tby\ttabs."				<= \t  inserting tabs
	
	$ echo -e "\aMy computer \\went \"beep\"." 			<= \a  alert	makes your terminal beep										
														<= \\	backslash	insert a backslash
	
	
	
	
	
	
### Path substitution ### 
	
### Pushd						<= add directory
	Add directories to stack.
    Adds a directory to the top of the directory stack, or rotates
    the stack, making the new top of the stack the current working
    directory.  With no arguments, exchanges the top two directories.
	
### dirs						<= list directories
	Dispay directory stack
	Display the list of currently remembered directories.  Directories
    find their way onto the list with the `pushd' command; you can get
    back up through the list with the `popd' command.

### popd						<= remove a directory
	are shell builtins which allow you manipulate the directory stack. 
	This can be used to change directories but return to the directory from which you came.
	https://unix.stackexchange.com/questions/77077/how-do-i-use-pushd-and-popd-commands
	
	$ pushd /var/log/httpd
	$ pushd /var/www/html
	$ dirs
	  /var/log/httpd  /var/www/html
	$ echo ~0
	  /var/log/httpd
	$ echo ~1
	  /var/www/html
	$ popd +1
	$ dirs
	  /var/log/httpd
	
	
	
	
	
	
	
---------------------------------------------------------------------------------------	
### $ last						<= show listing of last logged in users
###	$ Last Reboot ###
	$ who -b
	$ last reboot
	reboot system boot 3.2.13-grsec-xxx Tue Apr 3 07:34 - 09:17 (9     + 01 : 42) 
																 ^9days 1hr 42mins after restart
	
	$ last
	$ last -x
	$ last -x reboot
	$ last -x shutdown
	$ lastlog					<= reports the most recent login of all users 
    
	$ who
	$ who -r 					<= runlevel
	$ who -b

### shutdown 
https://www.computerhope.com/unix/ushutdow.htm
	halt 
	poweroff 
	reboot 
	shutdown
	
	$ shutdown -h 10 wall "Shutdown in 10 mins"				<= Shutdown immediately
	$ shutdown -h 0				<= Shutdown immediately
	$ shutdown now				<= Shutdown immediately
	$ shutdown -P now			<= Shutdown immediately
	$ shutdown -r now			<= Reboot immediately.
	
	$ reboot					<= Reboot immediately.
	
	$ echo "reboot" | at 0000 jun 27 | wall "Rebooting"
	
	$ shutdown -r 10 wall "Reudo sboot in 10 mins"			<= 1 for 60mins
		Shutdown scheduled for Tue 2017-12-05 11:08:28 PST, use 'shutdown -c' to cancel.
		Broadcast message from root@i7 (Tue 2017-12-05 10:58:28 PST):
				wall Reboot in 10 mins
		The system is going down for reboot at Tue 2017-12-05 11:08:28 PST!

	$ poweroff	
		
### Check
	$ last reboot			<= check last reboot time
	### CANCEL SHUTDOWN SCHEDULE ###
	$ shutdown -c
	### Time count issue for Shutdown command
	# 1 <= 1 hour, 30 
	$ sleep 30s; shutdown -r now			<= reboot after 30sec
	$ shutdown -P +60						<= shutdown after 60 mins
	$ shutdown -P 1:00						<= shutdown at 1AM
	$ 
	
	
---------------------------------------------------------------------------------------	
15. Is .bash_profile is a file???? 
	if [ -f .bash_profile ] ; then 					<= -f file is exist & regular file
		echo "you have the file in your home dir"		
    	else 
		echo "You do not have it"
	fi
---------------------------------------------------------------------------------------
### id -u        (-u <= --user for script)   
	print real and effective user and group ID
	uid=0(root) gid=0(root) groups=0(root) 				context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

---------------------------------------------------------------------------------------
### rm -rf ./tmp/test/delme/*    <=delete folder contents
    
	# deleting . file(hidden file)
	rm -rf .*
	rm -rfi .*
	
	## search all files bigger than 100MB
	# use ncdu utility
	
	$ find /  -type f -size +100M   					<= k-kilo, M-mega, G-giga

	$ find / -name "*.txt" | xargs rm -rf				<= search all and delete them
	
	$ find / -name “*.txt” | xargs grep “Tecmint”		<= search and search string "Techmint"
	
    ###!!! Danger !!!###
    rm -fr ./tmp/test/delme/ *                <=delete whole root folder(./tmp/test/delme)   

	$ find . -type f -name "*.bak" -exec rm -i {} \;
	
	$ find . -type f -name "FILE-TO-FIND" -exec rm -f {} \;
	
		-name "FILE-TO-FIND" : File pattern.
		-exec rm -rf {} \; : Delete all files matched by file pattern.
		-type f : Only match files and do not include directory names.
		
---------------------------------------------------------------------------------------	
#### Delete local backup folder older than 2 days	####
	$ sudo find /backup/ -mindepth 1 -mtime +2 -delete

	$ sudo find /data/backups/ -type f -mtime +0 -name 'rrs_us_db.sql.*' -delete   	<= mtime (days), mmin (mins)
	
	-mtime   
		Day 0~1, 2, 3, 4, 5
		+2 <= 3,4,5 older than 2days, -2 <= 1,2 less than 2days
		
	ctime
		ctime is the inode or file change time. The ctime gets updated when the file attributes are changed, 
		like changing the owner, changing the permission or moving the file to an other filesystem but will 
		also be updated when you modify a file.

	mtime
		mtime is the file modify time. The mtime gets updated when you modify a file. Whenever you update 
		content of a file or save a file the mtime gets updated. 
		
		Most of the times ctime and mtime will be the same, unless only the file attributes are updated. 
		In that case only the ctime gets updated.

	atime
		atime is the file access time. The atime gets updated when you open a file but also when a file is 
		used for other operations like grep, sort, cat, head, tail and so on.

 
---------------------------------------------------------------------------------------	
### touch 
	touch file_{1..100}    <= creating 100 files e.g. 1,2..100
    touch file_{01..100}   <= to resolve file format problem above logic e.g. 001, 002..100
    touch {1,2,3,4,5}	   <= creating 1,2,3,4,5 files
    touch ./ls/{1..100}    <= creating files under ./ls/ folder
    touch ./tmp/test/delme/file_{01..100}
  
    echo {1..10}            	 <= print out 1,2,..10
    echo {1..10..2}	   			 <= Prints out 1 3 5 7 9
    echo {1..10..3}        		 <= Prints out 1 4 7 10
    echo {A..Z}              
    echo {A..z}		   			 <= Capital first    
    echo {w..d..2}	    		 <= Reverse order for every 2nd alphabet

    touch {apple,banana,cherry,durian}_{01..100}{1..10} 
    ls -1 | wc -l    	   		 <= 4000 result

    Prints special character without errors	
    echo '"! # $ % & '\'' ( ) * + , - . / : ; & < = > ? @ [ \ ] ^ _ { | } ~"'
    echo "! # $ % & '\'' ( ) * + , - . / : ; & < = > ? @ [ \ ] ^ _ { | } ~"
	
---------------------------------------------------------------------------------------
23. 
	echo $BASH_VERSION
	echo $MACHTYPE
	echo $SECONDS 					<= Count script started 


	d=$(pwd)
	echo $d

	echo 1/3 | bc -l                <= .33333333333333333333

###	IP address 						<= finding IP from ifconfig using grep re  gex
	$ ifconfig | grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b"
	 192.168.232.132

	$ ip neighbour show			<= how your subnet's computers

	$ ip addr show eth0 | grep inet | awk '{ print $2; }' | sed 's/\/.*$//'
	$ ip addr show eth0 | grep inet | awk '{ print $2; }' | sed 's/\/.*$//' | head -n 1
	
### tail
  tail -f *Slave.log
  tail -f *vSlave.log | grep "Info: Slave - Sending status"	



25. a="hello"
	echo ${#a}   <= 5 number for character
	





27. Printf vs echo (http://wiki.bash-hackers.org/commands/builtin/printf)
	$ printf Hello World			Hello					<= Prints only first argument
	$ printf "Hello World"			Hello World				<= Prints both, but No New Line
		hello worldapark@i7~/
	$ printf '%s\n' "Hello World"							<= Prints both, % New Line
		Hello World											
	
	$ printf '%s\n' "$var"				<= %s	Interprets the associated argument literally as String
										<= %n	a new line
	$ echo "\n\t"
		-n     do not output the trailing newline
		-t     do not output the trailing horizontal
		 
	






29. Empty file contents or logs
						Bourne POSIX  zsh    csh/tcsh  rc/es  fish
	> file                Y      Y      N(1)   N(1)      N      N
	: > file              N/Y(2) Y(3)   Y      Y(4)      N(5)   N(5)
	true > file           Y(5)   Y      Y      Y(5)      Y(5)   Y(5)
	cat /dev/null > file  Y(5)   Y      Y(5)   Y(5)      Y(5)   Y(5)
	cp /dev/null file (7) Y(5)   Y      Y(5)   Y(5)      Y(5)   Y(5)
	printf '' > file      Y(5)   Y      Y      Y(5)      Y(5)   Y
	eval > file           Y(3,8) Y(3)   Y      Y(6)      Y      Y
------------------------------
### Eval  
------------------------------
	is part of POSIX. Its an interface which can be a shell built-in.

	https://unix.stackexchange.com/questions/23111/what-is-the-eval-command-in-bash
	Man Page: The args are read and concatenated together into a single command.   This  command  is
    then  read  and executed by the shell, and its exit status is returned as the value of
    eval.  If there are no args, or only null arguments, eval returns 0.
	
	Example-1
	1) foo=10 x=foo		<= define $foo with the value '10' and $x with the value 'foo'.
	2) y='$'$x			<= define $y, which consists of the string '$foo' / ecapted with '$'
	3) echo $y
	4) 	$foo			<= check $y value
	5) eval y='$'$x		<= evaluate $x to the string 'foo'. y=$foo which will get evaluated to y=10
	6) echo $y
	7) 10				<= The result of echo $y is now the value '10'
	Example-2
	$ A="ls | less"		<= two cmds to concatenated	
	$ $A 			
	  -bash: ls|less: command not found
	$ eval $A			<= use eval to execute the variable $a

------------------------------	
### let   
------------------------------	
	let, expr, $[],  (( ))
	https://askubuntu.com/questions/939294/difference-between-let-expr-and

	$ let arg [arg ...]  same as (( )) <- ARITHMETIC EVALUATION
        Each  arg  is an arithmetic expression let == (( )) to be evaluated.
        If the last arg evaluates to 0, let returns 1; 0 is returned otherwise.
	$ no1=4 no2=5
	$ let result=no1+no2
	$ echo $result
	  3
	$ let no1++ 
	$ let no2-- 
	
	$ result=no1+no2
	$ result=$[ no1 + no2 ]
	$ result=(( no1 + no2 ))
	$ echo $result
	  9
	$ result=`expr 3 + 4`				<= If no spze 3+4 out put will be 3+4, not 7
	$ result=$(expr no1 + 4)			<= If no spze no1 +4 out put will Syntax error!
	
	## BC  <- An arbitrary precision calculator language
	$ no1=4
	$ echo "4 * 0.56" | bc
	$ result=`echo "$no1 * 1.5" | bc`
	$ echo $result
	  6.0

	Decimal => binary => Octal  
	
	
	
30. df . and  df / are same result

	df -h / | grep -E "\/$" | awk '{print $4}'
	df -h / | xargs | awk '{print $11}'       <= Same result
	
	fdisk -l				
	
	sfdisk -l -uM				<=partition table manipulator for Linux
	
	cfdisk /dev/sda1  			<= a linux partition editor with an interactive user interface based on ncurses
	
	parted -l  <= list out partitions 
	
	df -h | gep ^/dev
	
	$ lsblk  				<= Lists out all the storage blocks
		NAME  MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
		xvda1 202:1    0  12G  0 disk /

	$ blkid 			 <= prints the block device(partitions and storage media) 
							attributes like uuid and file system type.
	




36. 







40. Clean Up Logs
	
    $ cd /var/log
	$ > log_file
    $ cat /dev/null > messages
	$ cat /dev/null > wtmp			<= utmp, wtmp - login records
  
	echo "Log files cleaned up."

	utmp <= complete picture of users logins at which terminals, logouts, system events and current status 		
			of the system, system boot time (used by uptime) etc.
	wtmp <= historical data of utmp.
	btmp <= records only failed login attempts.

	$ last 		<= default location without using -f(file) /var/log/wtmp  
	$ last -f /var/run/utmp   <= /var/run/utmp for different location using -f flag
	$ last -f /var/log/btmp

41.	Watch command to real time monitoring

	$ watch lsof -i			<= Watching who are connecting to a system and disconnecting, every 2sec
  

  
	$ watch -n 1 "ps aux | grep app_name"
<<<<<<< Updated upstream

=======
>>>>>>> Stashed changes

### encryption and decryption
	$ openssl passwd -1 your_password  
	  $1$nd1lHN8J$ehlK62ZgzrZDBq6ZHPT/   <= hashed value
	
	
	$ mkpasswd -l 15 -d 3 -C 5

	$ pwgen -s -1



	
42. strace - trace(monitor) system calls and signals of a program
	$ strace foobar.sh  	<= debugging a command/script   
	$ strace -p PID
	
	1. Trace the Execution of an Executable
		$ strace ls
		execve("/bin/ls", ["ls"], [/* 21 vars */]) = 0
		................
	2. Trace a Specific System Calls in an Executable Using Option -e
		$strace -e open ls
		open("/etc/ld.so.cache", O_RDONLY)      = 3
		.............
	#only the open system call of the ls command
		$ strace -e trace=open,read ls /home
	3. Save the Trace Execution to a File Using Option -o
		$ strace -o output.txt ls
	4. Execute Strace on a Running Linux Process Using Option -p
		$ strace -p PID
		
	5. Print Timestamp for Each Trace Output Line Using Option -t
		$ strace -t -e open ls /home
		11:59:25 open("/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3
		11:59:25 open("/lib64/libselinux.so.1", O_RDONLY|O_CLOEXEC) = 3

	6. Print Relative Time for System Calls Using Option -r
		$ strace -r ls 
	7. Generate Statistics Report of System Calls Using Option -c
		$ strace -c ls /home
		noza  nozatech
		% time     seconds  usecs/call     calls    errors syscall
		------ ----------- ----------- --------- --------- ----------------
		25.48    0.000859          29        30           mmap
		14.12    0.000476          24        20           mprotect
	8. 

# Network Troubleshooting
Traceroute, Ping, and MTR
	
	mtr(Mutli function TRracert) 
	
	https://www.linode.com/docs/networking/diagnostics/diagnosing-network-issues-with-mtr
	combines  the  functionality  of the traceroute and ping in a single network diagnostic tool.
	
	# Generating report
	$ mtr google.com 				<= realtime
	$ mtr -rw google.com			<= report <- default 10 report
    $ mtr -rw -c 5 google.com       <= -r report, -w wide report, -c count 5 instead of 10 by default
	
	Start: Thu Dec  1 13:55:32 2016
	HOST: cos7    Loss%   Snt   Last   Avg  Best  Wrst StDev  <= standard deviation of the latencies to each host
	1.|-- gateway  0.0%     5    0.1   0.1   0.1   0.2   0.0
	2.|-- i7       0.0%     5    0.3   0.3   0.3   0.4   0.0

	$ mtr --no-dns -rw -c 5 google.com

	# traceroute
	$ traceroute -n -w 3 -q 1 -N 32 -m 16 google.com
		-n         : Disable DNS lookup to speed up queries.
		-w seconds : Set the time (in seconds) to wait for a response to a probe (default 5.0 sec).
		-q  NUMBER : Sets the number of probe packets per hop. The default is 3.
		-m  max hop:
	
       -o fields order
       --order fields order
              Use this option to specify the fields and their order when loading mtr.
                                                     ┌──┬─────────────────────┐
                                                     │L │ Loss ratio          │
                                                     ├──┼─────────────────────┤
                                                     │D │ Dropped packets     │
                                                     ├──┼─────────────────────┤
                                                     │R │ Received packets    │
                                                     ├──┼─────────────────────┤
                                                     │S │ Sent Packets        │
                                                     ├──┼─────────────────────┤
                                                     │N │ Newest RTT(ms)      │
                                                     ├──┼─────────────────────┤
                                                     │B │ Min/Best RTT(ms)    │
                                                     ├──┼─────────────────────┤
                                                     │A │ Average RTT(ms)     │
                                                     ├──┼─────────────────────┤
                                                     │W │ Max/Worst RTT(ms)   │
                                                     ├──┼─────────────────────┤
                                                     │V │ Standard Deviation  │
                                                     ├──┼─────────────────────┤
                                                     │G │ Geometric Mean      │
                                                     ├──┼─────────────────────┤
                                                     │J │ Current Jitter      │
                                                     ├──┼─────────────────────┤
                                                     │M │ Jitter Mean/Avg.    │
                                                     ├──┼─────────────────────┤
                                                     │X │ Worst Jitter        │
                                                     ├──┼─────────────────────┤
                                                     │I │ Interarrival Jitter │
                                                     └──┴─────────────────────┘
    $ mtr -o "LSD NBAW" 	google.com

	
	* * *
	The firewall to block ICMP packets. As such, you will sometimes see a series of asterisks indicating 
	that trace route was not able to get information from a particular host.
	
	
	# Ping
	Windows Ping TTL is 128
	Redhat	Ping TTL is 64
	Other linux < 128
	
	$ ping -i 4   127.0.0.1    	<= send out ping packets every 4 sec
	$ ping -i 0.1 127.0.0.1    	<= send out ping packets every 0.1 sec
	
	# ping localhost
	$ ping localhost
	$ ping 127.0.0.1
	$ ping 127.0.0.2~254
	$ ping 0.0.0.0
	$ ping 0					<= 
	
	# flood localhost network
	$ ping -f 0
	
	# change the packet size which is by default 64 bytes
	$ ping -s 200   0			<= 0 localhost
	
	# how to specify time out
	
	$ ping -W 10     0			<= 0 localhost 	
	
	# ping every 10sec, ping localhost
	$ ping -i 10 0			<= ping 0(localhost) every 10sec 
	
<<<<<<< Updated upstream
43. stty 		   <= change and print terminal line settings
	$ stty -echo   <= hide(turn off) terminal typing(Can't see typing)
    $ stty echo    <= show(restore) terminal typing(able to see typing)
    $ TTY  		   <= TeleTYpe



=======
	
42. strace - trace system calls and signals	
	$ strace foobar.sh  	<= debugging a command/script   

43. stty 		 <= change and print terminal line settings
	$ stty -echo   <= hide(turn off) terminal typing(Can't see typing)
    $ stty echo    <= show(restore) terminal typing(able to see typing)
    $ TTY  		 <= TeleTYpe


>>>>>>> Stashed changes

43. stat 				<= detailed and statistic on a file (ls -l foo.bar)
	
	$ stat foo.bar 
		File: ‘foo.bar’
		Size: 354             Blocks: 8          IO Block: 4096   regular file
		Device: fc00h/64512d    Inode: 1046543     Links: 1
		Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)
		Access: 2016-04-08 21:16:25.603244615 -0700
		Modify: 2016-04-08 21:16:45.355245237 -0700
		Change: 2016-04-08 21:16:45.355245237 -0700
		Birth: -

<<<<<<< Updated upstream
44. lsmod = Show the status of modules in the Linux Kernel
			same as  "cat /proc/modules"

=======

44. lsmod = Show the status of modules in the Linux Kernel
			same as  "cat /proc/modules"
			
	
	mrmod					 <= rmmod - Simple program to remove a module from the Linux Kernel
	Error msg  "blk_update_request: I/O error, dev fd0, sector 0 errors"
	http://unix.stackexchange.com/questions/282845/blk-update-request-i-o-error-dev-fd0-sector-0
	$ rmmod floppy		
	$ echo "blacklist floppy" | sudo tee /etc/modprobe.d/blacklist-floppy.conf
	
	
	more info:
	fstab(5), 
	findfs(8), 
	mount(8) 
	blkid(8) 

	
	
>>>>>>> Stashed changes

45. Disabling User account
	#1. Change login passwd in /etc/passwd
	apark:x:503:504::/home/apark:/bin/bash
	apark:* or !:503:504::/home/apark:/bin/bash   <= X to '*' or '!'  
		note: will require you to assign a new password to the user if you re-enable the account,
	#2.
	apark:x:503:504::/home/apark:/bin/bash/nologin    <= add /nologin at the end
	apark:x:503:504::/home/apark:/bin/nologin
	apark:x:503:504::/home/apark:/usr/sbin/nologin
	
	e.g. $ ssh test@192.168.232.138
			test@192.168.232.138's password:
			Permission denied, please try again.
	
	#3. using usermod
	$ usermod --lock --expiredate 1970-01-01 <username>
	$ usermod --lock --shell /bin/nologin username
	$ usermod -L     -s      /bin/nologin username
	
	#4. Change Age to '0'
	$ chage -E 0 user_ID                                       <= $chage -E -1 user_ID   
	$ ssh test1@192.168.232.138
	test1@192.168.232.138's password:
	Your account has expired; please contact your system administrator
	Connection closed by 192.168.232.138
	
	#5. Lock the password
	http://unix.stackexchange.com/questions/7690/how-do-i-completely-disable-an-account
	$ passwd -l user_ID			<= -l lock
	
	Doesn't satisfy constraints because public key authentication bypasses PAM and still allows direct login.
	$ usermod -s /sbin/nologin <user>
	
	#####
	Doesn't satisfy constraints because breaks the enterprise scheduler
	$ usermod --lock --expiredate 1970-01-01 <user>  
	$ usermod -l 	 -e 		  1970-01-01 user_ID
	<<< This is our winner. Remote login disabled, yet root can still su <user>, as can 
		other users via sudo so the scheduler functions properly and authorized end users 
		can become the target service account as needed. >>>


45. messaging and anouncement
	#1. Wall 	<= messaging to everybody
	$echo testing > wall.txt
	$wall wall.txt					
		Broadcast Message from noza
			(/dev/pts/1) at 19:46 ...
		testing
	
	$ echo "Please log out" | wall 
		Broadcast Message from noza@fm
			(/dev/pts/1) at 19:47 ...
		Please log out
	#2. write — send a message to another user
		$ write <user> 
		  Some text goes here
		  Ctrl+D (EOF)
		$ echo "Some text goes here" | write <user>
	#3. mesg - Control if (non-root) users can send messages to your terminal.
	#4. talk - Talk with other logged in users.
	
	
### REDHAT ENT Linux Init Systems ###
Date		| 	Init Systems
---------------------------
2002-2010 	|	SysV Init
2010-2013	|	Upstart		
2013-current|	Systemd

		
### SysV Init ### 
	- Introduced runlevel or states
	- Default runlevel determined by /etc/inittab
	- Initialization scripts stored in /etc/init.d
	- Symbolic links to init scripts stored in /etc/rd.d/rc[0-6]
		e.g. rc 3  	<= server
			 rc 5   <= GUI
		- One directory per runlevel
	- Start/Stop and order determined by link name
		
		SysV init service link names
		
### Upstart Features ###
	- Asynchronous service start-up
	- Automatic restart of crashed services
	- event-based starting of services

### Systemd ### 
	- Parallel processing of start-up scripts
	- Service dependencies
	- On-demand service activation using sockets and D-Bus
	- System state snapshots
	- Process tracking using control groups
	- Parallel mounting and checking of file systems
	- Binary optimization of start-up programs
	

A. CentOS 6.5(sysV)
	
/sbin/chkconfig --list

	a.List of all services for system
	  chkconfig --list  | more
	b.Runlevel
	  cat /etc/inittab
	c.check runlevel
	  runlevel
	d.Service scripts
	  ls /etc/init.d/
 	e.symbolic link to init.d directory
	  ls /etc/rc3.d
	  

B. CentOS 7(systemd)
	a.List of all services for system
	  systemctl list-units --type service --all
	b.
	  /etc/systemd.system

	c. same as /etc/init.d
	  /usr/lib/systemd/system

	d.
  
   
  
  
### List processes

	## chkconfig ##					## systemd ##
	
	chkconfig --list				systemctl list-unit-files
	
	chkconfig nginx on				systemctl enable  nginx
	chkconfig nginx off				systemctl disable nginx
	service   nginx start			systemctl start   nginx
	service   nginx stop			systemctl stop 	  nginx
	service   nginx status			systemctl status  nginx
	
	$ systemctl list-unit-files | grep 'influx\|grafana'


CentOS 6.5(sysV)     		   vs     	 CentOS 7 (systemd)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
service httpd status				    systemctl -l(full) status httpd
chkconfig --level 35 httpd  on			systemctl enable/disable httpd.service
chkconfig --list | grep httpd			systemctl start httpd
service httpd start|stop		     	systemctl stop httpd
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
target							systemctl list-unit-files
cat /etc/inittab				systemctl list-units --type target  (--all for not active)					                        
								systemctl list-units --type services
								systemctl get-default
								change to multi-user target
								systemctl set-default multi-uiser.target
								systemctl reboot
								startx <= Windows environment 
								systemctl set-default graphical.target
								systemctl reboot
								systemctl start/stop sshd.service
								systemctl enable/disalbe sshd.service  (constant run)		
								systemctl is-enabled sshd 			 <= check for sshd
								sytemctl daemon-reload          <= Revert httpd.conf orginal file reload 
								systemctl reload httpd			<= httpd service reload

journalctl - Query the systemd journal

	journalctl may be used to query the contents of the systemd(1)
    journal as written by systF$(whoemd-journald.service(8).

	$ journalctl | grep fail
	$ journalctl -b          <= current boot
	$ journalctl -b -1       <= previous boot
	$ journalctl -f          <= follow
	$ journalctl -xe		 <= -x  Augment log lines with explanation texts from the message catalog.
							    -e  jump to the end of the journal 
	
	# Check program logs
	$ journalctl -u nginx 		 <= -u unit
	$ journalctl -u sshd    

	$ journalctl --since "10 min ago"

NignX Status page 
# nginx -V 2>&1| grep -o http_stub_status_module

	

----------------------------------------------------------------
Distribution			|	SystemD	  |	System V  |	 Upstart
----------------------------------------------------------------
Amazon Linux											V
CentOS 6												V
CentOS 7						V
Debian 7 ("Wheezy")							V
Debian 8 ("Jessie")				V 
RHEL 6													V
RHEL 7							V
Ubuntu 14.04 or lower									V
Ubuntu 16.04 or higher			V	
----------------------------------------------------------------		
	
	
46. List of package installed
	$ rpm -qa | grep ^httpd
	
# CentOS 'YUM'
	$ yum list *httpd  											<= online
    $ yum list installed (same as 'rpm -qa') | grep httpd		<= on system 
    $ yum info httpd  											<= version and detail pkg info


47. EPEL (Extra Packages for Enterprise Linux) 
	CentOS 6 x64 EPEL 
	$ rpm -ivh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm

	CentOS7 
	$ sudo yum install epel-release


	
49. echo command
	#!/bin/bash
	now=$(date)			              <= $(date) will print date command
	echo $now			              <= invoke variable now
	echo "Today's date is $(date)."   <= $(date) will print date command
	cal 	 		                  <= display calendar


50.
	
	

51. diff & sdiff
	https://www.lifewire.com/compare-two-text-files-linux-3861434
     
	$ colordiff file1 file2
    $ diff -u file1 file2 | colordiff

	https://stackoverflow.com/questions/16787916/difference-between-two-directories-in-linux
	$ diff -r dir1 dir2 | grep dir1 | awk '{print $4}' > difference1.txt	
	$ diff -rq dir1 dir2		
			-r - Recursively compare any subdirectories found.
			-q - Output only whether files differ.
			
	https://askubuntu.com/questions/421712/comparing-the-contents-of-two-directories
	
	
	# Diff between two remote folders through SSH

Server 1

IP: images.server1.com
User: ubuntu1
Password: pa$$word1
Images Path: /home/www/images/test_images

Server 2

IP: images.server2.com
User: ubuntu2
Password: pa$$word2
Images Path: /var/www/site/images/test_images


-------------------------------------------------------------------------------------------
diff -B <(sshpass -p 'pa$$word1' \
	ssh ubuntu1@images.server1.com "find /home/www/images/test_images -type f\ 
	| sed 's/\/home\/www\/images\/test_images\///g'" | sort -n) < \
	(sshpass -p 'pa$$word2' ssh ubuntu2@images.server2.com \ 
	"find /var/www/site/images/test_images -type f |  \ 
	sed 's/\/var\/www\/site\/images\/test_images\///g'" | sort -n)\ 
	| grep ">" | awk '{print $2}'
-------------------------------------------------------------------------------------------

Explanation:

You can use diff -B <() <() for taking the diff between two streams. The command first uses 
sshpass to ssh into the two servers without having to enter your passwords interactively.

Each parameter for diff -B uses find command to recursively list all your images in the 
specified directory and uses sed to remove the root path of the files (because they are 
different for two servers - and to make it work for the diff command); and the sort command to sort them.

Since the output of the diff command returns either > or <, grep is used to filter out 
only the diffs from your Server 2. Last, awk prints out only the second column (removes the > column from the output).

NOTE: You need to install sshpass first. 
sudo apt-get install sshpass


  csplit   Split a file into context-determined pieces
  cut      Divide a file into several parts


  
  
52. # chkconfig --add <script_name>
	# chkconfig <script_name> on


chkconfig  provides  a  simple  command-line  tool to maintaining the
       /etc/rc[0-6].d directory hierarchy by relieving  system  administrators
       of  the  task  of  directly manipulating the numerous symbolic links in
       those directories.

       This implementation of chkconfig was inspired by the chkconfig  command
       present  in the IRIX operating system. Rather than maintaining configu‐
       ration information outside of the  /etc/rc[0-6].d  hierarchy,  however,
       this  version  directly  manages  the  symlinks in /etc/rc[0-6].d. This
       leaves all of the configuration  information  regarding  what  services
       init starts in a single location.

       chkconfig  has five distinct functions: adding new services for manage‐
       ment, removing services from management, listing  the  current  startup
       information  for  services,  changing  the startup information for ser‐
       vices, and checking the startup state of a particular service.

       When chkconfig is run with only a service name, it checks to see if the
       service  is configured to be started in the current runlevel. If it is,
       chkconfig returns true; otherwise it returns false. The --level  option
       may be used to have chkconfig query an alternative runlevel rather than
       the current one.

       When chkconfig is run with the --list argument, or no arguments at all,
       a listing is displayed of all services and their current configuration.

       If  one  of  on,  off, reset, or reset priorities is specified after the
       service name, chkconfig changes the start-up information for the  specified  
	   service.  The on and off flags cause the service to be started or
       stopped, respectively, in the runlevels being changed. The  reset  flag
       resets  the  on/off state for all runlevels for the service to whatever
       is specified in the init script in question, while the  reset priorities
       flag  resets  the  start/stop priorities for the service to whatever is
       specified in the init script.

       By default, the on and off options affect only runlevels 2, 3,  4,  and
       5,  while  reset and reset priorities affects all of the runlevels.  The
       --level option may be used to specify which runlevels are affected.

       Note that for every service, each runlevel has either a start script or
       a  stop  script.   When  switching runlevels, init will not re-start an
       already-started service, and will not re-stop a  service  that  is  not
       running.

       chkconfig also can manage xinetd scripts via the means of xinetd.d con‐
       figuration files. Note that only the on, off, and --list  commands  are
       supported for xinetd.d services.

       chkconfig  supports  a  --type argument to limit actions to only a spe‐
       cific type of services, in the case where services of either  type  may
       share a name. Possible values for type are sysv and xinetd.

53. Nagios
	systemctl start nagios.service
	systemctl start httpd.service
	systemctl enable httpd.service		<= Apache in startup
	ln -s '/usr/lib/systemd/system/httpd.service'  '/etc/systemd/system/multi-user.target.wants/httpd.service'

54. List of groups
	
	# Check who owns folder?
	$ stat -c %U /var/www/html			<= %U User
		www-data
	$ stat -c %G /var/www/html			<= %G Group
		www-data	
	
	# groups
	$ getent - get entries from Name Service Switch libraries

	$ cat /etc/group  <= Group List
 	$ getent group | cut -d: -f1
	
	# Who(users list) is in the group
	$ getent group www-data
		www-data:x:33:
	
	# Add an User to Group
	$ sudo usermod -G www-data -a apark
	
	$ getent group www-data
		www-data:x:33:apark  <= apark added
	
	
	### gpasswd
	
	$  adduser apark
    $  gpasswd -a apark wheel
    $  gpasswd -a apark root
    $  vi /etc/sudoers
	
		The term "WHELL" is derived from the slang phrase big wheel, 
		referring to a person with great power or influence.
	
	
55. SElinux <= add port to security-enhanced linux 
	$ cat /etc/sysconfig/selinux

	semange port -a -t http_port_t -p tcp 10051  

	### set SELINUX Permissive mode
	$ getenforce  <= CHECK
    $ setenforce permissive
	$ sed -i 's\=enforcing\=permissive\g' /etc/sysconfig/selinux
	# To Check
	$ getenforce
	Permissive
  
  
  
  
  
56. Region and Time NTP

#### Region - TIME ZONE ####

  Ubuntu
  sudo dpkg-reconfigure tzdata
  sudo vi /etc/timezone	<= change to **America/Los_Angeles or America/New_York


#### NTP - TIME ZONE ####

	# CenOS 7.x #

	$ yum install ntp
	$ systemctl start ntpd
	$ systemctl enable ntpd

	$ ls -l /etc/localtime 
	$ timedatectl list-timezones
	$ timedatectl set-timezone America/Los_Angeles


	# CentOS 6.x # 
	America/Tijuana

	$ ntpdate pool.ntp.org  		<=0~3.us.pool.ntp.org 
	$ chkconfig ntpd on				<= put in into startup
	$ /etc/init.d/ntpd start

	# Ubuntu 14.04(Trusty) #
	$ apt-get install ntp
	$ ntpq -p  							<=-p peer list of ntp servers  (ntpq=std NTP query program)
	$ vi /etc/ntp.conf					<= replace with **US** 0~3.us.pool.ntp.org 
		########################
		server 0.us.pool.ntp.org
		server 1.us.pool.ntp.org
		server 2.us.pool.ntp.org
		server 3.us.pool.ntp.org
		########################
	$ sudo service ntp restart 

	# To stop ntpd #
	$ sudo /etc/init.d/ntp stop
	$ sudo service ntp stop

	# To prevent it from starting at boot:
	$ sudo update-rc.d -f ntp remove

# Update NTP time Ubuntu
	$ sudo ntpdate pool.ntp.org
	4 May 17:34:07 ntpdate[38018]: step time server 204.2.134.163 offset 53.202434 sec

	$ sudo ntpdate pool.ntp.org  <=  **** no servers can be used, exiting ***
		31 Aug 19:05:55 ntpdate[8911]: the NTP socket is in use, exiting
	$ sudo service ntp stop
		[ ok ] Stopping NTP server: ntpd.
	$ sudo ntpdate pool.ntp.org
		31 Aug 19:07:11 ntpdate[10355]: adjust time server 46.29.176.115 offset -0.002893 sec
	$ sudo service ntp start

	
	
57. CentOS6.x MySQL auto start and start MySQL services
	$ chkconfig mysqld on
	$ /etc/init.d/mysqld start


	
	
### Ubuntu 16.04 Iptables setup ###
	http://linux-sys-adm.com/ubuntu-16.04-lts-how-to-configure-firewall-iptables-fail2ban/
	https://oitibs.com/easy-ubuntu-16-server-firewall/
	http://dev-notes.eu/2016/08/persistent-iptables-rules-in-ubuntu-16-04-xenial-xerus/
	
	# Install IPTables Persistent Package 
	$ apt-get install -y iptables-persistent

	# Add netfilter-persistent Start-up
	$ invoke-rc.d netfilter-persistent save		<= enable for start up
	
	$ service netfilter-persistent stop

	# Start netfilter-persistent Service		<=	### After Edit IPtables ##
	
	$ service netfilter-persistent     start | stop | status
	$ /etc/init.d/netfilter-persistent start | stop | status
	
	$ iptables -nL			<= List  all rules in the selected chain. '-n' NO DNS lookup (Faster)
	$ iptables -S			<= Print all rules in the selected chain. 	
	
	### Restore
	sudo iptables-restore < ~/serenity-iptables-rules/ruleset-v4
	sudo ip6tables-restore < ~/serenity-iptables-rules/ruleset-v6

	### Preroute rediect
	$ sudo iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3000	
	

	
	### CentOS7 ### 

Iptables opens all	
$ iptables -I INPUT -j ACCEPT
	
	$ sudo iptables -L
	
	$ sudo yum -y install iptables-services
	$ sudo systemctl enable iptables 		
	$ sudo systemctl mask firewalld			
	$ sudo systemctl stop firewalld

	$ vi /etc/sysconfig/iptables
	$ sudo systemctl start iptables
	
	Writing a Simple Rule Set( https://wiki.centos.org/HowTos/Network/IPTables)

IMPORTANT: At this point we are going to clear the default rule set. If you are connecting 
remotely to a server via SSH for this tutorial then there is a very real possibility that 
you could lock yourself out of your machine. You must set the default input policy to 
accept before flushing the current rules, and then add a rule at the start to explicitly 
allow yourself access to prevent against locking yourself out.
We will use an example based approach to examine the various iptables commands. In this 
first example, we will create a very simple set of rules to set up a Stateful Packet 
Inspection (SPI) firewall that will allow all outgoing connections but block all 
unwanted incoming connections:


# iptables -P INPUT ACCEPT
# iptables -F
# iptables -A INPUT -i lo -j ACCEPT
# iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
# iptables -A INPUT -p tcp --dport 22 -j ACCEPT
# iptables -P INPUT DROP
# iptables -P FORWARD DROP
# iptables -P OUTPUT ACCEPT
# iptables -L -v
which should give the following output:


Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 ACCEPT     all  --  lo     any     anywhere             anywhere
    0     0 ACCEPT     all  --  any    any     anywhere             anywhere            state RELATED,ESTABLISHED
    0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere            tcp dpt:ssh
Chain FORWARD (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
Now lets look at each of the 8 commands above in turn and understand exactly what we've just done:

iptables -P INPUT ACCEPT 
	If connecting remotely we must first temporarily set the default policy on the INPUT chain to ACCEPT 
	otherwise once we flush the current rules we will be locked out of our server.
iptables -F 
	We used the -F switch to flush all existing rules so we start with a clean state from which to add new rules.
iptables -A INPUT -i lo -j ACCEPT 
	Now it's time to start adding some rules. We use the -A switch to append (or add) a rule to a specific chain, 
	the INPUT chain in this instance. Then we use the -i switch (for interface) to specify packets matching or 
	destined for the lo (localhost, 127.0.0.1) interface and finally -j (jump) to the target action for packets 
	matching the rule - in this case ACCEPT. So this rule will allow all incoming packets destined for the 
	localhost interface to be accepted. This is generally required as many software applications expect to be 
	able to communicate with the localhost adaptor.
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT 
	This is the rule that does most of the work, and again we are adding (-A) it to the INPUT chain. Here we're 
	using the -m switch to load a module (state). The state module is able to examine the state of a packet 
	and determine if it is NEW, ESTABLISHED or RELATED. NEW refers to incoming packets that are new incoming 
	connections that weren't initiated by the host system. ESTABLISHED and RELATED refers to incoming packets 
	that are part of an already established connection or related to and already established connection.
iptables -A INPUT -p tcp --dport 22 -j ACCEPT 
	Here we add a rule allowing SSH connections over tcp port 22. This is to prevent accidental lockouts when 
	working on remote systems over an SSH connection. We will explain this rule in more detail later.
iptables -P INPUT DROP 
	The -P switch sets the default policy on the specified chain. So now we can set the default policy on the 
	INPUT chain to DROP. This means that if an incoming packet does not match one of the following rules it 
	will be dropped. If we were connecting remotely via SSH and had not added the rule above, we would have 
	just locked ourself out of the system at this point.
iptables -P FORWARD DROP 
	Similarly, here we've set the default policy on the FORWARD chain to DROP as we're not using our computer 
	as a router so there should not be any packets passing through our computer.
iptables -P OUTPUT ACCEPT 
	and finally, we've set the default policy on the OUTPUT chain to ACCEPT as we want to allow all outgoing 
	traffic (as we trust our users).
iptables -L -v 	
	Finally, we can list (-L) the rules we've just added to check they've been loaded correctly.


### -j REJECT --reject-with icmp-host-prohibited
	https://unix.stackexchange.com/questions/124624/what-a-input-j-reject-reject-with-icmp-host-prohibited-iptables-line-does-ex
	

	
### Changing data directory on Centos 7 + MySQL 5.6	
	$ wget http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm
	$ yum localinstall mysql57-community-release-el7-7.noarch.rpm
	
	
	$ systemctl mysqld stop
	$ cp -rap /var/lib/mysql/ /target_dir/mysql/
	$ chown mysql:mysql /target_dir/mysql
	
	# edit /etc/my.conf to redirect datadir & socket to new path
	$ vi /etc/my.conf 
	# change to new data, socket & client connection location
	datadir=/target_dir/mysql
	socket=/target_dir/mysql/mysql.sock
	[client]
	socket=/target_dir/mysql/mysql.sock
	
	
	# manage SELinux => install semanage (In Centos 7 minimal is not installed)
	$ getenforce
	$ yum provides /usr/sbin/semanage
	$ yum -y install policycoreutils-python


	$ getenforce
	$ semanage fcontext -a -t mysqld_db_t "/new_dir/mysql(/.*)?"
	$ restorecon -Rv /new_dir/mysql
	$ systemctl start mysqld
	
	# Check SELinux reports any error on MySQL
	$ grep mysqld /var/log/audit/audit.log

	
	
58. # CentOS 6 Turn off firewall and SELINUX

	chkconfig iptables off && chkconfig ip6tables off && 
	service iptables stop &&  service ip6tables stop && setenforce 0
	$ vi /etc/sysconfig/iptables
	$ iptables -L		<= List
	$ iptables -L -n   <= NO DNS lookup (Faster)
	$ iptables -nL
	$ iptables -S
  
  
    
  ### Ubuntu 14.04 IPtables setup ###
	$ sudo apt-get update
	$ sudo apt-get install iptables-persistent
	$ sudo vi /etc/iptables/rules.v4 
  	
	$ /etc/init.d/iptables-persistent {start|restart|reload|force-reload| save|flush}
	$ service iptables-persistent start | stop | restart | reload
  
  ### Check error restore test ###
	$ iptables-restore -vv < /etc/iptables/rules.v4

	
	
  ### Ubuntu 16.04 Iptables setup ###
	http://linux-sys-adm.com/ubuntu-16.04-lts-how-to-configure-firewall-iptables-fail2ban/
	https://oitibs.com/easy-ubuntu-16-server-firewall/
	
	# Install IPTables Persistent Package 
	$ apt-get install -y iptables-persistent

	# Add netfilter-persistent Startup
	$ invoke-rc.d netfilter-persistent save
	
	$ service netfilter-persistent stop | start
	
	### Edit IPtables ##
	
	
	$ service netfilter-persistent enable
	$ service netfilter-persistent start
	$ service netfilter-persistent status
	
	$ iptables -nL







# Security Selinux
vi /etc/selinux/config #replace SELINUX=enforcing with SELINUX=disabled

58. Bash History with time stamp
	###										###	
	#  $ export HISTTIMEFORMAT='%F '          #		<= %F Equivalent to %Y - %m - %d
	#  $ export HISTTIMEFORMAT='%F %T  ' 	  #		<= %F Equivalent to %Y - %m - %d,  %T Replaced by the time ( %H : %M : %S )
	** Check the export list	
	$ export -p | grep HIST*
	$ env | sort | grep HIST


59. Prompt Color red

	Permanently add Green color scheme to .bashrc file
	######															  ####
	# echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'" >> ~/.bashrc #   <= GREEN Color
	######															  ####

	### Temp current shell
	export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]' 		 >> ~/.bashrc		<= Green 
	export PS1='\[\e[1;31m\][\u@\h \W]\$\[\e[0m\]' 		 >> ~/.bashrc		<= Red  
    export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"  	 >> ~/.bashrc		<= Green|Red 
    export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[1;32m\w$"  	 >> ~/.bashrc		<= Green|Red 
    
	### Permanent
	echo "export PS1='\[\e[1;32m\][\u@\h \W]\$\[\e[0m\]'" 	 >> ~/.bashrc		<= Green 
    echo 'export PS1="\e[1;32m\u\e[0m@\e[1;31m\h\e[0m\w$"'   >> ~/.bashrc		<= Green|Red 

								
	


	
59. network monitoring and troubleshooting
	
	# iftop
	$ iftop -n -i eth1        <= -n no name lookup -i interface eth1
		### batch mode ###
		$ iftop -t 					<= t text mode 
		$ iftop -t > iftop.log		<= save to iftop.log file 
		$ iftop -t -s 60 > log.txt	  <= 60 sec  
		$ iftop -t -s 60 > log.txt &  <= running in background
	
	# vnstat	 					<= a console-based network traffic monitor
		$ vnstat -l					<= real time network statistic
		$ vnstat --testkernel		<= check the kernel is providing all the information
		$ vnstat -d					<= -h hour, -d day, -w week, -w month,
		$ vnstat --dumpdb			<= export to Excel or 
	
	# traceroute TTL(time-to-live) value, also known as hop limit, option
	$ traceroute -w 3 -q 1 -m 16 example.com

	# MTR
		MTR - MTR represents an evolution of the traceroute command by providing a greater data sample, as if augmenting 
		traceroute with ping output. This document provides an in depth overview of MTR, the data it generates, and how 
		to properly interpret and draw conclusions based on the data provided by it.
	
	### GUI tool
	$ iptraf
	
	
	$ iperf 	<= is a tool for active measurements of the maximum achievable bandwidth on IP networks.
	https://iperf.fr/
	
	$ vnstat
	http://www.happyapps.io/blog/2015-08-15-track-and-log-a-linux-server-s-bandwidth-use

	$ nload
	
	$ nethogs
	
	
	
	
	
	
### netsniff-ng 								<- the packet sniffing beast
	Its gain of performance is reached by zero-copy mechanisms, so that on packet reception and transmission 
	the kernel does not need to copy packets from kernel space to user space and vice versa.
	toolkit can be used for network development and analysis, debugging, auditing or network reconnaissance.

	$ apt-get|yum install netsniff-ng 					
	http://netsniff-ng.org/
		
	For geographical AS TCP SYN probe trace route to a website:
	$ astraceroute -d eth0 -N -S -H netsniff-ng.org			<- autonomous system trace route utility

	For kernel networking statistics within promiscuous mode:
	$ ifpps -d eth0 -p						<-fetch and format kernel network statistics

	For high-speed network packet traffic generation, trafgen.txf is the packet configuration:
	$ trafgen -d eth0 -c trafgen.txf

	For compiling a Berkeley Packet Filter fubar.bpf:
	$ bpfc fubar.bpf

	For live-tracking of current TCP connections (including protocol, application name, city and country of source and destination):
	$ flowtop

	For efficiently dumping network traffic in a pcap file:
	$ netsniff-ng -i eth0 -o dump.pcap -s -b 0	
	
	
###	tcpdump
	http://www.rationallyparanoid.com/articles/tcpdump.html
	
	$ tcpdump -i eth1 -n tcp port 10050
	####################################
	# tcpdump -i any -n tcp port 10050 #    <= zabbix agent port
	####################################
	-i = listening interface
	-n = not to convert addresses

	### Capture Ping - icmp message ##
	$ tcpdump -n icmp
	$ tcpdump -v -n icmp
	$ tcpdump -n icmp and 'icmp[0] != 8 and icmp[0] != 0'
	$ tcpdump -n icmp and icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply

	### Capture any packets where the source host is 192.168.1.1. Display IP addresses and port numbers:
	http://www.rationallyparanoid.com/articles/tcpdump.html
	
	$ tcpdump 		  -n src host 192.168.1.1
	$ tcpdump -i eth1 -n src host 192.168.1.1
	
	### Exclude port 22 https://danielmiessler.com/study/tcpdump/
	
	$ tcpdump -i any -n tcp port not 22 and host 1.2.3.4
		
  
###	NetCat
	###################################
	#  nc -v 192.168.110.220  22 	  #  <= netcat  -v verbose / -z listening daemon
	###################################   

    Connection to 192.168.110.220 10051 port [tcp/zabbix-trapper] succeeded!
	
	nc -l 22 							<= listening mode
  
  https://www.digitalocean.com/community/tutorials/how-to-use-netcat-to-establish-and-test-tcp-and-udp-connections-on-a-vps
  
	Ping and capture IP only
	$ ping -c 1 www.google.com | gawk -F'[()]' '/PING/{print $2}'
  

### Domain Lookup
http://www.beetlebrow.co.uk/what-do-you-need/help-and-documentation/unix-tricks-and-information/dns-checking-with-whois-nslookup-and-dig
https://securityonline.info/tutorial-nslookuphostdigwhois-dns-information-gathering/

	# Simple to get theIP
	$ ping			<= get IP
	$ host 			
	
	# DNS Detail info
	$ nslookup
    $ dig 
	$ whois  domainName.com
	
    $ yum install bind-utils
    $ host $HOSTNAME | xargs | cut -f4 -d' '
	
	$ dig +short +trace www.name.com
	
	


  
  
61. telnet     ip_address    port_number  <= check port is open for connection
	$ telnet ip_address 22
	
	to exit ' ctrl + ] '  or ' ^] ' then type "close"



	
62. Ubuntu Package Clean ### Package Clean ###

# Installed package search
  
  apt-cache search mysql

  sudo apt-get update

# Backing ups:
  sudo cp /etc/apt/sources.list /etc/apt/sources.list.original
  sudo cp /var/lib/dpkg/status /var/lib/dpkg/status.original
  sudo cp -r --parents home/aws/* /home/apark/awn/   #--recursive

  sudo apt-get clean 
  sudo apt-get autoclean 


  # Fixing dependencies using apt's fix-broken mode
	sudo apt-get -f install
	
 E:Internal Error when using apt-get	
	sudo apt-get update
	sudo apt-get clean
	sudo apt-get cehck                       <= 
	sudo apt-get install -fy
	sudo dpkg -i /var/cache/apt/archives/*.deb
	sudo dpkg --configure -a
	sudo apt-get install -fy
	sudo apt-get dist-upgrade
	
####	Broken package fix ### 
sudo dpkg --configure -a
sudo apt-get install -f

$ sudo vi /var/lib/dpkg/status 			<= remove whole trouble block
$ cat /etc/apt/sources.list

#####
sudo sh -c "apt-get update;apt-get dist-upgrade;apt-get autoremove;apt-get autoclean"  
####
sudo sh -c "apt-get update;apt-get autoclean;apt-get clean;apt-get autoremove"
####
sudo dpkg --remove -force --force-remove-reinstreq package_name
####



### Clean APT Key

$ apt-key list
	pub   4096R/563278F6 2016-04-08 [expires: 2018-04-08]
	uid                  Foreman Automatic Signing Key (2016) <packages@theforeman.org>

$ apt-key del 563278F6  <= key_name

# how to remove -file
To remove a file whose name starts with a '-', for example '-foo',

$ rm --help
use one of these commands:
	rm -- -foo
	rm ./-foo


  

63. How do I fix a “Problem with MergeList” or “status file could not be parsed” error when trying to do an update?


sudo rm /var/lib/apt/lists/* -vf
sudo apt-get update



64.  AWS AMI MYSQL setup root pw issue 

1. apt-get/yum install mysql-server
2. /usr/bin/mysql_secure_installation
3. service mysqld stop 
4. rm -rf /var/lib/mysql/*
5. service mysqld start  <= recreate tables 


6. /usr/bin/mysqladmin -u root password 'Your-New-Passwd'  <= Your mysql root's pw

	mysql remote host login
	$ mysql -u webadmin –h 65.55.55.2 –p
	$ mysql -ID_admin -p'password'  -hlocalhost

	### MySQL remote login with different port ###
	$ mysql -uroot -p`cat ~/.mysql_shadow` -h localhost -P 7506

	
	
	
	
65. Swappiness

	Swappiness is a Linux kernel parameter that controls the relative weight given to 
	swapping out runtime memory, as opposed to dropping pages from the system page cache. 
	Swappiness can be set to values between 0 and 100 inclusive. A low value causes the 
	kernel to avoid swapping, a higher value causes the kernel to try to use swap space. 
	The default value is 60, and for most desktop systems, setting it to 100 may affect 
	the overall performance, whereas setting it lower (even 0) may decrease response latency.

Value	Strategy
vm.swappiness = 0	The kernel will swap only to avoid an out of memory condition.
vm.swappiness = 60	The default value.
vm.swappiness = 100	The kernel will swap aggressively.
To temporarily set the swappiness in Linux, write the desired value (e.g. 10) to /proc/sys/vm/swappiness 
using the following command, running as root user:


	# Out of meory
	$ grep oom /var/log/*
	$ grep total_vm /var/log/*

	http://unix.stackexchange.com/questions/128642/debug-out-of-memory-with-var-log-messages
	http://unix.stackexchange.com/questions/92525/can-linux-run-out-of-ram/92544#92544
	
	$ grep oom /var/log/*
	/var/log/messages:Jan  5 18:41:12 app-03 kernel: httpd invoked oom-killer: gfp_mask=0x200da, order=0, oom_score_adj=0
	/var/log/messages:Jan  5 18:41:14 app-03 kernel: [<ffffffff8116d24e>] oom_kill_process+0x24e/0x3b0
	/var/log/messages:Jan  5 18:41:17 app-03 kernel: [ pid ]   uid  tgid total_vm  rss nr_ptes swapents oom_score_adj name
	
	pid 			<= The process ID.
	uid 			<= User ID.
	tgid 			<= Thread group ID.
	total_vm 		<= Virtual memory use (in 4 kB pages)
	rss 			<= Resident memory use (in 4 kB pages)
	nr_ptes			<= Page table entries
	swapents 		<= Swap entries
	oom_score_adj 	<= Usually 0; a lower number indicates the process will be less likely to die when the OOM killer is invoked.
	
	
	
# Set the swappiness value as root
  $ echo 10 > /proc/sys/vm/swappiness
 
# Alternatively, run this 
  $ sysctl -w vm.swappiness=10   # sysctl - configure kernel parameters at runtime
 
# Verify the change
  $ cat /proc/sys/vm/swappiness
  10

 
# Alternatively, verify the change
  sysctl vm.swappiness
  vm.swappiness = 10
  Permanent changes are made in /etc/sysctl.conf via the following configuration line     
  inserted, if not present):
  vm.swappiness = 10

  
  
  
66. rpm package 
	### install 
    $ rpm -ivh foo-xxx.rpm <= install
    $ rpm -Uvh foo-xxx.rpm <= upgrade

    ### Remove
	$ rpm -qa | grep -i webmin
	$ rpm -e <package name>

	### Search installed sw
	$ rpm -qa | grep mysql
	$ /sbin/ldconfig -v | grep mysql

  
67. nmap  <= Short for Network Mapper 
    It is an open source security tool for network exploration, security scanning and     
	auditing. However, nmap command comes with lots of options that can make the utility     
	more robust and difficult to follow for new users.
	
	$ nmap -sT ip_address  ( s-scan, T-TCP)
	$
	nmap -v -A scanme.nmap.org

	$ nmap -T4 -A -v localhost
		-T<0-5>: Set timing template (higher is faster)
		-A: OS & version detection, script scanning, and traceroute
		-v: Increase verbosity level (use -vvvv)

	$ nmap -v -sn 192.168.0.0/16 10.0.0.0/8
		-sn: Ping Scan - disable port scan
	
	$ nmap -v -iR 10000 -Pn -p 80
		-iR <num hosts>: Choose random targets
		-Pn: Treat all hosts as online -- skip host discovery
		-p <port ranges>: Only scan specified ports

	
	
	
68. pgrep, pkill - look up or signal processes based on name and other attributes


69. rthunter(Root Kit Hunter)


70. 
	
		
# hanging terminal during the SSH connection.	
http://askubuntu.com/questions/344863/ssh-new-connection-begins-to-hang-not-reject-or-terminate-after-a-day-or-so-on	
A problem can arise when you are trying to connect from behind a NAT router using OpenSSH. 
During session setup, after the password has been given, OpenSSH sets the TOS (type of service) 
field in the IP datagram. Some routers are known to choke on this. The effect is that your 
session hangs indefinitely after you gave your password. Here is the example output from such 
an ssh session:		
$ ssh -vvv id@x.com	
$ ssh -o 		"ProxyCommand nc %h %p" id@x.com
	  ^option	
		

		
71.  Transfer file using SCP + pem key

	$ scp -i /path/to/key.pem     /path/to/file.jpg    user_id@ip:/home/id/folder
	
	## -i Identity_file such as pem key
	$ scp -i /AWS-OC-Key/oc-prod.pem /AWS-OC-Key/oc-sDATEe.pem ec2-user@ip_address:/home/ec2-user/.cert

	### Local ===> Remote 
	$ scp -pv file.zip noza@192.168.221.128:/home/noza/download
	$ scp -r /home/id/.ssh  id@ip:/home/id/			<= -r recursive for sub directories
	$ scp -i .ssh/oc.pem  ami.zip ubuntu@10.10.1.89:/home/ubuntu/

	### install unzip
	$ unzip file.zip -d /tmp/name			<= -d destin directory
    
	### Make a directory remotely
	$ ssh apark@remote_ip "mkdir /path/to/dir"


	### Remote ===> Local 
	$ scp -r id@ip:/home/id/.ssh    /home/id/.ssh
	

	
72. Combine Xargs with Find command 
	xargs -build and 'execute command lines' from standard input

	$ ls
	one.c  one.h  two.c  two.h

	$ find . -name "*.c" | xargs rm -rf

	$ ls
	one.h  two.h

	Use fgrep in order to match for plain ., or escape the dots.

	$ find . -type f | xargs fgrep '...'   <= or if you still want to use grep :
	$ find . -type f | xargs grep '\.\.\.' <= And if you only want the current directory 	and not its subdirs :
	$ find . -maxdepth 1 -type f | xargs fgrep '...'
	
	Finding directory files with word
	$ grep -Ril "finding_word" /target/dir  <= R-recursive, i-ignore case, l-show file name
	
	

	

73. 
	iptables -L
	iptables -L -n
	iptables -nvL

	Ubuntu	iptables-restore < /etc/iptables/rules.v4
	
	CentOS	iptables-restore < /etc/sysconfig/iptables

	iptables-restore and ip6tables-restore are used to restore
	IP and IPv6 Tables from data specified on STDIN.  Use  I/O
        redirection(> <) provided by your shell to read from a file
	(< /etc/iptables/rules.v4)

	



75. Error 
	1>&2    <= STD output should go to the same place as STD error is going.

	
76. Backtick
	Command within backticks is evaluated (executed) by the shell before the main command (like chown in your examples), 
	and the output of that execution is used by that command, just as if you'd type that output at that place in the command line.

	$ sudo chown `id -u` /somedir
	$ sudo chown 1000 /somedir



77. Hostname 
	$ hostname
	$ hostname -f			<= full
	$ hostname -s 			<= short
	
	
	Hostname Change
	# CentOS 6
	https://support.rackspace.com/how-to/centos-hostname-change/
	
	$ vi /etc/sysconfig/network
	$ vi /etc/hosts
	$ vi /etc/hostname
	$ /etc/init.d/netwrok restart
	# or REBOOT
	
	
78. DNS lookup utility
https://nilminus.wordpress.com/penetration-testing/enumeration/dns/using-nslookup-dig-and-host/
https://www.quora.com/Bash-shell-What-are-the-differences-between-host-dig-and-nslookup-and-when-should-I-use-each

	$ nslookup	<= was the first tool for querying the DNS

	$ dig		<= probing the DNS
	
	$ host 		<= Best for BASH scripting
					output has one value per line and is easily scriptable using awk to extract IP
	$ host google.com  
	
	google.com has address 216.58.194.174
	google.com has IPv6 address 2607:f8b0:4005:804::200e
	google.com mail is handled by 40 alt3.aspmx.l.google.com.
	...


Ubuntu
/etc/hosts
/etc/hostname


CentOS6.x file location =>  
	$ cat /etc/hostname 
	$ hostname 
	$ echo new_name > /etc/hostname 
	
	$ systemctl restart systemd-hostnamed
	
	$ cat /etc/sysconfig/network
	
	$ hostnamectl status
	
	
###########################################
#	$ hostnamectl set-hostname new_name   # 
###########################################
	
	
	$ timedatectl status 
	$ timedatectl list-timezones
	$ timedatectl set-timezone UTC
	$ timedatectl set-timezone America/New_York
	$ timedatectl set-timezone America/Los_Angeles
	
	
	Ubuntu timezone change
	$ sudo dpkg-reconfigure tzdata	
	
	
	
	
78. CenOS7 eno****** to eth0

	Step1#
	$sudo vi etc/sysconfig/grub	<= add "net.ifnames=0 biosdevname=0"
GRUB_CMDLINE_LINUX="rd.lvm.lv=centos/swap vconsole.font=latarcyrheb-sun16 rd.lvm.lv=centos/root \
					crashkernel=auto  vconsole.keymap=us rhgb quiet net.ifnames=0 biosdevname=0"

	Step2#
	$sudo grub2-mkconfig  -o /boot/grub2/grub.cfg
	<= Using “grub2-mkconfig” command to re-generate a new grub configuration file

	Step3#
	$sudo mv /etc/sysconfig/network-scripts/ifcfg-eno16777736  /etc/sysconfig/network-scripts/ifcfg-eth0
		<= Rename “Eno” network file using”mv”command


	Step4#$
	su vi /etc/sysconfig/network-scripts/ifcfg-eth0 <= configuration file and set 
							the value of “Name” field to “eth0".
	<strong>NAME=eth0</strong>

	Step5# 
	reboot system, after rebooting system, using “ifconfig” command check 
	network interface information again.
	
	


80.  Ubuntu apt-get update fail some of packages.
	W:Failed to fetch bzip2:
     	E:Some index files failed to download. 

	sudo rm -rf /var/lib/apt/lists/*
	sudo apt-get update



81. Check Kernel Version

	$ ls -al /etc/ld.so.conf.d

	kernel-2.6.32-431.11.2.el6.x86_64.conf
	
	$ uname -a				<= all info 
	
	$ uname -r 
	2.6.32-431.1.2.0.1.el6.x86_64
	
	
	# Result is all same	
	$ cat /boot/config-$(uname -r)  
	$ cat /boot/config-`uname -r`
	$ cat /boot/config-4.2.0-27-generic 
	

	# Ubuntu
    $ sudo dpkg -l | grep linux-headers | grep ii
    $ sudo dpkg -l | grep linux-headers | grep ii | awk '{print $3}'
	

82. rsync
	https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories-on-a-vps
	-v  --verbose
	-q  --quiet
	-a  --archive
	-p	--perm			<= preserver permissions
	-n  --dry-run
	-z  --compress
	
	--size-only
              This  modifies  rsync’s "quick check"
	
### LOCAL >>>> REMOTE ###  **PUSH to Remote**
	###			LOCAL file location  >>>>   REMOTE 'creating' remote folder
	rsync -avn  /home/apark/rtest  			apark@45.55.5.69:/home/apark/      
			<= -a, -v verbose -n dry-run
	rsync -av   ~/rtest          	 		apark@45.55.5.69:/home/apark/   
		
	###			Local files	  		 >>>>	Remote 'Copying' FILES in the remote folder 
	rsync -av   /home/apark/rtest/ 	 		apark@45.55.5.69:/home/apark/         <= copy to apark    
	rsync -av   ~/rtest/          	 		apark@45.55.5.69:/home/apark/rtest/   <= copy to rtest

	
### REMOTE >>>> LOCAL ###   **PULL to Local**
	###			REMOTE file & folder location	>>>>    LOCAL location		
	rsync -av   apark@45.55.5.69:/home/apark/			/home/apark/ 		   <= copy to apark    
	rsync -av   apark@45.55.5.69:/home/apark/rtest/ 	~/rtest/       	       <= copy to rtest
	rsync -av   apark@45.55.5.69:/home/apark/dir1    	~					   <= creating 'dir1' home folder
	rsync -av   apark@45.55.5.69:/home/apark/dir1    	~/					   <= home folder
	receiving incremental file list
	



### REMOTE  ===>>>>  LOCAL			  <Remote location>						<Local location>
	$ rsync -azP  --exlude=dir* 	     apark@45.55.5.69:/home/apark/     	 	~   	
	$ rsync -azP  --exclude={dir1,dir2}  apark@45.55.5.69:/home/apark/       	~/		
	
	
	$ rsync -avzp ssh  --progress jenkins@10.100.200.41:/Users/Shared/Jenkins/Home/jobs/Chroma* 	/tmp
	$ rsync -avzp --progress jenkins@10.100.200.41:/Users/Shared/Jenkins/Home/jobs/Chroma* 	/tmp
	
	
	
	Will it work????
	$ rsync -avzp --progress --exclude={Perforce*,Library*,2015*,Mono*,Out*,Back*,Vill*,Strange*} jenkins@10.100.200.41:/Users/Shared/Jenkins/* 	/Users/Shared/Jenkins/
	
rsync -avzp --progress --exclude={Perforce*,Library*,2015*,Mono*,Out*,Back*,Vill*,Strange*,\
Install*,*.dmg,backup_log,tmp*,test*} jenkins@10.100.200.41:/Users/Shared/Jenkins/*  /Users/Shared/Jenkins/



	
	
### CAN NOT do REMOTE >>>> 2 REMOTES ###
		Server A  <-Rsync-> XX Server B XX <-Rsync-> Server C
		http://unix.stackexchange.com/questions/183504/how-to-rsync-files-between-two-remotes
		Cannot use rsync with a remote source and a remote destination. 
		Assuming the two servers can't talk directly to each other, 
			it is possible to use "ssh to tunnel" via your local machine.

	$ ssh -R localhost:50000:host2:22   host1 'rsync -e "ssh -p 50000" -var /var/www localhost:/var/www'
	
	
	# Options	
	rsync -avz	source 		destination		<= -z Compression
	rsync -avzP	source 		destination	 	<= -P Progress

	Copy a File from a Remote Server to a Local Server with SSH
	To specify a protocol with rsync you need to give “-e” option with protocol name you want to use. 
	Here in this example, We will be using “ssh” with “-e” option and perform data transfer.
	$ rsync -avzhe ssh root@192.168.0.100:/root/install.log     /tmp/	
	
	
### Delete Destination folder has different than Source folder
	$ rsync -az -e --delete ssh /cdn/data/  apark@10.128.34.80:/cdn/data/
		
		# -e option is used to run a different remote shell
		http://unix.stackexchange.com/questions/232041/rsync-permission-denied-13-while-executing-rsync-as-a-root
	
		--delete
		https://unix.stackexchange.com/questions/203846/how-to-sync-two-folders-with-command-line-tools
	


	$ rsync -a --exclude=pattern 					source 		destination
	$ rsync -a --exclude=pattern --include=pattern 	source 		destination
	$ rsync -az -e(--ignore-existing) 				source		destination 
	
	
	# LOCAL >>> REMOTE
	$ rsync -azP  /home/apark/rtest/  	apark@45.55.5.69:/home/apark/rtest2 
	
	
	
	
	---
    rsync files in rtest folder
	rsync -av      /home/apark/rtest/   apark@45.55.5.69:/home/apark/rtest

	rsync -av iptables.vm5-sgas           remote_host:/etc/sysconfig
	rsync -av rog sgas sg-rog:/home

    rsync -av /etc/hosts                  remote_host:/etc
    rsync -av /etc/sysconfig/iptables     remote_host:/etc/sysconfig
    
	rsync -av   lobby1:/root/       /root --exclude ".ssh"

	ssh sg-eu "service iptables restart"



### Rsync issues ### 
### rsync failed due to remote host doesn't have the program installed.
	$rsync -av /RRServerLinux/ apark@10.128.8.174:/home/apark/RRServerSlave/
		apark@10.128.8.174's password:
		bash: rsync: command not found
		rsync: connection unexpectedly closed (0 bytes received so far) [sender]
		rsync error: remote command not found (code 127) at io.c(600) [sender=3.0.6]
		### FIX, Install rsync on REMOTE ###

	### S3CMD  
	# https://tecadmin.net/s3cmd-file-sync-with-s3bucket/
	
	$ s3cmd sync --dry-run --delete-removed --skip-existing s3://mobile-xpromo/  /cdn/data/
	
	# If file has been modified but suing same file name, remove the '--skip-existing' option(using date, size, md5sum)
	$ s3cmd sync --dry-run --delete-removed s3://mobile-xpromo/  /cdn/data/
	
	
	# Directory(include files) upload
		$ s3cmd sync -recursive local_dir1   s3://remote_dir1/
		local_dir1/file{1..10}  =>  s3://remote_dir1/local_dir1/file{1..10}
	
	# Files upload
		$ s3cmd put -r local_dir1/   s3://remote_dir1/
		local_dir1/file{1..10}  =>  s3://remote_dir1/file{1..10}
	
	
	
	
	
	
### IFS (Internal Field Separator)###
http://unix.stackexchange.com/questions/16192/what-is-ifs-in-context-of-for-looping
IFS isn't directly related to looping, it's related to word splitting. IFS indirectly determines how 
the output from the command is broken up into pieces that the loop iterates over.
When you have an unprotected variable substitution $foo or command substitution $(foo), there are 
two cases:
•	If the context expects a single word, e.g. when the substitution is between double quotes "$foo", 
	or in a variable assignment x=$foo, then the string resulting from the substitution is used as-is.
•	If the context expects multiple words, which is the case most of the times, then two further .
	expansions are performed on the resulting string:
•	The string is split into words. Any character that appears in $IFS is considered a word separator. 
	For example IFS=":"; foo="12:34::78"; echo $foo prints 12 34  78 (with two spaces between 34 and 
	78, since there's an empty word).
•	Each word is treated as a glob pattern and expanded into a list of file names. For example, 
	foo="*"; echo $foo prints the list of files in the current directory.

	For loops, like many other contexts, expect a list of words. So
	for x in $(foo); do …
	breaks $(foo) into words, and treats each word as a glob pattern. The default value of IFS isspace,
	tab and newline, so if foo prints out two lines hello world and howdy then the loop body is 
	executed with x=hello, then x=world and x=howdy. If IFS is explicitly changed to contain a newline 
	only, then the loop is executed for hello world and howdy. If IFS is changed to be o, then the 
	loop is executed for hell,  w, rld␤h (where ␤ is a newline character) and wdy.

	





### SSH TUNNEL
# origin
	$ ssh -N -R 8822:localhost:22 remote.host.com
	The optional -N says "don't execute a command" (helpful to prevent accidents caused by leaving 
	remote shells laying around.)

	Now from remote, you can SSH to host1 like this: (The remote port 8822 forwards to host1, but 
	only on the loopback interface.)

# remote
	$ ssh -p 8822 localhost
	For extra credit, you can export the forwarding to the whole world, allowing anyone get to host1 
	by hitting remote's port 8822. (Note the extra initial colon)

# origin
	$ ssh -N -R :8822:localhost:22 remote.host.com



	
	
############### 
# System Info #
###############

1. ssh -i .ssh/pem_key 		user_id@IP_Address

   ssh -p <port>       		user_id@IP_Address

   ssh-copy-id 	       		user_id@IP_Address

   # CentOS sudo without passwd
   
	$ sudo visudo   (old way   vi /etc/sudoers)
	  nozatech ALL=(ALL:ALL) NOPASSWD:ALL
	  %sudo    ALL=(ALL:ALL) NOPASSWD:ALL
	
	# Run specific 'script' without password prompt! 
	$ username  ALL=(ALL) NOPASSWD: /home/username/pydatertc.sh	
	
	# Ubuntu sudo without passwd
	$ sudo visudo
 		# User privilege specification
		root    ALL=(ALL:ALL) ALL
		apark   ALL=(ALL:ALL) NOPASSWD:ALL									<= Add NOPASSWD
		# Members of the admin group may gain root privileges
		%admin ALL=(ALL) NOPASSWD:ALL										<= Add NOPASSWD
		# Allow members of group sudo to execute any command
		%sudo   ALL=(ALL:ALL) NOPASSWD:ALL									<= Add NOPASSWD

	
	
	
	
	# Allowing/Denying User-Level Cron
	$ /etc/cron.allow

2 Processor
	killall <process name>
    kill -9 
	kill 15
	pkill <pid>  		<		= Kill all child processes, too.
	ps -u <user_id>				<= u (user)
		
		
3. Linux CentOS or Ubuntu Version check	
    $ cat /etc/*rel*
    $ uname -a				<= all include kernel, processor(32/64bit), hostname, 
    $ uname -r 				<= kernel version
    $ cat /proc/version

	###Install lsb_release on CentOS7
	$ yum provides */lsb_release			
	$ yum install redhat-lsb 
	$ yum install redhat-lsb-core
	$ lsb_release -a
   

    $ yum provides
	When you install sw and got a message saying that "xxx requires xxx library and xxx are missing."
	Then run 'yum provides "xxx"' to check dependencies.
	
	$ lsb_release -a						<= lsb <-Linux Standard Release
		No LSB modules are available.
		Distributor ID: Ubuntu
		Description:    Ubuntu 16.04.4 LTS
		Release:        16.04
		Codename:       xenial

   
4. ulimit 		
	system wide, user resource limits
	 Provides control over the resources available to the shell and to processes
     started by it, on systems that allow such control.  The -H and  -S  options
     specify  that the hard or soft limit is set for the given resource. 
			  
	OS needs memory to manage each open file
	$ ulimit -a							<= All current limits are reported
	$ ulimit -n 						<= *** The maximum number of open file *****
	$ lsof | wc -l						<= Count all opened files by all process
	$ cat /proc/sys/fs/file-max			<= Get maximum open files count allows
	$ cat /proc/sys/fs/file-nr			<= to get the current number of open files from the Linux kernel's point of view
	
	Example: This server has 40096 out of max 65536 open files, although lsof reports a much larger number:
	$ cat /proc/sys/fs/file-max
		65536
	$ cat /proc/sys/fs/file-nr 
		40096   0       65536
	$ lsof | wc -l
		521504
	
	### System-wide File Descriptors (FD) Limits  ###
	
	The Number Of Maximum Files Was Reached, How Do I Fix This Problem?????
	$ cat /proc/sys/fs/file-max
	  65536
	
	$ sudo sysctl -w fs.file-max=800000             <= change from 65536 to 800000
	# (or) $ echo 800000 > /proc/sys/fs/file-max
	
	$ cat /proc/sys/fs/file-max
	  800000
	
	$ vi /etc/sysctl.conf 
		fs.file-max=800000			<= append so that after reboot, still load 800000
	#(or) $ echo 'fs.file-max=800000" >> /etc/sysctl.conf
	#exit & relogin
	
	$ sudo sysctl -p
	
	$ cat /proc/sys/fs/file-max
		800000
	# (or)$ sysctl fs.file-max
		fs.file-max = 800000
	
	### User Level FD Limits
	$ cat /etc/security/limits.conf
	# for user 
	apark  -  nofile    64000
	
	# for all users
	*    soft    nofile 8192
	*    hard    nofile 8192
	
#---------------------------------------------------------------------------------------------------------	
	### Ulimit ### There are two limits in play: 
#-----------------------------------------------------------------------------------------------------
	1) the maximum number of open files the OS kernel allows (fs.file-max) and
	2) the per-user limit (ulimit -n). The former(Kernel) must be higher than the latter(User).
	
	
	If you are getting error “Too many open files (24)” then your application/command/script is hitting max 
	open file limit allowed by linux. You need to increase open file limit as below:
	
	### Check
	$ ulimit -a			<= All current limits
	$ ulimit -n			<= maximum number of open file
	$ ulimit -Sn		<= soft limit number
	$ ulimit -Hn		<= hard limit number
	
	### Current shell
	$ ulimit -n 1000		<= change
	$ ulimit -n 			<= check
	
	
	$ cat /proc/sys/fs/file-max
	 784694	
	
	### setting a new value in kernel variable /proc/sys/fs/file-max
	$ sysctl -w fs.file-max=100000   					 <= -w write
	
	$ vi /etc/sysctl.conf		
		fs.file-max = 100000			<= add
		## $ echo "fs.file-max = 100000"  >>  /etc/sysctl.conf
	
	$ sysctl -p		( or --load)						 <= load 
	  fs.file-max = 100000
	
	$ cat /proc/sys/fs/file-max 						 (or  $ sysctl fs.file-max )
	

	### Specific user limits ### 
	$ vi /etc/security/limits.conf		<= add this lines
	#<domain>   <type>  <item>         <value>
		*       soft     nproc          65535		<= * all users but no root
		*       hard     nproc          65535
		*       soft     nofile         65535
		*       hard     nofile         65535
		apark    -       nofile         65535		<= apark specific
		root     -       nofile         65535		<= root specific
		
		*** Relog-in to apply changes ***
	
		
	### Per-User Limit
	
	$ cat /etc/security/limits.conf
	#<domain>      <type>  <item>         <value>
	#root            hard    core            100000		<=100k files to open
	#@student        hard    nproc           20			<= only 20 files to open
	
	### pam-limits							<= session-related modules common to all services
	$ cat /etc/pam.d/common-session 		
	
	### System-Wide Limit
	$ echo 'fs.file-max = 2097152' >> /etc/sysctl.conf 			<= configure kernel parameters at runtime
	
	# configure kernel parameters at runtime
	$ sysctl -p 						sys						<= -p Load in sysctl settings from the file 
																	specified or /etc/sysctl.conf 
											
	### Verify New Limits
	$ cat /proc/sys/fs/file-max 			<= max limit of file descriptors:
	2097152
		
	### Check limit for other user
	$ su - www-data -c 'ulimit -aHS' -s '/bin/bash'
	$ su - user_ID  -c 'ulimit -aHS' -s '/bin/bash'
		core file size          (blocks, -c) 0
		data seg size           (kbytes, -d) unlimited
	
	
	### Check limits of a running process:
	$ cat /proc/Process_ID/limits
		Limit                     Soft Limit           Hard Limit           Units
		Max cpu time              unlimited            unlimited            seconds
	
	
	####
	$ echo 'fs.inotify.max_user_watches=100000' | sudo tee -a /etc/sysctl.conf; 			<=tee -a(append)
	$ sudo sysctl -p
	####06
	
	https://help.ubuntu.com/community/RootSudo
	$ ls | sudo tee -a /root/somefile 
	
	# pass the whole command to a shell process run under sudo to have the file written to with root permissions
	$ sudo sh -c "ls > /root/somefile"
	#-----------------------------------------------------------------------------------------------------

	### is my ulimit exceeded?
	Under high load it happens from time to time that network connections fail although everything they work 
	perfectly under low load. The reason be the ulimit set on a process. A ulimit restricts the process from 
	opening up more files (and network connections) than a certain number.
	http://www.linuxintro.org/wiki/Is_my_ulimit_exceeded
	$ ulimit -a
	
	$ You can permanently set the limits in /etc/security/limits.conf
	# to check
	$ ulimit -n 
		1024  			<= default max file open value
	$ ps -A (or -e)  select All processes
	  PID TTY          TIME CMD
		1 ?        00:00:10 systemd
		2 ?        00:00:00 kthreadd

	
	
	
	
### AWS Access Key/ Secret key
	
	/home/ubuntu/jarvis/ami/tcg/production/
	
    $ sudo sh -c "export OC_AWS_ACCESS_KEY=AKIAI6FBDACL5LFOEACA; \
				  export OC_AWS_SECRET_KEY=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; \
				  fab tcg.credit_gems:host=10.10.1.50,player_id="544053a5c66354d43f5f07d3",amount=50"
		
	To make a usage listing of the directories in the /home partition.  Note that this runs the commands in a
    sub-shell to make the cd and file redirection work.

    $ sudo sh -c "cd /home ; du -s * | sort -rn > diskUsage"   <= diskUsage file located in /home
	
	
#### Ubuntu  ###	
http://stackoverflow.com/questions/21515463/how-to-increase-maximum-file-open-limit-ulimit-in-ubuntu

echo "* soft nofile 102400" > /etc/security/limits.d/*_limits.conf && \
echo "* hard nofile 102400" >> /etc/security/limits.d/*_limits.conf 
####	
	
	$ man bash  					
	/ulimit  				<= ulimit is built in shell
	-S   Change and report the soft limit associated with a resource. 
    -H   Change and report the hard limit associated with a resource. 
    -a   All current limits are reported. 
    -c   The maximum size of core files created. 
    -d   The maximum size of a process's data segment. 
    -f   The maximum size of files created by the shell(default option) 
    -l   The maximum size that can be locked into memory. 
    -m   The maximum resident set size. 
    -n   The maximum number of open file descriptors. 
    -p   The pipe buffer size. 
    -s   The maximum stack size. 
    -t   The maximum amount of cpu time in seconds. 
    -u   The maximum number of processes available to a single user. 
    -v   The maximum amount of virtual memory available to the process. 
	###
	
#-------------------------------------------------------------------------------------------------------------
	
	
5. sysctl  			<= Configure kernel parameters at runtime
	The ulimit and sysctl programs allow to limit system-wide resource use. 
	This can help a lot in system administration, e.g. when a user starts too 
	many processes and therefore makes the system unresponsive for other users.

	
	
	
	
6. Swap space swapon
	swapon -s 		<= Summary
		   -a 		<= All
	
	sudo dd if=/dev/zero of=/var/swapfile bs=1M count=2048
	sudo chmod 600 /var/swapfile
	sudo mkswap /var/swapfile
	echo /var/swapfile none swap defaults 0 0 | sudo tee -a /etc/fstab
	sudo swapon -a

  
	sudo dd if=/dev/zero of=/mnt/{filename}.swap bs=1M count={swap_size}
	sudo mkswap /mnt/{filename}.swap
	sudo swapon /mnt/{filename}.swap
	sudo vi /etc/fstab
	Add the following text at the end of the file, 
	/mnt/{filename}.swap  none  swap  sw  0 0
  

  ### Create a swap space script ### 
	#!/bin/sh
	# make swap space s
	dd if=/dev/zero of=/swapfile bs=1024 count=8388608
	mkswap /swapfile
	echo '/swapfile         swap            swap    defaults 0 0' >> /etc/fstab
	swapon -a
	swapon -s
	###
  
  ### creating random data for 10MB # dd <= covert and copy a file
    dd if=/dev/urandom of=foo bs=1000 count=10000     
		^ if=FILEread from FILE instead of stdin
		                ^ of=FILE, write to FILE instead of stdout
							  ^ bs=bytes, read and write up to BYTES bytes at a time



  
7. Start Up service for Ubuntu
  
	sudo mv /filename /etc/init.d/
	sudo chmod +x /etc/init.d/filename 
	sudo update-rc.d filename defaults 
	
	
	
  
8. UUID  <= identifier for block devices. 
	UUIDs are 128 bit long numbers represented by 32 hexadecimal digits and which are used in 
	software development to uniquely identify information with no further context. 
	
	#Linux implementation and generation
	 In Linux UUIDs are generated in /drivers/char/random.c?id=refs/DATEs/v3.8, and you can generate new ones via proc:
	# Usage in fstab
	As mentioned UUIDs are most often used in Linux to identify block devices. Imagine, you have a couple of hard disks 
	attached via USBs, than there is no persistent, reliable naming of the devices: sometimes the first USB hard disk is 
	named “sda”, sometimes it is named “sdb”. So to uniquely address the right disk for example in your /etc/fstab, 
	you have to add an entry like:
	
	UUID=9043278a-1817-4ff5-8145-c79d8e24ea79 /boot ext3 defaults 0 2

	For the block device itself, the uuid is stored in the superblock.
	Beware however that UUIDs should not be used in fstab when you work with LVM snapshots(no 2 device using same uuid). 
	
	
	
	$ cat /proc/sys/kernel/random/uuid
		eaf3a162-d770-4ec9-a819-ec96d429ea9f
	
	# There is also the library libuuid which is used by uuidgen and especially 
		by the ext2/3/4 tools E2fsprogs to generate UUIDs:
	$ uuidgen 
	f81cc383-aa75-4714-aa8a-3ce39e8ad33c
	
	# bash style
	$ls -l /dev/disk/by-uuid
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 116a4716-c5ec-4ce5-aea4-9fea29d78f76 -> ../../dm-1
	lrwxrwxrwx. 1 root root  9 Apr 13 03:17 2015-12-09-23-03-16-00 -> ../../sr0
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 3479cc28-9e7d-4798-abef-a50bafef8761 -> ../../dm-0
	lrwxrwxrwx. 1 root root 10 Apr 13 03:17 62f1fbbf-e39b-4140-90ed-3d88a7988fa5 -> ../../sda1

	
	$sudo blkid /dev/sda1
	/dev/sda1: UUID="62f1fbbf-e39b-4140-90ed-3d88a7988fa5" TYPE="xfs"
	
	$ udevadm info -q all -n /dev/sda1|grep uuid
	S: disk/by-uuid/62f1fbbf-e39b-4140-90ed-3d88a7988fa5
	E: DEVLINKS=/dev/disk/by-path/pci-0000:00:10.0-scsi-0:0:0:0-part1 /dev/disk/by-uuid/62f1fbbf-e39b-4140-90ed-3d88a7988fa5
	
	### When not to use UUID ###
	Since it is not possible to mount two file systems with the same UUID, extra care need to be taken 
	when LVM snapshots (or cloned disks) are used in an environment: mounting might fail due to duplicate UUIDs.
	
	XFS: Filesystem dm-2 has duplicate UUID – can’t mount
	
	One way to deal with this is by the way to change the UUID during creation or afterwards, 
	another way is to mount with the nouuid option.

### Package List
	# Ubuntu
	$ dpkg -l | grep mysql
	
	# CentOS
	$ rpm -qa | grep mysql
	
	


#########################
# 5. PROCESS HANDLING.  #
#########################

To suspend a job, type CTRL+Z while it is running. You can also suspend a job with CTRL+Y. 
This is slightly different from CTRL+Z in that the process is only stopped when it attempts to 
read input from terminal. Of course, to interupt a job, type CTRL+C.


Linux_CMD &  # runs job in the background using '&' at the end and prompts back the shell

jobs         # lists all jobs (use with -l to see associated PID)

fg           # brings a background job into the foreground
fg %+        # brings most recently invoked background job
fg %-        # brings second most recently invoked background job
fg %N        # brings job number N
fg %string   # brings job whose command begins with string
fg %?string  # brings job whose command contains string

kill -l      # Options, returns a list of all signals on the system, by name and number
kill PID     # terminates process with specified PID
killall
pkill

$ pgrep nginx		# pgrep, pkill - look up or signal processes based on name and other attributes
$ pkill nginx		# kill all child process that spawned from nginx process id

$ pgrep -u apark	# process grep all apark's process
$ pkill -u apark	# kill all apark's process


kill 	PID			# same as 'kill 15'
kill 15 PID			# gracefully shutdown (terminate)
kill 9	PID			# force to kill
killall name 		#




### kill all processes relate to same process name
	$ pkill -f s3cmd
	$ ps -ef | grep myProcessName | grep -v grep | awk '{print $2}' | xargs kill -9


	ps           # prints a line of information about the current running login shell and any processes running under it
	ps -a        # selects all processes with a tty except session leaders

### ps -ef	vs ps aux
		Unix, BSD and Debian
		ps aux 				<= BSD system		(Ubuntu, Debian) 	
		ps -ef 				<= System V system	(Redhat, CentOS) <= Unix family
EXAMPLES
       To see every process on the system using standard syntax: <= Unix family
          ps -e
          ps -ef
          ps -eF
          ps -ely

       To see every process on the system using BSD syntax:
          ps ax
          ps axu

       To print a process tree:
          ps -ejH
          ps axjf

       To get info about threads:
          ps -eLf
          ps axms

       To get security info:
          ps -eo euser,ruser,suser,fuser,f,comm,label
          ps axZ
          ps -eM

       To see every process running as root (real & effective ID) in user format:
          ps -U root -u root u

       To see every process with a user-defined format:
          ps -eo pid,tid,class,rtprio,ni,pri,psr,pcpu,stat,wchan:14,comm
          ps axo stat,euid,ruid,tty,tpgid,sess,pgrp,ppid,pid,pcpu,comm
          ps -Ao pid,tt,user,fname,tmout,f,wchan

       Print only the process IDs of syslogd:
          ps -C syslogd -o pid=

       Print only the name of PID 42:
          ps -q 42 -o comm=

		

	$ ps aux | grep RRServer | grep -v grep | awk '{print $2}'
	

	$ ps  **options
	********* simple selection *********  ********* selection by list *********
	-A all processes                      -C by command name
	-N negate selection                   -G by real group ID (supports names)
	-a all w/ tty except session leaders  -U by real user ID (supports names)
	-d all except session leaders         -g by session OR by effective group name
	-e all processes                      -p by process ID
	T  all processes on this terminal     -s processes in the sessions given
	a  all w/ tty, including other users  -t by tty
	g  OBSOLETE -- DO NOT USE             -u by effective user ID (supports names)
	r  only running processes             U  processes for specified users
	x  processes w/o controlling ttys     t  by tty
	*********** output format **********  *********** long options ***********
	-o,o user-defined  -f full            --Group --User --pid --cols --ppid
	-j,j job control   s  signal          --group --user --sid --rows --info
	-O,O preloaded -o  v  virtual memory  --cumulative --format --deselect
	-l,l long          u  user-oriented   --sort --tty --forest --version
	-F   extra full    X  registers       --heading --no-heading --context
    ********* misc options *********
	-V,V  show version      L  list format codes  f  ASCII art forest
	-m,m,-L,-T,H  threads   S  children in sum    -y change -l format
	-M,Z  security data     c  true command name  -c scheduling class
	-w,w  wide output       n  numeric WCHAN,UID  -H process hierarchy

	
## kill pts (pseudo terminal device  e.g. xterm, screen, ssh)
	A tty is a Regular Terminal Device (the console on your server).
	A pts is a Psuedo  Terminal Slave  (ssh, xterm, screen connection).
	
	$ who
		apark    pts/1        2017-08-01 18:32 (65.87.26.124)
		apark    pts/0        2017-08-01 16:25 (65.87.26.124)   <= need to kill!!!
		
	$ ps -ft pts/0				<= -f <-full-format listing,  -t <- by tty
								
		UID        PID  PPID  C STIME TTY          TIME CMD
		apark    28110 28109  0 16:25 pts/0    00:00:00 -bash
		root     28129 28110  0 16:25 pts/0    00:00:00 sudo su
	$ kill -9  28110 28129 
	$ who
		apark    pts/1        2017-08-01 18:32 (65.87.26.124)

		
###  How to kill zombie processTo suppress error message
		A zombie is already dead, so you cannot kill it. To clean up a zombie, it must be waited on by its parent, 
		so killing the parent should work to eliminate the zombie. (After the parent dies, the zombie will be 
		inherited by init or systemD (process ID 1), which will wait on it and clear its entry in the process table.) 
		If your daemon is spawning children that become zombies, you have a bug. Your daemon should notice 
		when its children die and wait on them to determine their exit status.

		Example command:
		kill $(ps -A -ostat,ppid | awk '/[zZ]/{print $2}')

			
###	
	Zombie processes (also show as <defunct>), aren't real processes at all. They are just entries in the kernel process table. 
	This is the only resource they consume. They do not consume any CPU or RAM. 
	*** The only danger of having zombies, is running out of space in process table!!!***
	
	$ cat /proc/sys/kernel/threads-max 
		30112
		
	They appear only when their parent process (i.e. process which fork()'ed them) is alive, but did not yet call wait() 
	system function. Once parent dies, the zombies are wait()'ed for by INIT(1st process) and disappear.
###


trap cmd sig1 sig2    # executes a command when a signal is received by the script
trap ""  sig1 sig2    # ignores that signals
trap -   sig1 sig2    # resets the action taken when the signal is received to the default

disown <PID|JID>      # removes the process from the list of jobs

wait                  # waits until all background jobs have finished



6. TIPS AND TRICKS.

# to quickly go to a specific directory
	$ cd; nano .bashrc
	> shopt -s cdable_vars
	> export websites="/Users/mac/Documents/websites"

	$ source .bashrc
	$ cd websites
	$ cd *				<= if there is only one folder


############################################
###
############################################
1. Find and Sort Files Based on Modification Date and Time
	$ ls -lt		<= Modification Time
	$ ls -ltr		<= Modification Time but Reverse
	$ ls -ln		<= by name	
	$ ls -lnr		<= by name	
	
	B. List Files Based on Last Access Time
		


1. Grep 
	# grep  pattern  file_name  #search for pattern in files
	$ grep -i					# Case insensitive search
	$ grep -r 					# Recursive
	$ grep -r "port 80" /etc/httpd/*   <= search subdirectories with key words port 80.
	$ grep -v httpd				# Inverted search (everything but httpd)
	$ grep -o					# Show matched part of file only
	$ grep -v grep    			# don't match the grep word
    To remove the grep in search word, use "grep -v grep" 2nd grep is a word to -v <= don't match word .
	
	$ ps aux | grep some_PID => out puts grep word in list, and it counts the line as 1 + 1 actual process name some_PID
	$ pgrep PID				<= short



#####################################################################
mkdir /var/www/html/status && chown apark: /var/www/html/status
#####################################################################
#!/bin/bash
# Check and put ok status if the RRS game service is running for Lobby 7504 & 7505 services

if (( $(ps aux | grep ./RRServer_0617_7504 | grep -v grep | wc -l) > 0  &&  \
$(ps aux | grep ./RRServer_0617_7505 | wc -l) > 0 ));
        then
        echo OK > /var/www/html/status/index.html;
else rm /var/www/html/status/index.html 2>&1 /dev/null;
fi
###############################################################



### 
# Find biggest | largest files top 5 list Only
###

# NCurses Disk Usage	<= Utility
	$ ncdu /
	$ ncdu -x /    <= scan a full filesystem
		# Since scanning a large directory may take a while, you can scan a directory and 
			export the results for later viewing:
	
	$ ncdu -1xo- / | gzip > ncdu-list.gz
	 ...some time later...
	$ zcat ncdu-list.gz | ncdu -f-				<= read scanned list as a file
	
	
	$ find    / -type f -exec du -Sh {} + | sort -rh | head -n 10  				<= Top 10 list
	$ find /*/* -type f -exec du -Sh {} + | sort -rh | head -n 5

	### Top largest files (>100M) on your filesystem
	$ find / -xdev -type f -size +100M -exec du -sh {} ';' | sort -rh | head -n10 
			# -xdev  Don't descend directories on other filesystems.
	
	$ find / -xdev -type f -size +100M -exec ls -la {} \; | sort -nk 5
	
	$ find / -xdev -type f -size +100M
	
	
	$ du -ahx / | sort -rh | head -10		<= -ahx <-all, human, skip dirs on diff file systems
											<= -rh 	<- reverse, human
	
	
	$ find /dir/ -user 	user_ID			#Find files owned by name in dir
	$ find /dir/ -name 	file_name
	$ find /dir/ -iname fine_name
	


	$ find /etc | grep -e ulimit -e 4096 -e nofile   		<= e <-pattern (multi pattern search )


	$ find /lib64 | grep mysql
		/lib64/libmysqlcppconn.so
		/lib64/libmysqlcppconn.so.7.1.1.3

####
Tmux  Terminal Multiplexer

ctrl-b <command>
ctrl-b c - new window


#####
Ubuntu Package Installed 

	$ dpkg -l | grep mongo

	$ root@puppet:~# dpkg --get-selections
	$ root@puppet:~# dpkg --get-selections | grep puppet
		puppet-common                                   install
		puppetlabs-release                              install


### sed (stream editor) 
	https://linuxconfig.org/learning-linux-commands-sed
	http://www.grymoire.com/Unix/Sed.html

	$ sed -i 's/START=no/START=yes/g'    /etc/default/puppet		
			i 	<= in place(INSERT MODE for Same file)
			g 	<= global (replace all)
	
	$ sed -i 's/original/new/g' file.txt
			-i 		 <= in-place (Insert mode to original file)
			'		 <= gate open
			s 		 <= the substitute command
			original <= a regex describing the word to replace(or just the word itself)
			new 	 <= the text to be replaced with
			g 		 <= global (replace all)
			'		 <= gate close
			file.txt <= target file name

	$ sed -i 's/START=no/START=yes/'     /etc/default/puppet	<= 1st line
	$ sed -i 's/START=no/START=yes/4'    /etc/default/puppet	<= 4th line
	
	$ sed 's/yes/no/g' < yesNo.txt								<= feed file 
	
	$ sed '/Name/s/PAK/PARK/g' report.txt						<= only contain 'Name' lines
	
	# sed => a New file
	$ sed 's/PAK/PARK/g' reportNew.txt	 > new.txt
	
	# cat => sed => a New file
	$ cat report.txt | sed 's/PAK/PARK/g' > reportNew.txt

	
	#hostname change from Ubuntu14
	$ sed -i 's/u14/rieman/g' /etc/hosts


	
	# IP change (change ip) using sed
	$ sed -i -r  "s/10.1 28.232.181\b/10.130.244.212/g"     start-us-lobby-7505.sh
	$ sed -i 	 's/10.10.10.10/1.1.1.1/g'    ip.txt		<= works too
	$ sed 's/^/ /' ip.txt    > ip_new.txt					<= add a space in front
	$ sed 's/^/    /' ip.txt > ip_new.txt					<= add 4 space in front
		(space)1.1.1.1
	$ sed -n 1p ip.txt										<= display one 1st line
		1.1.1.1
	
	
	### CMD Input Mode
	$ sed 's/the/THE/'
		the
		THE
	$ Ctrl +D  <= exit
	$ nl file.txt | sed 's/the/THE'
	
	# from CMD using echo & sed
	$ echo day | sed 's/day/night/'			
		night
----------------------------------
### Regular Expressions (regex)###
----------------------------------
. 		<= Any single character except newline
*		<= zero or more occurances of any character
[...]	<= Any single character specified in the set
[^...]	<= Any single character not specified in the set
^		<= Anchor - beginning of the line
$		<= Anchor - end of line
\<		<= Anchor - begining of word
\>		<= Anchor - end of word
\(...\)	<= Grouping - usually used to group conditions
\n		<= Contents of nth grouping

[...]   <= Set Examples
[A-Z]	<= The SET from Capital A to Capital Z
[a-z]	<= The SET from lowercase a to lowercase z
[0-9]	<= The SET from 0 to 9 (All numerals)
[./=+]	<= The SET containing . (dot), / (slash), =, and +
[-A-F]	<= The SET from Capital A to Capital F and the dash (dashes must be specified first)
[0-9 A-Z]	<= The SET containing all capital letters and digits and a space
[A-Z][a-zA-Z]	<= In the first position, the SET from Capital A to Capital Z
In the second character position, the SET containing all letters

Regular Expression Examples
/Hello/		<= Matches if the line contains the value Hello
/^TEST$/	<= Matches if the line contains TEST by itself
/^[a-zA-Z]/	<= Matches if the line starts with any letter
/^[a-z].*/	<= Matches if the first character of the line is a-z and there is at least one more of any character following it
/2134$/		<= Matches if line ends with 2134
/\(21|35\)/	<= Matches is the line contains 21 or 35
			   #Note: the use of ( ) with the pipe symbol to specify the 'or' condition
/[0-9]*/	<= Matches if there are zero or more numbers in the line
/^[^#]/		<= Matches if the first character is not a # in the line


Regex tutorial- A quick cheatsheet 
https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285
Anchors ^ and $
^The        <= matches any string that starts with The
            https://regex101.com/r/cO8lqs/2
end$        <= matches a string that ends with end
^The end$   <= exact string match (starts and ends with The end)
roar        <= matches any string that has the text roar in it




### IP Regular Expression ###
http://www.analyticsmarket.com/freetools/ipregex

------------------------------------------------------------------------------------------------------------
Notes:
1. Regular expressions are case sensitive
2. Regular expressions are to be used where pattern is specified
------------------------------------------------------------------------------------------------------------

		
		  
### EOF (End Of File <= Add some rows and text)
$ cat << EOF >> /etc/puppet/puppet.conf
[agent]
server = puppetmaster.domain.tld
EOF


The 'cat << EOF' Bash syntax is very useful when one needs to work with multiline strings in Bash, 
	eg. when passing multiline string to a variable, file or a piped command.
1. Passing multiline string to a variable:
	$ sql=$(cat <<EOF
	  SELECT foo, bar FROM db
	  WHERE foo='baz'
	  EOF
	  )
	
	
noza@fm:~$ echo $sql
SELECT foo, bar FROM db WHERE foo='baz'

noza@fm:~$ echo -e $sql
SELECT foo, bar FROM db WHERE foo='baz'

noza@fm:~$ echo -e "$sql"
SELECT foo, bar FROM db
WHERE foo='baz'
The $sql variable now holds newlines as well, you can check it with echo -e "$sql" cmd.

2. Passing multiline string to a file:

$ cat << EOF > print.sh
#!/bin/bash
echo \$PWD
echo $PWD
EOF
The print.sh file now contains:

#!/bin/bash
echo $PWD
echo /home/user

3. Passing multiline string to a command/pipe:

$ cat <<EOF | grep 'b' | tee b.txt | grep 'r'
foo
bar
baz
EOF
This creates b.txt file with both bar and baz lines but prints only the bar.


"EOF" is known as a "Here DATE". Basically << Here tells the shell that you are going to enter 
	a multiline string until the "DATE" Here. You can name this DATE as you want, it's often EOF or STOP.
	- The DATE can be any string, uppercase or lowercase, though most people use uppercase by convention.
	- The DATE will not be considered as a Here DATE if there are other words in that line. In this case, 
		it will merely be considered part of the string. The DATE should be by itself on a separate line, 
		to be considered a DATE.
	- The DATE should have no leading or trailing spaces in that line to be considered a DATE. Otherwise 
		it will be considered as part of the string.
example:

$ cat >> test <<HERE
> Hello world HERE <--- Not the end of string
> This is a test
>  HERE <-- Leading space, so not end of string
> and a new line
> HERE <-- Now we have the end of the string
More: http://stackoverflow.com/questions/2500436/how-does-cat-eof-work-in-bash




####
Install Fabric, boto, yaml

   1) Install fabric, boto and yaml

         wget https://bootstrap.pypa.io/get-pip.py
         sudo python get-pip.py
         sudo pip install fabric
         sudo pip install boto
         sudo pip install PyYaml

		 

#################	 
###  Repair   ###
#################
Recover from File System Corruption Using 'FSCK' and a Recovery ISO
https://www.digitalocean.com/community/tutorials/how-to-recover-from-file-system-corruption-using-fsck-and-a-recovery-iso

# fsck - check and repair a Linux filesystem
	$ fsck -yf /dev/sda			<= -y, always  attempt to fix
  after reboot 
	$ ls lost+found
# blkid - locate/print block device attributes
















		 
#################	 
### Security  ###
#################

1.Verifying Which Ports Are Listening
CentOS
#which ports are listening for TCP connections from the network:
	nmap -sT -O localhost   # -sT <= scan TCP  -O <= OS

#To check if the port is associated with the official list of known services	
	cat /etc/services | grep unknown_port

	# check for information about the port	
	netstat -anp | grep unknown_port     # -anp  <= All, no dns lookup(faster), program
	
	lsof -i | grep port_number		<= internet connection open
	lsof -p                      	<= process_number
	
	https://www.cyberciti.biz/faq/what-process-has-open-linux-port/
	fuser -k 80/tcp  <- kill port?
	
	
2. Changing SSH port from default #22 to 2222 
	$ vi /etc/ssh/sshd_config
		#22 to 2222 
	$ /etc/init.d/sshd restart
	
	Update the IPTables
	$ iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
	
	Allow all from x.x.x.x IP 
	-A INPUT -m state -s x.x.x.x --state NEW -j ACCEPT

###
Allow everything from single IP using cmd	
###------------------------------------------------------------------
$ iptables -A INPUT -m state -s 65.87.26.0/24     --state NEW -j ACCEPT		<= BNEA Corp IP

$ iptables -A INPUT -m state -s 64.95.137.0/24    --state NEW -j ACCEPT		<= BNEA PacCity WIFI Comcast IP

$ iptables -A INPUT -m state -s 73.231.235.144/32 --state NEW -j ACCEPT   	<= Apark Home
###	-----------------------------------------------------------------

# iptables -t [filter|nat|mangle] [-A|-D|-L|-F] [INPUT|OUTPUT|FORWARD] -p [tcp|udp] -m [tcp|udp] -s [sourceip] -d [destip] --dport [port] -j [DROP|REJECT|ACCEPT] 

# iptables -t [filter] [-A] [INPUT   ]   -p [tcp] -m [tcp] -s [sourceip] -d [destip] --dport [port] -j [ DROP   ] 	
			   nat	    -D   OUTPUT          udp      udp												 REJECT
			   mangle	-L   FORWARD																     ACCEPT
						-F

# http://www.iana.org/assignments/icmp-parameters/icmp-parameters.xhtml#icmp-parameters-codes-8
# allow ping from IPs
$ iptables -A INPUT -s x.x.x.x -p ICMP --icmp-type 8 -j ACCEPT

# block all ping
$ iptables -A INPUT -p ICMP --icmp-type 8 -j DROP


### Ubuntu Port Forwarding using Iptables

	# Using Linux iptables for port 80 -> 8080
	
	This enables port forwarding of traffic between ports 80 and 8080. 
	You can keep Jenkins on the default port 8080.
	
	$ sudo vi /etc/rc.local
	Then add the following just before the exit 0
	
	#Requests from outside
	iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 8080
	
	#Requests from localhost
	iptables -t nat -I OUTPUT -p tcp -d 127.0.0.1 --dport 80 -j REDIRECT --to-ports 8080
	
	Now reboot or run sudo /etc/rc.local to enable port forwarding. 
	Additional info: https://gist.github.com/m5m1th/6870a54717c0387468c3



### email setup and send out from shell ###
	http://www.binarytides.com/linux-mailx-command/
	https://www.digitalocean.com/community/tutorials/how-to-send-e-mail-alerts-on-a-centos-vps-for-system-monitoring
	
	$ yum -y update
	$ yum install -y mailx
	$ ln -s /bin/mailx /bin/email
	
	# Sending an email to G-mail
		## Method 1
	$ mail -s "Test" albert@gmail.com < /dev/null
		## Method 2
	$ mail -s "Test" albert@gmail.com 
		This is the MSG Test
	  'CTRL+D'  to end the program.
	
	# $ vi /etc/mail.rc     <= ???
	
############################################################################
#!/bin/bash
# If IP Address changes, it will send out an email for update.
currentIP=`ifconfig | grep 'inet'| awk '{print $2}' | head -1`;
time=`date`;
oldIP_file='/home/apark/ipAddress/oldIP.txt';
oldIP=`cat ${oldIP_file}`;

if [[ ${currentIP} != ${oldIP} ]]
then
    echo "New IP = ${currentIP}"
    echo "IP has been changed and need to send an email."
    echo -e "Hello\n\n\
            TimeOfChanges = ${time}\n \
			NEW_IP = ${currentIP}\n\nBye" | \
            /usr/bin/mail -s \
            "[INFO] IP has been changed to ${currentIP}" \
            albertpark5@gmail.com;
        echo ${currentIP} > ${oldIP_file};
else
        echo "no IP changes!"
fi
############################################################################
$crontab -e
*/30 * * * * /home/apark/ipAddress/ip_changes_alert.sh > /dev/null
############################################################################
	
	
	
	
	
### email send out from SHELL
	echo "hello" | mailx -v -s 'test' apark@bnga.com		<= -v verbose
	echo "hello" | mail  -s    'test' apark@bnga.com
	echo "Test"  | mail  -s    "This is Subject" -r "Icinga2<apark@bnga.com>" apark@bnga.com
															^ -r <= from<email>?	
 
	### Check mail log
	
 
 ### Use external SMTP server
	echo "This is the message" | mailx -v \
		-r "apark@bnga.com" \
		-s "subject" \
		-S smtp="email-smtp.us-west-2.amazonaws.com:465" \
		-S smtp-use-starttls \
		-S smtp-auth=login \
		-S smtp-auth-user="AKIAIRJHSW462O4HVOVQ" \
		-S smtp-auth-password="ApWdT46E5NqybHu0l+Iw3iiUnHPf8I7LmmZUb1aCLXXz" \
		-S ssl-verify=true \
		apark@bnga.com
		
### SMTP setup (email server setup)
	$vi	/etc/ssmtp/ssmtp.conf

	$cat /etc/ssmtp/ssmtp.conf.rpmsave
	-----------------------------------------------------------------------------------
	#mailhub=email-smtp.us-west-2.amazonaws.com:465
	#UseTLS=yes
	#FromLineOverride=yes
	#AuthUser=AKIAIRJHSW462O4HVOVQ
	#AuthPass=ApWdT46E5NqybHu0l+Iw3iiUnHPf8I7LmmZUb1aCLXXz

	# Config file for SMTP sendmail
	# The person who gets all mail for userids < 1000
	# Make this empty to disable rewriting.
	root=postmaster

	# The place where the mail goes. The actual machine name is required no
	# MX records are consulted. Commonly mailhosts are named mail.domain.com
	AuthUser=albertpark5@gmail.com
	AuthPass=
	mailhub=smtp.gmail.com:587
	UseSTARTTLS=YES

	# Where will the mail seem to come from?
	rewriteDomain=Icinga2.Host

	# The full hostname
	hostname=Icinga2

	# Are users allowed to set their own From: address?
	# YES - Allow the user to specify their own From: address
	# NO - Use the system generated From: address
	FromLineOverride=YES
	-----------------------------------------------------------------------------------
		
		

###########
### SSL ###
###########

DSA is faster in signing, but slower in verifying. A DSA key of the same strength as RSA (1024 bits) 
generates a smaller signature. An RSA 512 bit key has been cracked, but only a 280 DSA key. 
Also note that DSA can only be used for signing/verification, whereas RSA can be used for 
encryption/decrypt as well.


SSL has been around for long enough you'd think that there would be agreed upon container formats. And you're right, 
	there are. Too many standards as it happens. So this is what I know, and I'm sure others will chime in.

.csr This is a Certificate Signing Request. Some applications can generate these for submission to 
	certificate-authorities. The actual format is PKCS10 which is defined in RFC 2986. It includes 
	some/all of the key details of the requested certificate such as subject, organization, state, 
	whatnot, as well as the public key of the certificate to get signed. These get signed by the CA 
	and a certificate is returned. The returned certificate is the public certificate (which includes 
	the public key but not the private key), which itself can be in a couple of formats.
.pem Defined in RFC's 1421 through 1424, this is a container format that may include just the public 
	certificate (such as with Apache installs, and CA certificate files /etc/ssl/certs), or may 
	include an entire certificate chain including public key, private key, and root certificates. 
	Confusingly, it may also encode a CSR (e.g. as used here) as the PKCS10 format can be translated 
	into PEM. The name is from Privacy Enhanced Mail (PEM), a failed method for secure email but the 
	container format it used lives on, and is a base64 translation of the x509 ASN.1 keys.
.key This is a PEM formatted file containing just the private-key of a specific certificate and is 
	merely a conventional name and not a standardized one. In Apache installs, this frequently resides 
	in /etc/ssl/private. The rights on these files are very important, and some programs will refuse 
	to load these certificates if they are set wrong.
.pkcs12 .pfx .p12 Originally defined by RSA in the Public-Key Cryptography Standards, the "12" variant 
	was enhanced by Microsoft. This is a passworded container format that contains both public and private 
	certificate pairs. Unlike .pem files, this container is fully encrypted. Openssl can turn this into 
	a .pem file with both public and private keys: openssl pkcs12 -in file-to-convert.p12 -out converted-file.pem -nodes
A few other formats that show up from time to time:

.der A way to encode ASN.1 syntax in binary, a .pem file is just a Base64 encoded .der file. OpenSSL 
	can convert these to .pem (openssl x509 -inform der -in to-convert.der -out converted.pem). Windows 
	sees these as Certificate files. By default, Windows will export certificates as .DER formatted 
	files with a different extension. Like...
.cert .cer .crt A .pem (or rarely .der) formatted file with a different extension, one that is recognized 
	by Windows Explorer as a certificate, which .pem is not.
.p7b Defined in RFC 2315, this is a format used by windows for certificate interchange. Java understands 
	these natively. Unlike .pem style certificates, this format has a defined way to include certification-path certificates.
.crl A certificate revocation list. Certificate Authorities produce these as a way to de-authorize 
	certificates before expiration. You can sometimes download them from CA websites.
In summary, there are four different ways to present certificates and their components:

PEM Governed by RFCs, it's used preferentially by open-source software. It can have a variety of extensions 
	(.pem, .key, .cer, .cert, more)
PKCS7 An open standard used by Java and supported by Windows. Does not contain private key material.
PKCS12 A private standard that provides enhanced security versus the plain-text PEM format. This can 
	contain private key material. It's used preferentially by Windows systems, and can be freely converted 
	to PEM format through use of openssl.
DER The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. 
	Not routinely used by much outside of Windows.



#####################################################
###  mistakenly deleted /boot folder and rebooted ###
#####################################################
1. Bootup with Live CD.

2. Find the drive/partition where you have installed your root filesystem. 
	To do this you can open a terminal and run either 
	$ sudo parted -l     # parted  is  a  disk  partitioning  and  partition resizing program.o
	$ sudo fdisk -l 	 #  a menu-driven program for creation and manipulation of partition tables.

3. Assuming that your root partition that you found from the last step is /dev/sda1
	Since Linux 2.4.0 it is possible to remount part of the file hierarchy somewhere else. The call is 
	'mount --bind old_dir new_dir'

	$ mkdir mnt
	$ sudo mount /dev/sda1 mnt
	$ sudo mount --bind /dev /mnt/dev
	$ sudo mount --bind /proc /mnt/proc
	$ sudo mount --bind /sys /mnt/sys
	$ sudo chroot mnt						<= chroot - run command or interactive shell with special root directory

4. You will now be inside a chroot environment meaning that running commands here is equivalent to running 
	them on your installed system. 
	The first thing you want to do is reinstall GRUB2 to the device so that it copies the correct files into the /boot folder. 
	To do this run the following with the drive that your root partition is on (ie /dev/sda1 ):

	$ grub-install /dev/sda
	You now want to find out which packages you have installed that have files in the boot directory and reinstall them. 
	This will replace the kernel images that have been deleted among other things. The command to find the packages is:

5. $ dpkg -S /boot
		memtest86+, linux-image-3.13.0-32-generic, base-files: /boot
		 And to reinstall them:

6. $ apt-get --reinstall install linux-image-3.13.0-32-generic
	
	This step will probably require internet access (unless the packages are already in the cache), so make 
	sure you are connected if there is an problem.

	Since you will have deleted your kernels and reinstalled them, this should have triggered a GRUB2 update 
	automatically. But just in case they haven't, you can run:

7. $ update-grub
	Reboot and things should now be fixed. 
	One issue that I had the last time I did something similar was that Windows installs where not found by update-grub 
	One issue that I had the last time I did something similar was that Windows installs where not found by update-grub 
	when run in the chroot due to a bug in os-prober. If this is an issue, just run sudo update-grub 
	again in the repaired system.

	
### CentOS package update vs upgrade	
'yum upgrade' and 'yum update' will perform the same function that update to the latest current version of package.

But the difference is 'Upgrade' will delete obsolete packages, while 'update' will preserve them.
$ yum upgrade kernel -y   	<= only upgrade the kernel 
$ yum update  kernel -y		<= 


### CHROOT
	Using chroot cmd to switch different file systems
	e.g. Host OS kernel, module, drivers on host file system1(Ubuntu /dev/sda1) can modify on file system2(Debina /dev/sda2) 

Ubuntu 
$nmap 			<- works
$ cd /debian
$ chroot . bash
Debian 			<- Using host kernel but running on Debian file system to modify
$ nmap 			<- need to install
$ exit

### Ubuntu Kernel Update
https://wiki.ubuntu.com/Kernel/LTSEnablementStack#Ubuntu_16.04_LTS_-_Xenial_Xerus
Ubuntu 16.04 LTS Server- Xenial Xerus

$ sudo apt-get install --install-recommends linux-generic-hwe-16.04 

$ sudo apt-get update && sudo apt-get dist-upgrade
	Update command is used to update cache, dist-upgrade will update your kernel to 
	latest available in repository.

	
### CentOS Kernel Update	
$ sudo yum update kernel -y	
	
	
	
### Byobu multi screen utility ###
# CentOS 7
$ sudo yum install --enablerepo=epel byobu

	$ byobu 

	F2 <= New terminal   
	Moving      F3 <=   => F$
	
	Shift+F2  <= Split 1/2 Vertically
	Ctrl +F2  <= Split 1/2 Horizontally

	# Make 4 Split screens with in 1 terminal
	Shift+F2 => Ctrl+F2 => Shift+4 => Ctrl+F2
	
	# Moving around
	Shift+F4
	
	# Too kill
	Ctrl+d    <=Exit
	
# Ubuntu 14
    $ byobu-enable/disable
	$ ctrl+F2
	
DOS - Remove all files including subdirectory
# Remove All directory with data and sub
rmdir "c:/temp/1" /s/q  







################################################################################################################
### Memcache  ##################################################################################################
################################################################################################################

$ watch -n1  'memcached-tool localhost stats'     <= number of time 1 sec
$ watch -n 1 'memcached-tool localhost stats'
















################################################################################################################
### Nginx  ##################################################################################################
################################################################

1. Nginx log rotate

$ mv /path/to/access.log /path/to/access.log.0
$ kill -USR1 `cat /var/run/nginx.pid`
$ sleep 1
[ post-rotation processing of old log file ]
	# rotates the logs is "kill -USR1 /var/run/nginx.pid". This does not kill the Nginx process, 
	# but instead sends it a signal causing it to reload its log files. This will cause new 
	# requests to be logged to the refreshed log file.


	
	
	
###
#  If the Nginx is running with PID, it copies to the /var/run/nginx.pid 	
$ ps aux | grep nginx
$ cat /var/run/nginx.pid














##################
### Log rotate ###
##################
to check logrotate, run cmd

$ logroate

### Configurations and default options
/etc/logrotate.conf

### Application-specific log file information (to override the defaults) 
$ ll /etc/logrotate.d/
-rw-r--r-- 1 root root 185 Feb  4  2016 httpd
-rw-r--r-- 1 root root 871 May 11 02:31 mysqld
-rw-r--r-- 1 root root 136 May  5 01:27 newrelic-sysmond


#### The logrotation for dpkg monitors the /var/log/dpkg.log file and does this on a 
/var/log/dpkg.log {
	monthly					# monthly basis - this is the rotation interval.
	size 100M				#### <===logs are rotated once the file size reaches 100M and this need not 
								wait for the monthly cycle.
	rotate 12				# 12 days worth of logs would be kept.
	compress				# logfiles can be compressed using the gzip format by specifying
	delaycompress			# will work only if 'compress' option is specified.
	missingok				# avoids halting on any error and carries on with the next log file.
	notifempty				# avoid log rotation if the log file is empty.
	create 644 root root	# create <mode> <owner> <group> creates a new empty file with the specified 
								properties after log-rotation.
}
####
	# Though missing in the above example, 'size' is also an important setting if you want to control the sizing of the logs growing in the system.

### Example ###	
$ cat /etc/logrotate.d/httpd
####
/var/log/httpd/*log {
    missingok
    notifempty
    sharedscripts
    delaycompress
    postrotate
        /sbin/service httpd reload > /dev/null 2>/dev/null || true
    endscript
}
####



### using Cron Job ###
	You can also set the logrotation as a cron so that the manual process can be avoided 
		and this is taken care of automatically. 
	By specifying an entry in /etc/cron.daily/logrotate , the rotation is triggered daily.

	$ /etc/cron.daily/logrotate
###
#!/bin/sh

/usr/sbin/logrotate /etc/logrotate.conf
EXITVALUE=$?
if [ $EXITVALUE != 0 ]; then
    /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"
fi
exit 0
###	
	
	

##########################################
### CUSTOM LOG FILE Rotation logrotate ###
##########################################

$ vi /etc/logrotate.d/rrserver

#####
/RRLinux/*log {
    weekly
    size 1G
    rotate 10
    missingok
    notifempty
    delaycompress
    postrotate
        /RRLinux/restart-eu-s1.sh > /dev/null		#switch to new log file
    endscript
}

#####
	

### Check log 	
$ cat /var/lib/logrotate.status

"/RRLinux/run.log" 2016-10-24     <=== New logrotate creation for /etc/logrotate.d/rrserver




### Testing logrotate.conf ###

$ logrotate -vdf /etc/logrotate.conf
		verbose flag, “-v”  
		debug flag,   “-d” 
		force flag,   "-f"

		
rotating pattern: /RRLinux/*log  forced from command line (10 rotations)
empty log files are not rotated, old logs are removed
considering log /RRLinux/error.log
  log does not need rotating

considering log /RRLinux/RidgerRacer_Server_vSlave.log
  log needs rotating

considering log /RRLinux/run.log
  log needs rotating
running postrotate script
running script with arg /RRLinux/error.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
rotating log /RRLinux/RidgerRacer_Server_vSlave.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
glob finding old rotated logs failed

renaming /RRLinux/RidgerRacer_Server_vSlave.log to /RRLinux/RidgerRacer_Server_vSlave.log-20161024
creating new /RRLinux/RidgerRacer_Server_vSlave.log mode = 0644 uid = 0 gid = 0

running postrotate script
running script with arg /RRLinux/RidgerRacer_Server_vSlave.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
rotating log /RRLinux/run.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
glob finding old rotated logs failed

renaming /RRLinux/run.log to /RRLinux/run.log-20161024
creating new /RRLinux/run.log mode = 0644 uid = 0 gid = 0
running postrotate script
running script with arg /RRLinux/run.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"

	


### httpd logrotate check ###

$ logrotate -vdf /etc/logrotate.d/httpd

##########
reading config file /etc/logrotate.d/httpd
reading config info for /var/log/httpd/*log

Handling 1 logs

rotating pattern: /var/log/httpd/*log  forced from command line (no old logs will be kept)
empty log files are not rotated, old logs are removed
considering log /var/log/httpd/access_log
  log does not need rotating
considering log /var/log/httpd/error_log
  log does not need rotating
not running postrotate script, since no logs were rotated
########	
	
	
	
### specific logrotate run ###	

$ logrotate -vdf /etc/logrotate.d/rrserver


####	
reading config file /etc/logrotate.d/rrserver
reading config info for /RRLinux/*log
Handling 1 logs

#---------------------------------------------------------------------------------------
rotating pattern: /RRLinux/*log  forced from command line (10 rotations)
empty log files are not rotated, old logs are removed
considering log /RRLinux/error.log
  log does not need rotating
considering log /RRLinux/RidgerRacer_Server_vSlave.log
  log needs rotating
considering log /RRLinux/run.log
  log needs rotating
running postrotate script
running script with arg /RRLinux/error.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"

#---------------------------------------------------------------------------------------
rotating log /RRLinux/RidgerRacer_Server_vSlave.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
renaming /RRLinux/RidgerRacer_Server_vSlave.log.10 to /RRLinux/RidgerRacer_Server_vSlave.log.11 (rotatecount 10, logstart 1, i 1                                                              0),
.....                                                          
renaming /RRLinux/RidgerRacer_Server_vSlave.log.0 to /RRLinux/RidgerRacer_Server_vSlave.log.1 (rotatecount 10, logstart 1, i 0),                                                              
renaming /RRLinux/RidgerRacer_Server_vSlave.log to /RRLinux/RidgerRacer_Server_vSlave.log.1
running postrotate script
running script with arg /RRLinux/RidgerRacer_Server_vSlave.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
removing old log /RRLinux/RidgerRacer_Server_vSlave.log.11
error: error opening /RRLinux/RidgerRacer_Server_vSlave.log.11: No such file or directory


#---------------------------------------------------------------------------------------
rotating log /RRLinux/run.log, log->rotateCount is 10
dateext suffix '-20161024'
glob pattern '-[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
renaming /RRLinux/run.log.10 to /RRLinux/run.log.11 (rotatecount 10, logstart 1, i 10),
....
renaming /RRLinux/run.log.0 to /RRLinux/run.log.1 (rotatecount 10, logstart 1, i 0),
renaming /RRLinux/run.log to /RRLinux/run.log.1
running postrotate script
running script with arg /RRLinux/run.log: "
        /RRLinux/restart-eu-s1.sh > /dev/null
"
removing old log /RRLinux/run.log.11
error: error opening /RRLinux/run.log.11: No such file or directory
####	
	
	
73. Logrotate	
	Configurations and default options 
	$ /etc/logrotate.conf
	
	Application-specific log file information
	$ /etc/logrotate.d/
	
	 /etc/logrotate.d/dpkg. One of the entries in this file would be:

	/var/log/dpkg.log {
			monthly						<= this on a monthly basis - this is the rotation interval.
			rotate 12					<= 'rotate 12' signifies that 12 days worth of logs would be kept.
			compress					<= logfiles can be compressed using the gzip format by specifying 'compress'
			delaycompress				<= 'delaycompress' delays the compression process till the next log rotation. 
											'delaycompress' will work only if 'compress' option is specified.
			missingok					<= 'missingok' avoids halting on any error and carries on with the next log file.
			notifempty					<= 'notifempty' avoid log rotation if the logfile is empty.
			create 644 root root		<= 'create <mode> <owner> <group>' creates a new empty file with the 
											specified properties after log-rotation.
	}

	Though missing in the above example, 'size' is also an important setting if you want to control the sizing 
	of the logs growing in the system.
	A configuration setting of around 100MB would look like:
	size 100M
	
	# Cron Job setup
	0 0 * * * (or @daily)  /etc/cron.daily/logrotate
	$ cat /var/lib/logrotate/status 
	


	
	

74.  Apache Log

Check config file

	$ apachectl configtest
	https://httpd.apache.org/docs/2.4/programs/apachectl.html
	
	
$ while true; do tail -n0 -f /var/log/apache2/access.log > /tmp/tmp.log & sleep 2; \ #continue
  kill $! ; wc -l /tmp/tmp.log | cut -c-2; done 2> /dev/null

  # kill $! <= kill PID of the most recent background command
  
 
	A. View Apache requests per minute

$ grep "16/Mar/2015:06" example.com | cut -d[ -f2 | cut -d] -f1 | awk -F: '{print $2":"$3}' | sort -nk1 -nk2 | uniq -c | awk '{ if ($1 > 10) print $0}'

	# grep "16/Mar/2015:06" example.com	
	  <= Use the grep command to only show hits from today during the 06th hour 
		from our Apache access log.
	# cut -d[ -f2 | cut -d] -f1	
	  <=Use the cut command with the -delimiter set to an opening bracket [ and print 
		out the -field of data that shows up 2nd, then use the cut command again 
		with the -delimter set to a closing bracket ] and print out the -field of 
		data that shows up 1st which gives us just the time stamp.
	# awk -F: '{print $2":"$3}'	
	  <=Use the awk command with the -Field delimiter set to a colon :, then print out 
		the $2nd column which is the hour, followed by the $3th column which is 
		the minute.
	# sort -nk1 -nk2 | uniq -c	
	  <= Sort the hits numerically by the 1st column which is the hour, then by the 
		2nd column which is the minute.
	# awk '{ if ($1 > 10) print $0}'	
	  <= Finally use the awk command with an if statement to only print out data when 
		the $1st column which is the number of hits in a minute is greater than 10.	
	

	
	
	
	
	
	
	
	
	

	
	
	
	
	
	
	
	
### Awk ### 
	Built-in Variables – FS, OFS, RS, ORS, NR, NF, FILENAME, FNR
	http://www.thegeekstuff.com/2010/01/8-powerful-awk-built-in-variables-fs-ofs-rs-ors-nr-nf-filename-fnr/?ref=binfind.com/web
	
	1. Awk FS Example: Input field separator variable.
	
	
	
	
	
# Remove a space in string
	https://stackoverflow.com/questions/13659318/how-to-remove-space-from-string
	sed 's/ //g'
	tr -d ' '
	$ echo "   3918912k " | sed 's/ //g'
		3918912k 
	
	
	
	
	
### What is a .h file?
Files that contain the .h file extension are normally header files used with the C or C++ programming languages. 
.h files are commonly known by programmers as "header files". They may contain constants, function prototypes 
and external variable definitions.

The header files are included by source code files that need to use the constants or functions defined in 
the header file. For example, if you define a text string with your company name in a header file, 
this constant can be used everywhere you need to display or otherwise use the company name in your source code. 
Should you later decide to change the name, you only have to do it in the header file and recompile to make the 
change take effect everywhere.	
	
	
	
	
### Network Trobleshooting
	# Wireshark Filter <= https://www.youtube.com/watch?v=68t07-KOH9Y
	filters are here:
	ip.addr == 10.0.0.1
	tcp or dns
	tcp.port == 443
	tcp.analysis.flags
	!(arp or icmp or dns)
	follow tcp stream
	tcp contains facebook
	http.response.code == 200
	http.request
	tcp.flags.syn == 1
	
	### Tshark ### 
	# Dump and analyze network traffic for CLI environment and READ from GUI WIRESHARK!!!! 
	https://linuxsimba.com/tshark-examples
	
	$ yum install wireshark
	$ tshark -D  				<= Print a list of the interfaces on which TShark can capture
	
	$ tshark -w /tmp/capture.log -f "port 22" -i eth0 -P     
			# -w  <= Write raw packet data to outfile 
			# -f  <= Set the capture filter expression.
			# -i  <= Network interface 
			# -P  <= Decode and display the packet summary
			
	# http://explainshell.com/explain?cmd=tshark+-w+%2Ftmp%2Fdhcp.pcap+-f+%22port+67+or+port+68%22+-i+eth1+-P
	
	$ tshark -r /tmp/capture.log
	
	$ tshark -D
		1. eth0
    $ tshark -i 1 -a duration:10 -w /tmp/10secs.pcap
	$ tshark -r /tmp/10secs.pcap
	
			
	##  ldd 
		prints the shared objects (shared libraries) required by each program or shared object specified 
		on the command line.  An example of its use and output is the following:
	$ ldd /usr/sbin/sshd
	
	## Debug
	$ yum groupinstall -y 'Development Tools'
	$ debuginfo-install openssh-server-6.6.1p1-33.el7_3.x86_64
	$ yum info openssl
	
	$rpm -q --changelog openssl | grep CVE-2015-319
		- fix CVE-2015-3197 - SSLv2 ciphersuite enforcement
		- fix CVE-2015-3194 - certificate verify crash with missing PSS parameter
		- fix CVE-2015-3195 - X509_ATTRIBUTE memory leak
		- fix CVE-2015-3196 - race condition when handling PSK identity hint
	
	# change MTU size
	$ ifconfig eth0 mtu 1000
	
	### check network RX size
	$ ifconfig eth0 | grep "RX " | awk '{print $2}' | cut -d: -f2
	$ RX=`ifconfig eth0 | grep "RX " | awk '{print $2}' | cut -d: -f2`; echo $RX

	
	
	### Ping
	- Is the remote host alive?		=> Host reachability
	- Is the network speed good? 	=> Network congestion
	- Is the remote host far? 		=> Travel length
	
	# Ping MTU discovery
	http://muzso.hu/2009/05/17/how-to-determine-the-proper-mtu-size-with-icmp-pings
	
	https://openmaniak.com/ping.php
	
	$ ping -M  do -s 1500 remote_host
			-M pmtudisc_opt
              Select  Path  MTU Discovery strategy.  pmtudisc_option may be either 
				do   <-(prohibit fragmentation, even local one), 
				want <-(do PMTU discovery, fragment locally when packet size is large),
                dont <-(do not set DF flag).
			-s packetsize
              Specifies  the number of data bytes to be sent.  The default is 56, 
			  which translates into 64 ICMP data bytes when combined with the 8 bytes 
			  of ICMP header data.
			1500 MTU size
			
			
	$ ping -M do -s 1500 45.55.5.69
	PING 45.55.5.69 (45.55.5.69) 1500(1528) bytes of data.
	ping: local error: Message too long, mtu=1500

	$ ping -M do -s 1400 45.55.5.69
	PING 45.55.5.69 (45.55.5.69) 1400(1428) bytes of data.
	1408 bytes from 45.55.5.69: icmp_seq=1 ttl=52 time=9.05 ms
	
	#------------------------------------------------------------------------------------
	#### Works solution ###
	$ vi /etc/ssh/ssh_config
	# uncomment on Ciphers and MACs under 
	#Host *
	Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc
	MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160
	#------------------------------------------------------------------------------------
	
	
	# TTL (Time TO LIVE)
	CentOS 	 64 TTL
	Windows 128 TTL
	Ubuntu  192 TTL
	
	### ICMP Packet 
	
	<----------><---------><-----------><--------->
	MAC Header 	 IP Header  ICMP Header  ICMP Data
	14-bytes	  20-bytes	 8-bytes
	<--------------------------------------------->     <= Ethernet Frame
				<--------------------------------->		<= IP Packet
				           <---------------------->		<= ICMP Packet
						   
						   
						   
						   
						   
						   
						   
						   
### tput 
		(http://linuxcommand.org/lc3_adv_tput.php)
		# When we start a terminal session on our Linux system, the terminal emulator sets the TERM environment 
		  variable with the name of a terminal type.	
		https://en.wikipedia.org/wiki/Tput		  
		# tput is a standard Unix operating system command which makes use of terminal capabilities.
		  Depending on the system, tput uses the terminfo or termcap database, as well as looking into the environment for the terminal type.
						   
						   
	### infocmp - compare or print out terminfo descriptions
		infocmp  can  be used to compare a binary terminfo entry with other terminfo entries, rewrite a
       terminfo description to take advantage of the use= terminfo field,  or  print  out  a  terminfo
       description  from  the  binary  file (term) in a variety of formats.  In all cases, the boolean
       fields will be printed first, followed by the numeric fields, followed by the string fields.				
						   
	$ echo $TERM
	$ infocmp screen
		#       Reconstructed via infocmp from file: /usr/share/terminfo/s/screen
		screen|VT 100/ANSI X3.64 virtual terminal,

	$ logname
	$ whoami
	
	The tput command can be used to test for a particular capability or to output the assigned value.
	$ tput logname
	xterm terminal emulator (X Window System)
	"screen" used by terminal multiplexers such as screen and tmux.
						   
	
	
	
	# Screen( multi shells)  
	https://www.rackaid.com/blog/linux-screen-tutorial-and-how-to/
		1. Use multiple shell windows from a single SSH session.
		2. Keep a shell active even through network disruptions.
		3. Disconnect and re-connect to a shell sessions from multiple locations.
		4. Run a long running process without maintaining an active shell session.

	$ screen						<= launch a screen Shell
	
	$ 'Ctrl+a' then '?'				<= get help list from the inside of screen
		# 'ctrl+a' is a signal to send commands to 'Screen' instead of the 'Shell'
                                
								Command key:  ^A   Literal ^A:  a
		break       ^B b        history     { }         other       ^A          split       S
		clear       C           info        i           pow_break   B           suspend     ^Z z
		colon       :           kill        K k         pow_detach  D           time        ^T t
		copy        ^[ [        lastmsg     ^M m        prev        ^H ^P p ^?  title       A
		detach      ^D d        license     ,           quit        \           vbell       ^G
		digraph     ^V          lockscreen  ^X x        readbuf     <           version     v
		displays    *           log         H           redisplay   ^L l        width       W
		dumptermcap .           login       L           remove      X           windows     ^W w
		fit         F           meta        a           removebuf   =           wrap        ^R r
		flow        ^F f        monitor     M           reset       Z           writebuf    >
		focus       ^I          next        ^@ ^N sp n  screen      ^C c        xoff        ^S s
		hardcopy    h           number      N           select      '           xon         ^Q q
		help        ?           only        Q           silence     _
		.......
	
	# 'C'reating Windows
	$ Ctrl+a+c  or(||) Ctrl+a  c
	
	# Switching Between Windows 'N'ext & 'P'revious
	$ Ctrl+a+n  || Ctrl+a  n   			<= Next 
	$ Ctrl+a+p  || Ctrl+a  p 			<= Previous  
	
	# 'D'etaching From Screen to go back to ORIGINAL SCREEN
	$ Ctrl+a+d	|| 	Ctrl+a d
	
	# Listing screens
	$ screen -ls	
		There are screens on:
        31184.pts-8.i7  (Detached)
        30957.pts-8.i7  (Detached)
        30323.pts-8.i7  (Detached)
		3 Sockets in /var/run/screen/S-apark
	
	# 'R'eattach to Screen
	$ screen -r	 31184.pts-8.i7
	
	# 'D'etach(Log Out) screen
	$ Ctrl+a+d	or 	Ctrl+a d
	
	# history save to 'H'ome ($HOME)
	$ Ctrl+a  H(Capital H)
	
	# Getting Alerts 'M'essage
	$ Ctrl+a  M(Capital M)
	
	# To monitor for silence or no output use 
	$ Ctrl-A  _(Underscore)
	
	
	# Screen Examples
	$ screen					<= launch a screen Shell
		$ ping google.com		<= a New screen Shell
		$ 'Ctrl+a'  'd' 		<= detach from the New Screen

	$ screen -ls				<= screen list
		There are screens on:
        31184.pts-8.i7  (Detached)
        30957.pts-8.i7  (Detached)
        30323.pts-8.i7  (Detached)
		3 Sockets in /var/run/screen/S-apark		<= socket location
	
	or	$ ls -l /var/run/screen/S-apark/
			srw-------. 1 apark apark 0 Jul 27 09:47 30323.pts-8.i7
			srw-------. 1 apark apark 0 Jul 27 09:48 30957.pts-8.i7
	
	$ screen -r 31184.pts-8.i7			<= 'R'eattach PID 31184 
	
	$ exit  || Ctrl+d					<= exit from screen PID     31184
		
	$ screen -ls
		There are screens on:
        30957.pts-8.i7  (Detached)
        30323.pts-8.i7  (Detached)

	$ screen -dr	PID				<=	Reattach a session and if necessary to detach it first.
 		There are several suitable screens on:
        31184.pts-8.i7  (Detached)
        30957.pts-8.i7  (Detached)
        30323.pts-8.i7  (Detached)
		Type "screen [-d] -r [pid.]tty.host" to resume one of them.

	$ screen -dr 31184.pts-8.i7			<= reattach '$ping google.com' screen


	
### ETL https://en.wikipedia.org/wiki/Extract,_transform,_load
	Extract, Transform, & Load 
	Three database functions that are combined into one tool to pull data out of one database 
	and place it into another database. Extract is the process of reading data from a database.						   
	
	
### UML	https://en.wikipedia.org/wiki/Unified_Modeling_Language
	The Unified Modeling Language (UML) is a general-purpose, developmental, modeling language in the field 
	of software engineering, that is intended to provide a standard way to visualize the design of a system.
	
### RSS https://en.wikipedia.org/wiki/RSS
	(Rich Site Summary; originally RDF Site Summary; often called Really Simple Syndication) uses a family of 
	standard web feed formats[2] to publish frequently updated information: blog entries, news headlines, 
	audio, video. An RSS document (called "RSS feed", "web feed", or "channel") includes full or summarized 
	text, and metadata, like publishing date and author's name.
	'RSS Feeds' enable publishers to syndicate data automatically. A standard XML file format ensures 
	compatibility with many different machines/programs. 
	RSS feeds also benefit users who want to receive timely updates from favourite websites 
	or to aggregate data from many sites
	
	
### Nexus or Artifictory for a Maven Repo	
	http://stackoverflow.com/questions/364775/should-we-use-nexus-or-artifactory-for-a-maven-repo
	
	
### /dev 		<= directory 
	http://unix.stackexchange.com/questions/18239/understanding-dev-and-its-subdirs-and-files
	crw-r-----. 1 root kmem 1, 1 Feb  2 17:50 /dev/mem
	
	^ Character devices
	There are two types of device files: block devices (indicated by b as the first character in the output of ls -l), 
	and character devices (indicated by c).
	
	- Block Devices (indicated by b as the first character in the output of ls -l), and character 
	devices (indicated by c). The distinction between block and character devices is not 
	completely universal. Block devices are things like disks, which behave like large, 
	fixed-size files: if you write a byte at a certain offset, and later read from the device 
	at that offset, you get that byte back.  
	
	- Character devices are just about anything else, where writing a byte has some immediate 
	effect (e.g. it's emitted on a serial line) and reading a byte also has some immediate 
	effect (e.g. it's read from the serial port).	
	
	
### /var/log 
	http://superuser.com/questions/565927/differences-in-var-log-syslog-dmesg-messages-log-files

	Log files from the system and various programs/services, especially login (/var/log/wtmp, which logs all 
	logins and logouts into the system) and syslog (/var/log/messages, where all kernel and system program 
	message are usually stored). Files in /var/log can often grow indefinitely, and may require cleaning at 
	regular intervals. Something that is now normally managed via log rotation utilities such as 'logrotate'. 
	This utility also allows for the automatic rotation compression, removal and mailing of log files. 
	Logrotate can be set to handle a log file daily, weekly, monthly or when the log file gets to a certain 
	size. Normally, logrotate runs as a daily cron job. This is a good place to start troubleshooting general 
	technical problems.

/var/log/messages 	– Contains global system messages, including the messages that are logged during system "START-UP". 
(up to start up)	  There are several things that are logged in /var/log/messages including mail, cron, daemon, kern, auth, etc.


# dmesg vs /var/log/dmesg

dmesg				- Realtime Kernel Log  <= Run from Prompt 
 					  Current content of the kernel syslog ring buffer messages
					  
/var/log/dmesg   	– Contains what was in that ring buffer when the boot process last completed
					  Contains kernel ring buffer information up till START-UP. 
					  When the system boots up, it prints number of messages 
					  on the screen that displays information about the hardware devices that the kernel 
					  detects during boot process. These messages are available in kernel ring buffer and 
					  whenever the new message comes the old message gets overwritten. You can also view the 
					  content of this file using the dmesg command.	  

					  
/var/log/secure		- CentOS: security login
/var/log/auth.log 	– Ubuntu: Contains system authorization information, including user logins and 
							  authentication machinsm that were used.

/var/log/boot.log 	– Contains information that are logged when the system boots
/var/log/daemon.log – Contains information logged by the various background daemons that runs on the system
/var/log/dpkg.log   – Contains information that are logged when a package is installed or removed using dpkg command
/var/log/kern.log   – Contains information logged by the kernel. Helpful for you to troubleshoot a custom-built 
						kernel. */var/log/lastlog – Displays the recent login information for all the u sers. 
						This is not an ascii file. You should use lastlog command to view the content of this file.
/var/log/mail.log   – Contains the log information from the mail server that is running on the system. For example, 
						sendmail logs information about all the sent items to this file
/var/log/user.log   – Contains information about all user level logs
/var/log/Xorg.x.log – Log messages from the X
/var/log/alternatives.log – Information by the update-alternatives are logged into this log file. On Ubuntu, 
						update-alternatives maintains symbolic links determining default commands.
/var/log/btmp 		– This file contains information about failed login attemps. Use the last command to view 
						the btmp file. For example, “last -f /var/log/btmp | more”
/var/log/cups 		– All printer and printing related log messages
/var/log/anaconda.log – When you install Linux, all installation related messages are stored in this log file
/var/log/yum.log 	– Contains information that are logged when a package is installed using yum
/var/log/cron 		– Whenever cron daemon (or anacron) starts a cron job, it logs the information about the 
						cron job in this file
/var/log/secure 	– Contains information related to authentication and authorization privileges. For example, 
						sshd logs all the messages here, including unsuccessful login.
/var/log/wtmp   	– Contains login records. Using wtmp you can find out who is logged into the system. 
/var/log/utmp		  who command uses this file to display the information.
/var/log/faillog 	– Contains user failed login attemps. Use faillog command to display the content of this file. 
						Apart from the above log files, /var/log directory may also contain the following 
						sub-directories depending on the application that is running on your system.
/var/log/httpd/ 
/var/log/apache2 	– Contains the apache web server access_log and error_log
/var/log/lighttpd/ 	– Contains light HTTPD access_log and error_log
/var/log/conman/ 	– Log files for ConMan client. conman connects remote consoles that are managed by conmand daemon.
/var/log/mail/ 		– This subdirectory contains additional logs from your mail server. For example, sendmail 
						stores the collected mail statistics in /var/log/mail/statistics file
/var/log/prelink/ 	– prelink program modifies shared libraries and linked binaries to speed up the startup 
						process. /var/log/prelink/prelink.log contains the information about the .so file that 
						was modified by the prelink.
/var/log/audit/ 	– Contains logs information stored by the Linux audit daemon (auditd).
/var/log/setroubleshoot/ – SELinux uses setroubleshootd (SE Trouble Shoot Daemon) to notify about issues in the 
						security context of files, and logs those information in this log file.
/var/log/samba/ 	– Contains log information stored by samba, which is used to connect Windows to Linux.
/var/log/sa/ 		– Contains the daily sar files that are collected by the sysstat package.
/var/log/sssd/ 		– Use by system security services daemon that manage access to remoA servlet is simply a 
						class which responds to a particular type of network request - most commonly an HTTP request. 
						Basically servlets are usually used to implement web applications - but there are also various 
						frameworks which operate on top of servlets (e.g. Struts) to give a higher-level abstraction 
						than the "here's an HTTP request, write to this HTTP response" level which servlets provide.




### Servlets 
	A servlet is simply a class which responds to a particular type of network request - most commonly an HTTP request. 
	Basically servlets are usually used to implement web applications - but there are also various frameworks which 
	operate on top of servlets (e.g. Struts) to give a higher-level abstraction than the "here's an HTTP request, 
	write to this HTTP response" level which servlets provide.

	Servlets run in a servlet container which handles the networking side (e.g. parsing an HTTP request, 
	connection handling etc). One of the best-known open source servlet containers is Tomcat.	
	
	
	
### Binary releases - contain computer OS readable version of the application, meaning it is compiled. 
### Source releases - contain human readable version(code) of the application, meaning it has to be compiled 
					  before it can be used.	
	
	
### primitive types <= basic types of data
					   byte, short, int, long, float, double, boolean, char
					   primitive variables store primitive values
		
### reference types <= any instantiable class as well as arrays
						String, Scanner, Random, Die, int[], String[], etc.
						reference variables store addresses	
	
	 
### Directory . and .. means
http://www.basicconfig.com/linux/linux_cd_command_tutorial	
	.  means the current directory.
	.. means the parent directory.
	e.g. when you type cd .. , you will move one level up. 
		 When you type cp /etc/resolv.conf  .  , you will copy file /etc/resolv.conf in the current directory.	
	
### Regression testing
	is a type of software testing that verifies that software previously developed and tested still 
	performs correctly even after it was changed or interfaced with other software. Changes may 
	include software enhancements, patches, configuration changes, etc.	
	
	
### Makefile - build 
	Make is a build automation tool that automatically builds executable programs and libraries from 
	source code by reading files called Makefiles which specify how to derive the target program. 
	Though integrated development environments and language-specific compiler features can also be 
	used to manage a build process, Make remains widely used, especially in Unix and Unix-like 
	operating systems.
	Besides building programs, Make can be used to manage any project where some files must be 
	updated automatically from others whenever the others change.	
	
### Node.JS
	It lets you write web apps that use Javascript on both the server and the client, so you 
	don't need to know multiple programming languages to program your website. It's also 
	really good at handling real-time concurrent web applications, which makes it a great 
	choice for a lot of modern web apps.
	
	NVM - Node Version Manager
	
###	Paginators 
	a feature of the SDK that act as an abstraction over this process to make it easier for developers 
	to use paginated APIs. A Paginator is essentially an iterator of results. They are created via the 
	getPaginator() method of the client.
	https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/paginators.html
	

### SSL cert location
public root certificates:

"/etc/ssl/certs/ca-certificates.crt", // Debian/Ubuntu/Gentoo etc.
"/etc/pki/tls/certs/ca-bundle.crt",   // Fedora/RHEL


	
	
###A bastion host 
is a special purpose computer on a network specifically designed and configured to withstand attacks. 
The computer generally hosts a single application, for example a proxy server, and all other services are removed or 
limited to reduce the threat to the computer. It is hardened in this manner primarily due to its location and purpose, 
which is either on the outside of the firewall or in theDMZ and usually involves access from untrusted networks or computers.
Internet Protocol Security (IPsec) is a protocol suite for securing Internet Protocol (IP) communications by 
authenticating and encrypting each IP packet of a communication session. IPsec includes protocols for establishing
 mutual authentication between agents at the beginning of the session and negotiation of cryptographic keys to be 
 used during the session. IPsec can be used in protecting data flows between a pair of hosts (host-to-host), 
 between a pair of security gateways (network-to-network), or between a security gateway and a host (network-to-host).[1]
IPsec is an end-to-end security scheme operating in the Internet Layer of the Internet Protocol Suite, while 
some other Internet security systems in widespread use, such as Transport Layer Security (TLS) and Secure Shell (SSH), 
operate in the upper layers of the TCP/IP model. Hence, IPsec protects any application traffic across an IP network. 
Applications do not need to be specifically designed to use IPsec. Without IPsec, the use of TLS/SSL must be designed 
into an application to protect the application protocols.

### Hardware Virtualization Machine (HVM)
Red Hat Enterprise Linux 6.4 (PV) - ami-6283a827 (64-bit) / ami-4a83a80f (32-bit)
Red Hat Enterprise Linux version 6.4 (PV), EBS-backed
Root device type: ebs Virtualization type: paravirtual


### Paravirtual (PVM)
A virtual machine(VM) mode in which operating systems do not require complete emulation of hardware devices. 
Paravirtual mode uses an API to interact with the host virtualization platform. Paravirtual mode doesn't require 
special virtualization technology hardware.



### Wildcard certificate
 A wildcard certificate is a public key certificate which can be used with multiple subdomains of a domain.[1]
Depending on the number of subdomains an advantage could be that it saves money and also could be more convenient. 
Limitation[edit]
Only a single level of subdomain matching is supported.[2]
It is not possible to get a wildcard for an Extended Validation Certificate.[3]
A workaround could be to add every virtual host name in the Subject Alternative Name (SAN) extension.
	[4][5][6] The major problem being that the certificate needs to be reissued whenever a 
	new virtual server is added.[7]
Wildcards can be added as domains in multi-domain certificates or Unified Communications Certificates(UCC).
	[8] In addition, wildcards themselves can have subjectAltName extensions, including other wildcards. 
	For example: The wildcard certificate *.wikipedia.org has *.m.wikimedia.org as an Subject Alternative Name. 
	Thus it secureshttps://www.wikipedia.org as well as the completely different website name 
	https://meta.m.wikimedia.org.[9] *.company.com

	
	
### Generate SSL Certificate Signing Request (CSR)
The first part of enrolling for your SSL Certificate is to generate a Certificate Signing Request (CSR). 
	CSR generation is wholly dependent on the software you use on your webserver. Select your webserver 
	software from the list after reading the following general points:
General Points to remember before creating your CSR:
•	The Common Name field should be the Fully Qualified Domain Name (FQDN) or the web address for which 
	you plan to use your Certificate, e.g. the area of your site you wish customers to connect to using SSL. 
	For example, an SSL Certificate issued forcomodogroup.com will not be valid for secure.comodogroup.com. 
	If the web address to be used for SSL is secure.comodogroup.com, ensure that the common name submitted 
	in the CSR is secure.comodogroup.com
Our SSL Certificate are compatible with almost all popular webserver software. If your webserver software 
	does not appear on the list, please contact support@comodo.com with full details of your webserver 
	software and we will contact you with further instructions.

### Jscrambler

JScrambler is an online JavaScript obfuscator and code optimization tool available as a Web application and Web API. 
	First release was in 2010 [1] by Auditmark, a startup based on a Portuguese business incubator known as UPTEC.
	[2][3] JScrambler is on the third version since April 17.[4] Features like locking down code to a specific domain, 
	setting the code to expire after a specific date, function outlining, changing the control flow of the program, 
	code obfuscation, code optimization, debugging code and assert elimination, deadcode elimination, constant 
	folding may be found at latest release of the software.[5][6]

A colocation centre or colocation center (also spelled co-location, collocation, colo, or coloc) is a type 
	of data centre where equipment, space, and bandwidth are available for rental to retail customers. 
	Colocation facilities provide space, power, cooling, and physical security for the server, storage, 
	and networking equipment of other firms—and connect them to a variety of telecommunications and network 
	service providers—with a minimum of cost and complexity.

OpenStack OpenStack is a global collaboration of developers and cloud computing technologists producing 
	the ubiquitous open source cloud computing platform for public and private clouds. The project aims 
	to deliver solutions for all types of clouds by being simple to implement, massively scalable, and 
	feature rich. The technology consists of a series ofinterrelated projects delivering various components 
	for a cloud infrastructure solution.

### Hash function
Hash functions are primarily used to generate fixed-length output data that acts as a shortened 
reference to the original data. This is useful when the original data is too cumbersome to use in its entirety.
One practical use is a data structure called a hash table where the data is stored associatively. 
Searching linearly for a person's name in a list becomes cumbersome as the length of the list increases, 
but the hashed value can be used to store a reference to the original data and retrieve constant time 
(barring collisions). Another use is in cryptography, the science of encoding and safeguarding data. 
It is easy to generate hash values from input data and easy to verify that the data matches the hash, 
but hard to 'fake' a hash value to hide malicious data. This is the principle behind the PGP algorithm for data validation.

Hash functions are also frequently used to accelerate table lookup or data comparison tasks such as finding 
items in a database, detecting duplicated or similar records in a large file and finding similar stretches in DNA sequences.
A hash function should be deterministic: when it is invoked twice on identical data (e.g. two strings 
containing exactly the same characters), the function should produce the same value. This is crucial 
to the correctness of virtually all algorithms based on hashing. In the case of a hash table, the lookup 
operation should look at the slot where the insertion algorithm actually stored the data that is being 
sought for, so it needs the same hash value.
Hash functions are typically not invertible, meaning that it is not possible to reconstruct the input datum x 
from its hash value h(x) alone. In many applications, it is common that several values hash to the same value, 
a condition called a hash collision. Since collisions cause "confusion" of objects, which can make exact 
hash-based algorithm slower and approximate ones less precise, hash functions are designed to minimize 
the probability of collisions. For cryptographic uses, hash functions are engineered in such a way that is 
impossible to reconstruct any input from the hash alone without expending great amounts of computing time 
(see also One-way function).
Hash functions are related to (and often confused with) checksums, check digits, fingerprints, randomization 
functions, error-correcting codes, and ciphers. Although these concepts overlap to some extent, each has its 
own uses and requirements and is designed and optimized differently. The Hash Keeper database maintained by the 
American National Drug Intelligence Center, for instance, is more aptly described as a catalog of file fingerprints 
than of hash values.

### Oplog
The oplog (operations log) is a special capped collection that keeps a rolling record of all operations that 
modify the data stored in your databases. MongoDB applies database operations on the primary and then records 
the operations on the primary's oplog.

### Tableau
Tableau Online is a hosted version of Tableau Server. It makes rapid-fire business analytics easier than ever. 
Share dashboards with your whole company and with customers and partners— in minutes. Provide live, interactive 
views of data that let people answer their own questions, right in a web browser or on a tablet. And do it in a secure, 
hosted environment.
Get up and running in minutes. Add new users in a few clicks. Tableau Online can scale up as much as you 
need and you can use it wherever you need it. You don't need to purchase, set up or manage any infrastructure.

### GridFS
1.	GridFS is a specification for storing and retrieving files that exceed the BSON-document size limit 
	of 16MB. Instead of storing a file in a single document, GridFS divides a file into parts, or chunks, 
	and stores each of those chunks as a separate document.
  

### OLAP
In computing, online analytical processing, or OLAP, is an approach to answering multi-dimensional analytical(MDA) 
queries swiftly.[1] OLAP is part of the broader category of business intelligence, which also encompasses 
relational database, report writing and data mining.[2] Typical applications of OLAP include business 
reporting for sales, marketing, management reporting, business process management (BPM),
[3] budgeting andforecasting, financial reporting and similar areas, with new applications 
coming up, such as agriculture.[4] The term OLAP was created as a slight modification of the 
traditional database term OLTP (Online Transaction Processing).[5]
OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. 
OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, 
and slicing and dicing.[6] Consolidation involves the aggregation of data that can be 
accumulated and computed in one or more dimensions. For example, all sales offices are 
rolled up to the sales department or sales division to anticipate sales trends. By contrast, 
the drill-down is a technique that allows users to navigate through the details. For instance, 
users can view the sales by individual products that make up a region’s sales. Slicing and 
dicing is a feature whereby users can take out (slicing) a specific set of data of the OLAP 
cube and view (dicing) the slices from different viewpoints.
Databases configured for OLAP use a multidimensional data model, allowing for complex 
analytical and ad hoc queries with a rapid execution time.[7] They borrow aspects of 
navigational databases, hierarchical databases andrelational databases.

Underlying data

When we do a survey or experiment over a period of time we usually plot the results 
and present the information to the reader in this condensed form. If the reader wants 
to check how we actually got the results they will want to examine the data that we used ... 
the data that underlies the results.

### MapReduce ###
MapReduce is a programming model for processing large data sets with a parallel, 
distributed algorithm on a cluster.[1]
A MapReduce program is composed of a Map() procedure that performs filtering and sorting 
(such as sorting students by first name into queues, one queue for each name) and 
a Reduce() procedure that performs a summary operation (such as counting the number 
of students in each queue, yielding name frequencies). The "MapReduce System" 
(also called "infrastructure" or "framework") orchestrates by marshalling the 
distributed servers, running the various tasks in parallel, managing all communications 
and data transfers between the various parts of the system, and providing for redundancy and fault tolerance.
The model is inspired by the map and reduce functions commonly used in functional 
programming,[2] although their purpose in the MapReduce framework is not the same 
as in their original forms.[3] The key contributions of the MapReduce framework are 
not the actual map and reduce functions, but the scalability and fault-tolerance 
achieved for a variety of applications by optimizing the execution engine once. 
As such, a single-threaded implementation of MapReduce will usually not be faster 
than a traditional implementation. Only when the optimized distributed shuffle 
operation (which reduces network communication cost) and fault tolerance features 
of the MapReduce framework come into play, the use of this model is beneficial.
MapReduce libraries have been written in many programming languages, with different 
levels of optimization. A popular open-source implementation is Apache Hadoop. 
The name MapReduce originally referred to the proprietaryGoogle technology but has since been genericized.

### Nginx ###

Nginx (pronounced "engine-ex") is an open source reverse proxy server for HTTP, 
HTTPS, SMTP, POP3, and IMAP protocols, as well as a load balancer, HTTP cache, 
and a web server (origin server). The nginx project started with a strong focus 
on high concurrency, high performance and low memory usage. It is licensed 
under the 2-clause BSD-like license and it runs on Linux, BSD variants, Mac OS X, 
Solaris, AIX, HP-UX, as well as on other *nix flavors. It also has a proof 
of concept port for Microsoft Windows.[6]

### Null (SQL)
Null is a special marker used in Structured Query Language (SQL) to indicate that a data 
value does not exist in the database. Introduced by the creator of the relational database 
model, E. F. Codd, SQL Null serves to fulfill the requirement that all true relational 
database management systems (RDBMS) support a representation of "missing information and 
inapplicable information". Codd also introduced the use of the lowercase Greek omega (ω) 
symbol to represent Null in database theory. NULL is also an SQL reserved keyword used 
to identify the Null special marker.

Null has been the focus of controversy and a source of debate because of its 
associatedthree-valued logic (3VL), special requirements for its use in SQL joins, 
and the special handling required by aggregate functions and SQL grouping operators. 
Computer science professor Ron van der Meyden summarized the various issues as: "The 
inconsistencies in the SQL standard mean that it is not possible to ascribe any intuitive 
logical semantics to the treatment of nulls in SQL."[1] Although various proposals 
have been made for resolving these issues, the complexity of the alternatives has prevented 
their widespread adoption.

For people new to the subject, a good way to remember what null means is to remember that 
in terms of information, "lack of a value" is not the same thing as "a value of zero"; 
similarly, "lack of an answer" is not the same thing as "an answer of no". For example, 
consider the question "How many books does Juan own?" The answer may be "zero" 
(weknow that he owns none) or "null" (we do not know how many he owns, or doesn't own). 
In a database table, the column reporting this answer would start out with a value of null, 
and it would not be updated with "zero" until we have ascertained that Juan owns no books.
SQL null is a state (unknown) and not a value. This usage is quite different from 
programming languages, where null means not assigned to a particular instance


	
	
	
	
	
	
###### Print Usage	######
#------------------------------------------------------------------------------------	
#!/bin/sh
print_usage() {
        echo -e "`basename $0` ssh_connexion local_script"
        echo -e "Remote executes local_script on ssh server"
        echo -e "For convinient use, use ssh public key for remote connexion"
        exit 0
}

[ $# -eq "2" ] && [ $1 != "-h" ] && [ $1 != "--help" ] || print_usage

INTERPRETER=$(head -n 1 $2 | sed -e 's/#!//')

cat $2 | grep -v "#" | ssh -t $1 $INTERPRETER	
#------------------------------------------------------------------------------------	
	
https://coderwall.com/p/85jnpq/bash-built-in-variables
$#, $@ & $? bash built-in variable

$ vi test.sh
---------------
#!/bin/sh
echo '$#' $#
echo '$@' $@
echo '$?' $?
---------------
$ bash ./test.sh 1 2 3
output:
$#  3
$@  1 2 3
$?  0

*You passed 3 parameters to your script.*

$# = number of total arguments. <= 'Usage' error report if any arg missing
$@ = what parameters were passed. Answer is 1 2 3
$? = was last command successful. Answer is 0 which means 'True'	
#------------------------------------------------------------------------------------	
	
	
	
	
### Stress test using 'stress'
http://www.tecmint.com/linux-cpu-load-stress-test-with-stress-ng-tool/

# Install
$ wget ftp://fr2.rpmfind.net/linux/dag/redhat/el7/en/x86_64/dag/RPMS/stress-1.0.2-1.el7.rf.x86_64.rpm 
$ yum localinstall stress-1.0.2-1.el7.rf.x86_64.rpm
	
	

# Internet speed test
http://askubuntu.com/questions/104755/how-to-check-internet-speed-via-terminal

$ curl -s  https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py | python -	
$ wget --output-document=/dev/null http://speedtest.wdc01.softlayer.com/downloads/test500.zip	
	
# Ubuntu 16.04	
$ sudo apt install speedtest-cli
$ speedtest-cli	

$ pip install speedtest-cli	
	


### SONAR
	Open Source Project and Code Quality Monitoring
	https://www.sonarsource.com/
	http://www.methodsandtools.com/tools/tools.php?sonar

### Anemometer[ӕnəmάmitər 애너마미털] 풍향게
	https://github.com/box/Anemometer
	The MySQL Slow Query Monitor. This tool is used to analyze slow query logs collected 
	from MySQL instances to identify problematic queries.	

	
### OSI Layers - Quick Summary
	Application 	<- Responsible for determining when access to the network is required
	Presentation	<- Ensures data is received in a useable format.
						Data encryption is done here!
	Session			<- Establishing & maintaing connections
						Responsible for ports and ensure queries for services.
	Transport		<- Breaks data into frames & assigns sequence numbers. Checks errors in data received.
						UPD and SPX
	Network			<- How systems on different network segments find each other.
						Source-Destination addresses, Subnets, Path determination.  
						IP & IPX
	DataLink		<- Frames.  Handles flow control.  MAC
	Physical		<- Transmissionof the raw bit stream. Electrical signalling.
	
	
	
***Troubleshoot***
CentOs 7 YUM update not working
	$echo "http_caching=packages" >>   /etc/yum.conf file
	

### Received disconnect from server: 2: Too many authentication failures for username
https://superuser.com/questions/187779/too-many-authentication-failures-for-username
resolution: config .ssh/config

	
	
###########################################
### Lynda.com - Unix for Mac OS X Users ###
###########################################

1-1 The Terminal Application
	/Applications/Utilities/Terminal
	# Shift + CMD + u	

	$ Ctrl + A 			<- Move cursor to start of line	
	$ Ctrl + E			<- Move cursor to end of line
	$ Option + Click 	<- Move cursor to click point
	
	
	
	
	
	
###########################################	
### Terminology 		###
###########################################
	
Stack 		<- used for static memory allocation 
				int x=1
				int x=
Heap 		<- dynamic memory allocation	
				frm Object
	int x=1
	int y=2
	Form1 frm=newForm1()

You can use the stack if you know exactly how much data you need to allocate before compile time 
and it is not too big.	You can use heap if you don't know exactly how much data you will need 
at runtime or if you need to allocate a lot of data.

In a multi-threaded situation each thread will have its own completely independent stack but 
they will share the heap. Stack is thread specific and Heap is application specific. The stack 
is important to consider in exception handling and thread executions.	
	
	
	
------------------------------------------------------------------------------	
Cross-Site Request Forgery (CSRF)
https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF)	
Cross-Site Request Forgery (CSRF) is an attack that forces an end user to execute unwanted actions on a web 
application(e.g. Jenkins) in which they're currently authenticated. CSRF attacks specifically target 
state-changing requests, not theft of data, since the attacker has no way to see the response to the 
forged request. With a little help of social engineering (such as sending a link via email or chat), 
an attacker may trick the users of a web application into executing actions of the attacker's choosing. 

If the victim is a normal user, a successful CSRF attack can force the user to perform state changing 
requests like transferring funds, changing their email address, and so forth. If the victim is an 
administrative account, CSRF can compromise the entire web application.
	

------------------------------------------------------------------------------
What is HTTPS?
HTTPS is HTTP over a connection secured by TLS(used to be called SSL)
	<= HTTP wraped over TLS encryped  connection	

	   
	   
------------------------------------------------------------------------------
### Web site 'Random' unavailable error 
nginx (lb) -> Apache(WWW) -> MySQL9DB)

$ cat /var/log/messages		(CentOS)
$ cat /var/log/syslog     	(Ubuntu)
------------------------------------------------------------------------------
### Problem: Nginx LB server is dropping packets in OS Kernel level.
#Jul 10 02:27:54 nlt-lb-01 kernel: nf_conntrack: table full, dropping packet
#.....
#Jul 10 02:27:54 nlt-lb-01 kernel: nf_conntrack: table full, dropping packet
#Jul 10 02:27:59 nlt-lb-01 kernel: net_ratelimit: 169 callbacks suppressed
------------------------------------------------------------------------------

After installed the IPTABLES, all netfilter related values are installed and set!

## what is CURRENT Count?
$ /sbin/sysctl    net.netfilter.nf_conntrack_count	
$ /sbin/sysctl -a | grep netfilter | grep count
	31774	
$ cat /proc/sys/net/netfilter/nf_conntrack_count


## What is CURRENT 'nf_conntrack_max' value
$ cat /proc/sys/net/netfilter/nf_conntrack_max
	262144
$ /sbin/sysctl net.netfilter.nf_conntrack_max		
	262144


# sysctl 	<= configure kernel parameters at runtime
$ sysctl -a | grep netfilter
$ sysctl -a | grep conntrack
$ sysctl --names --all | grep -i conntrack
 
# 

### Increase the MAX connection issue 'kernel: nf_conntrack: table full, dropping packet"
https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt
http://antmeetspenguin.blogspot.com/2011/01/high-performance-linux-router.html

$ vi /etc/sysctl.conf	
	net.netfilter.nf_conntrack_max = 500000   <= add a VAR and value  

$ /sbin/sysctl -p  (or -f)    <= reload


## check again for "net.netfilter.nf_conntrack_max"
	# sysctl - configure kernel parameters at runtime

$ sysctl -a | grep netfilter | grep max
$ sysctl -a | grep conntrack | grep max

	sysctl: reading key "net.ipv6.conf.all.stable_secret"
	sysctl: reading key "net.ipv6.conf.default.stable_secret"
	sysctl: reading key "net.ipv6.conf.enp0s25.stable_secret"
	sysctl: reading key "net.ipv6.conf.lo.stable_secret"
	net.netfilter.nf_conntrack_expect_max = 1024
	net.netfilter.nf_conntrack_max = 500000
	net.netfilter.nf_conntrack_tcp_max_retrans = 3
	net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300
	net.nf_conntrack_max = 500000

net.netfilter.nf_conntrack_max = 500000   <- increase from 262144 to 500000
	
### Increase the "Hash-Table" Size	
$ cat /sys/module/nf_conntrack/parameters/hashsize
	65536				<= It should be around Max(5000000)/8 = 62500
	
	
	
### you can store about 32 times more conntrack entries than the default,	###
### and get better performance for conntrack entry access.					###
	
	
------------------------------------------------------------------------------
# sysctl examples
       /sbin/sysctl -a
       /sbin/sysctl -n kernel.hostname
       /sbin/sysctl -w kernel.domainname="example.com"
       /sbin/sysctl -p/etc/sysctl.conf
       /sbin/sysctl -a --pattern forward
       /sbin/sysctl -a --pattern forward$
       /sbin/sysctl -a --pattern 'net.ipv4.conf.(eth|wlan)0.arp'
       /sbin/sysctl --system --pattern '^net.ipv6'
------------------------------------------------------------------------------


























































	   
	   
	   
	   
	   
	